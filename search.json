[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handbuch IT in Bibliotheken",
    "section": "",
    "text": "Einleitung",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Handbuch IT in Bibliotheken",
    "section": "Motivation",
    "text": "Motivation\n\nBecause the library has become software, it is no longer viable for our services to exist separately from our software. [...] Most importantly, all library staff must understand that our software is our library, and is everyone’s responsibility.\n— (Hanson, Cody 2015)\n\nMit der wachsenden Bedeutung der Informationstechnologie (IT) im Allgemeinen und für Bibliotheken im Besonderen bleibt kaum ein Aspekt bibliothekarischer Aufgaben, der nicht durch IT unterstützt wird. Deutlich wird dies z.B. durch das stetig zunehmende Angebot an elektronischen Informationsmitteln, die Digitalisierung historischer Bestände, interoperable Metadaten oder auch die digitale Langzeitarchivierung. Die alltägliche Handhabung von IT (Smartphones, Automatisierung, Vernetzung …) wird häufig einfacher, die zugrunde liegenden Systeme werden jedoch immer komplexer und erfordern entsprechend mehr Wissen zu ihrem Aufbau und Betrieb. Während sich einige Teile der IT in Bibliotheken nicht wesentlich von IT in anderen Bereichen unterscheiden, gibt es doch zahlreiche Aspekte von Bibliotheks-IT, die nicht oder nicht speziell genug an anderer Stelle behandelt werden. Das vorliegende Handbuch möchte diese Lücke schließen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#zielgruppe",
    "href": "index.html#zielgruppe",
    "title": "Handbuch IT in Bibliotheken",
    "section": "Zielgruppe",
    "text": "Zielgruppe\nAls Einführung und Nachschlagewerk wendet sich dieses Handbuch vor allem an Personen, die sich einen ersten Überblick über die verschiedenen IT-Dienste an Bibliotheken verschaffen wollen. Dies können z. B. Personen sein, die sich im Rahmen ihrer bibliothekarischen Ausbildung mit IT-Diensten in Bibliotheken beschäftigen oder die sich im Rahmen der Einarbeitung in eine neue Position mit IT-Diensten in Bibliotheken beschäftigen. Zur Veranschaulichung der Zielgruppen dienen mehrere sogenannte Personas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#inhalt",
    "href": "index.html#inhalt",
    "title": "Handbuch IT in Bibliotheken",
    "section": "Inhalt",
    "text": "Inhalt\nDieses Handbuch soll einen knappen und gleichzeitig umfassenden Überblick über die wichtigsten IT-bezogenen Themen in Bibliotheken geben. Die einzelnen Themenkapitel sind weitgehend unabhängig voneinander lesbar und mit Querverweisen verbunden. Die Kapitel bilden grob zwei Blöcke:\n\nAllgemeine technische Grundlagen\n\nTechnische Infrastruktur beschreibt grundlegende technischen Einrichtungen einer Bibliothek für den Betrieb von Prozessen und Dienstleistungen\nManagement von IT-Systemen beinhaltet die Einführung und den Betrieb von IT-Systemen allgemein\nAnforderungsanalyse umfasst die Ermittlung und Erfüllung von Bedarfen und Anforderungen an IT-Systeme\nSicherheit & Datenschutz beschreibt Vorgaben und Maßnahmen zur Förderung der IT-Sicherheit und des Datenschutz\nDaten & Metadaten stellt wichtige Begriffe, Standards und Prozesse der Datenverarbeitung in Bibliotheken vor\n\n\n\nBibliotheksspezifische Dienste\n\nBibliotheksmanagementsysteme sind spezialisierte Anwendungen für Arbeitsprozesse rund um Erwerbung, Erschließung, Ausleihe, Zugriff und Auffindbarmachung von Bibliotheksbeständen\nDiscovery-Systeme stellt Arten, Bestandteile und Funktionen von Rechercheplattformen vor und liefert Hinweise zu Auswahl und Betrieb von Discovery-Systemen\nDigitalisierung umfasst Prozesse und Werkzeuge zur Digitalisierung, Erschließung und Präsentation von Kulturgütern\nForschungsnahe Dienste beschreibt Dienste wie Repositorien und Forschungsdatenmanagement zur Unterstützung von Forschungsprozessen\nKommunikation beinhaltet Werkzeuge und Methoden interner und externer Kommunikation von Wissensmanagement bis Öffentlichkeitsarbeit\n\n\n\n\n\n\n\nGut zu wissen\n\n\n\nDas Handbuch ist ein „lebendiges Buch“, das stetig ergänzt und aktualisiert werden kann und soll. Tipps und Korrekturen sind daher sehr willkommen! Hinweise zur Mitarbeit und Details zur Umsetzung des Handbuchs finden sich im Anhang.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#entstehungsgeschichte",
    "href": "index.html#entstehungsgeschichte",
    "title": "Handbuch IT in Bibliotheken",
    "section": "Entstehungsgeschichte",
    "text": "Entstehungsgeschichte\nZur Erstellung des Handbuchs wurden zwischen April 2022 und Oktober 2023 drei Book Sprints an der Bibliothek der Technischen Hochschule Wildau durchgeführt. Dabei trafen sich IT-affine Expert*innen aus dem Bibliotheksbereich, um innerhalb von wenigen Tagen eine umfassende Übersicht zu den wichtigsten Themen rund im IT in Bibliotheken zu verfassen. Die Veranstaltung wurde mit Mitteln des Publikationsfonds für Open-Access-Monografien des Landes Brandenburg gefördert. Kontaktinformationen, Neuigkeiten und Hintergrund zum Projekt finden sich auf der Seite https://www.th-wildau.de/book-sprint/ und in den Artikeln von Bach (2022) und Christensen und Seeliger (2022).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#autorinnen",
    "href": "index.html#autorinnen",
    "title": "Handbuch IT in Bibliotheken",
    "section": "Beteiligte Autor*innen",
    "text": "Beteiligte Autor*innen\n\n\nNicolas Bach, Student an der HdM Stuttgart\nnicolas.bach at posteo dot de\nPascal-Nicolas Becker, Gründer und Geschäftsführer der The Library Code GmbH\nhttps://orcid.org/0000-0003-2169-1261 pascal at the-library-code dot de\nMonty Bitto, Student an der HSH\nhttps://orcid.org/0009-0004-0865-257X dorian-bela-monty.bitto at stud dot hs-hannover dot de\nJanna Brechmacher, Stabstelle IT in der Benutzungsabteilung der Staatsbibliothek zu Berlin\nhttps://orcid.org/0000-0001-7233-7153 janna.brechmacher at sbb dot spk-berlin dot de\nJens Bemme, SLUB Dresden, Referat Saxonica und Kartensammlung (Citizen Science)\nhttps://orcid.org/0000-0001-6860-0924 jens.bemme at slub-dresden dot de\nSascha A. Carlin, Agile Coach für Führungskräfte in der Softwareentwicklung\nrqst at nvsbl dot cm\nAnne Christensen, Bibliothekarin und Projektmanagerin bei effective WEBWORK sowie Lehrbeauftragte an verschiedenen Hochschulen\nhttps://orcid.org/0000-0001-7753-1078 christensen at effective-webwork dot de\nJana Eger, Stadtbibliothek Chemnitz\njana.eger at stadtbibliothek-chemnitz dot de\nMatthias Finck, Professur für Informationstechnologie, Hochschule für Angewandte Wissenschaft Hamburg\nhttps://orcid.org/0000-0002-0848-2935 matthias.finck at haw-hamburg dot de\nUlrike Golas, IT-Leitung der Universitätsbibliothek der TU Berlin\nhttps://orcid.org/0000-0002-6567-0000 ulrike.golas at tu-berlin dot de\nTina Goldammer, stellv. Leiterin/Fortbildungen bei der SLUB/Sächsische Landesfachstelle für Bibliotheken\ntina.goldammer at slub-chemnitz dot de\nGerrit Gragert, Leitung IT-Services für die Digitale Bibliothek in der Staatsbibliothek Berlin\nhttps://orcid.org/0000-0002-0542-1555 gerrit.gragert at sbb dot spk-berlin dot de\nLambert Heller, Leitung Open Science Lab an der TIB - Leibniz‐Informationszentrum Technik und Naturwissenschaften\nhttps://orcid.org/0000-0003-0232-7085 lambert.heller at tib dot eu\nSina Hurnik, Studio für Kommunikationsdesign\nhttps://sinahurnik.com/\nAdienne Karsten, Referentin für Forschungsdatenmanagement, Universität Münster/Universitäts- und Landesbibliothek\nhttps://orcid.org/0000-0002-0562-7393 akarsten at uni-muenster dot de\nClemens Kynast, Discoverysysteme & Bibliotheksautomatisierung an der ThULB Jena\nclemens.kynast at uni-jena dot de\nLukas Lerche, UB Dortmund\nhttps://orcid.org/0000-0002-4027-6840 lukas.lerche at tu-dortmund dot de\nLuis Moßburger\nhttps://orcid.org/0000-0002-5326-219X lmossburger at t-online dot de\nStefanie Nagel, Abteilungsleiterin Open Science an der Universitätsbibliothek TU Bergakademie Freiberg\nhttps://orcid.org/0000-0001-8020-1440 stefanie.nagel at ub dot tu-freiberg dot de\nSilvia Polla, Forschungsdatenmanagement, Bibliothek\nhttps://orcid.org/0000-0002-2395-2448 polla at wias-berlin dot de\nMichael Schaarwächter, Bibliotheks-IT an der UB Dortmund\nhttps://orcid.org/0000-0002-0180-5930 michael.schaarwaechter at tu-dortmund dot de\nZoe Schubert, Wissenschaftliche Mitarbeiterin & Entwicklerin bei der TIB Hannover und der Staatsbibliothek zu Berlin\nhttps://orcid.org/0000-0001-9043-3632 zoe.schubert at tib dot eu\nFrank Seeliger, Bibliotheksleiter TH Wildau\nhttps://orcid.org/0000-0003-0602-8082 fseeliger at th-wildau dot de\nBritta Steinke, Referentin für Forschungsdatenmanagement\nhttps://orcid.org/0000-0001-6816-5168 b.steinke at tu-berlin dot de\nKatja Sternitzke, Wissenschaftliche Mitarbeiterin an der Staatsbibliothek zu Berlin\nhttps://orcid.org/0000-0002-9815-0490 katja.sternitzke at sbb dot spk-berlin dot de\nRalf Stockmann, Direktor Digitale Entwicklung und Verbundangelegenheiten (DEVA) - Zentral- und Landesbibliothek Berlin\nhttps://orcid.org/0000-0002-0977-5908 ralf.stockmann at zlb dot de\nFlorian Strauß, Abteilungsleiter forschungsunterstützende Dienste Universitätsbibliothek Clausthal\nhttps://orcid.org/0000-0003-0168-0450 florian.strauss at tu-clausthal dot de\nAlexandra Streck, Online-Redakteurin, Marketing ZB MED - Informationszentrum Lebenswissenschaften\nstreck at zbmed dot de\nRobert Strötgen, Leiter der Universitätsbibliothek der TU Braunschweig\nhttps://orcid.org/0000-0003-3320-5187 r.stroetgen at tu-braunschweig dot de\nJakob Voß, Forschung und Entwicklung an der VZG Göttingen\nhttps://orcid.org/0000-0002-7613-4123 jakob.voss at gbv dot de\nMichael Voss, Consultant; IT-Expert-Voss\nhttps://orcid.org/0000-0002-7402-1598 info at it-expert-voss dot de\nKerstin Wendt, Stabsstelle Digitalisierung, Staats- und Universitätsbibliothek Hamburg\nkerstin.wendt at sub dot uni-hamburg dot de\nCarolin Zapke, Fachreferat und ÖA an der UB Chemnitz\nhttps://orcid.org/0000-0002-2516-535X carolin.zapke at bibliothek dot tu-chemnitz dot de\nDavid Zellhöfer, Professor für Digitale Innovation in der öffentlichen Verwaltung an der HWR Berlin\nhttps://orcid.org/0000-0002-0403-457X david.zellhoefer at hwr-berlin dot de",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "index.html#lizenz",
    "href": "index.html#lizenz",
    "title": "Handbuch IT in Bibliotheken",
    "section": "Rechte an den Inhalten",
    "text": "Rechte an den Inhalten\nAlle Inhalte dieses Buches werden unter der Lizenz Creative Commons Namensnennung 4.0 International (CC BY 4.0) veröffentlicht. Für Abbildungen kann auch eine CC-BY-Lizenz (kein -NC oder -ND) verwendet werden (siehe Abbildungsverzeichnis).\nDas heißt: Sie dürfen das Material in jedwedem Format oder Medium vervielfältigen und weiterverbreiten (Teilen) und das Material remixen, verändern und darauf aufbauen (Bearbeiten) und zwar für beliebige Zwecke, inklusive kommerzielle Zwecke, unter der Bedingung, dass Sie angemessene Urheber*innen- und Rechteangaben machen, einen Link zur Lizenz beifügen und angeben, ob Änderungen vorgenommen wurden (Namensnennung).\n\n\n\n\nBach, Nicolas. 2022. „Das Handbuch IT in Bibliotheken: Einblicke in den ersten bibliothekarischen Book Sprint Deutschlands“. Informationspraxis 8 (1). https://doi.org/10.11588/ip.2022.1.94475.\n\n\nChristensen, Anne, und Frank Seeliger. 2022. „„Wie schreiben wir gemeinsam ein nützliches Buch?”“. b.i.t.online 25 (6): 509–10. https://www.b-i-t-online.de/heft/2022-06-nachrichtenbeitrag-christensen.pdf.\n\n\nHanson, Cody. 2015. „Opinion: Libraries are Software“. 2015. https://www.codyh.com/writing/software.html.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Einleitung</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html",
    "href": "infrastruktur.html",
    "title": "Technische Infrastruktur",
    "section": "",
    "text": "Einleitung\nDie technische Infrastruktur einer Bibliothek umfasst alle IT-Systeme, die die Prozesse und Dienstleistungen einer Bibliothek abbilden, unterstützen oder ergänzen. Neben dem zentralen Bibliotheksmanagementsystem (BMS) (auf Englisch auch LMS genannt) zur Verwaltung und Bereitstellung von Bibliotheksbeständen und Discovery-Systemen zur Recherche, gibt es zahlreiche weitere etablierte Anwendungen von IT in Bibliotheken. Die in diesem Kapitel vorgestellte Infrastruktur ist grob nach Hauptanwendungsfall gegliedert in:\nWeitgehend ausgeklammert, weil an anderer Stelle behandelt, bleiben forschungsnahe Dienste wie Repositories und Open Data, Infrastruktur zur Digitalisierung sowie Anwendungen für Kommunikation und Wissensmanagement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html#einleitung",
    "href": "infrastruktur.html#einleitung",
    "title": "Technische Infrastruktur",
    "section": "",
    "text": "Allgemeine Infrastruktur für den grundlegenden Betrieb von Prozessen und Dienstleistungen in Bibliotheken wie die Verbuchung.\nGrundlagenwissen zur Objektidentifikation physischer Medien mit RFID\nDienste primär für Nutzer*innen wie Webseite, Internetzugang und Arbeitsplätze\nDienste primär für Mitarbeiter*innen wie Intranet und mobiles Arbeiten\n\n\n\n\n\n\n\n\nInfo\n\n\n\nZu Einführung und Betrieb und zur Entwicklung samt Anforderungsanalyse von IT-Systemen gibt es eigene Kapitel.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html#allgemeine-infrastruktur-für-bibliotheksprozesse-und--dienste",
    "href": "infrastruktur.html#allgemeine-infrastruktur-für-bibliotheksprozesse-und--dienste",
    "title": "Technische Infrastruktur",
    "section": "Allgemeine Infrastruktur für Bibliotheksprozesse und -dienste",
    "text": "Allgemeine Infrastruktur für Bibliotheksprozesse und -dienste\n\nVerbuchung\nZur Verbuchung zählen die Ausleihe und Rückgabe von Medien sowie die Verlängerung von Leihfristen. Für automatische Verbuchung müssen entsprechende Geräte vorhanden sein, um die für Medien eingesetzten Identifikationsmerkmale (Barcodes oder [RFID]-Tags) zu lesen und ggf. auch zu schreiben. Außerdem muss eine Kommunikation mit dem BMS stattfinden, um darin den entsprechenden Ausleih-, Rückbuchungs-, oder Verlängerungsvorgang durchzuführen. Hierzu muss außerdem eine Identifikation der Nutzer*innen erfolgen, also das Einlesen eines Ausweises oder das Anmelden mit gewissen Zugangsdaten und schließlich das Eintragen einer Ausleihe. Da es Nutzer*innen möglich sein sollte, die Rückgabe von Medien nachweisen zu können, kann bei der Rückgabe mit Verbuchung auf Wunsch eine Quittung gedruckt werden. Dies ist sowohl an den Ausleihtheken als auch bei Rückgabeautomaten möglich. Ein weiterer Self-Service ist das Bezahlen und Begleichen von Gebühren an speziell dafür aufgestellten Bezahlstationen.\n\n\nAutomaten\nZur Gestaltung eines Angebots, das über die Servicezeiten hinausgeht, ist der Einsatz von Automaten für Standarddienstleistungen sinnvoll. Vorgänge wie Ausleihe, Verlängerung und Rückgabe von Medien, Bezahlung von Gebühren oder auch die Abholung von Bestellungen können mit Automaten weitgehend ohne Personal realisiert werden.\nAusleihautomaten bestehen aus Lesegeräten für Ausweise, Lesegeräten für Medien und Entsicherungsgeräten für Buchsicherungen. Beim Einsatz von RFID-Geräten können Medien von einem RFID-Reader sowohl gelesen als auch entsichert werden. Der Verbuchungsprozess beinhaltet eine Anmeldung (Ausweisnummer und ggf. Passwort), den eigentlichen Verbuchungsprozess inklusive Entsicherung der Medien, Ausgabe einer optionaler Quittung auf Papier oder als E-Mail sowie eine manuelle oder automatisierte Abmeldung (i.d.R. durch Wegnehmen des Benutzerausweises).\nRückgabeautomaten sind in zwei Gruppen zu unterteilen. Im einfacheren Fall ist der Ausleihautomat auch gleichzeitig ein Rückgabeautomat. Der Anmeldeprozess kann hier entfallen, denn es ist grundsätzlich nicht relevant, wer das Medium zurückgibt. Die zurückgebrachten Medien werden dann durch die Nutzer*innen selbst auf einen Bücherwagen oder ein separates Regal gestellt und müssen anschließend durch das Bibliothekspersonal sortiert und eingestellt werden.\nDer zweite und komplexere Fall ist ein Rückgabeautomat mit einem Eingabeschacht und dahinterliegendem Förderband, auf dem die Medien weiter transportiert, verbucht und gesichert werden. Im einfachen Fall werden die Medien in einem Behälter gesammelt oder es ist ein automatisches Sortiersystem angebunden. Medien, die nicht zum Bestand der Bibliothek gehören und andere Gegenstände müssen bei der Rückgabe erkannt und zurückgewiesen werden.\nFernleihautomaten dienen der personalfreien Abholung von bestellten Fernleihmedien oder anderen bestellten Dingen, die nicht durch die Ausleihautomaten verbucht werden können, etwa weil sie keinen RFID-Tag besitzen. Nach dem Einlegen des abzuholenden Mediums durch Mitarbeiter*innen der Bibliothek werden Bestellende informiert (z.B. durch eine E-Mail), dass in einem Fach x etwas abzuholen sei. In diesem Vorgang kann man den Bestellenden auch ein PIN mitteilen, alternativ ist das Fach durch einen Bibliotheksausweis elektronisch zu öffnen. Wird das Fach geöffnet, wird zeitgleich das Medium im Nutzerkonto verbucht.\nKassenautomaten erlauben die Bezahlung von offenen Gebühren oder auch den Kauf von Gutscheinen/Tickets für Dienstleistungen, die im Anschluss in Anspruch genommen werden.\n\n\nMediensicherung\nMit dem Begriff wird die Methode beschrieben, am Ausgang der Bibliothek mittels einer Art von Schleuse (auch Gate genannt) Medien und ggf. auch Dinge des Interieurs zu detektieren, die gesichert und insofern unverbucht sind. Eine absolute Absicherung gegen Verluste ist mit dieser Methode nicht zu erreichen. Der vornehmliche Einsatzzweck ist daher auch weniger die Verhinderung von vorsätzlichem Diebstahl, sondern vielmehr das Entdecken des versehentlichen Vergessens der Ausleihverbuchung.\nIm Fall von EM-Sicherung oder HF-RFID (siehe [RFID]) werden zur Erkennung ungesicherter Medien Gates aufgestellt, durch die Menschen beim Verlassen der Bibliothek geleitet werden. In diesen Gates sind die entsprechende Detektionstechnologie sowie Alarmsysteme (Ton und/oder Licht) verbaut. Im Fall von UHF-RFID kann man auf Gates verzichten, hier genügen wegen der großen Reichweite auch Antennen, die an der Decke montiert sind. Für die Anzeige der detektierten Medien kann auch ein Monitor verwendet werden, auf dem dann etwa gleich das entsprechende Buchcover angezeigt wird.\nDie Sicherungsanlagen können je nach Konfiguration auch als Besucherzähler genutzt werden. Die Aussagekraft der Zahlen ist zwar nicht exakt, aber hilfreich genug für eine Ermittlung der Auslastung.\n\n\nBezahlung\nIn Bibliotheken fallen an verschiedenen Stellen Gebühren oder andere zu zahlende Beträge an. Diese werden einerseits elektronisch erzeugt und z.B. im Bibliothekssystem gespeichert (z.B. Überziehungsgebühren) oder fallen andererseits direkt an (z.B. Verkaufspreise für Dubletten, Tragetaschen …). Benötigt werden dafür Geräte, die bei Bezahlung den Betrag direkt in dem System verbuchen können, in dem die Beträge erfasst sind, z.B. Kassenautomaten mit Anbindung an das Bibliothekssystem. An diesen Automaten können sich Nutzer*innen anmelden, erhalten eine Anzeige von offenen Posten und können sie direkt begleichen. Manche Bibliotheken erlauben lediglich die bargeldlose Zahlung, manche ermöglichen (auch) die Zahlung mit Bargeld. Unterschiedlich gehandhabt wird auch, ob die gesamte geschuldete Summe zu bezahlen ist oder ob einzelne Posten beglichen werden können. Mit dem Einsatz von Kassenautomaten können Bezahlvorgänge unabhängig von anwesendem Personal ermöglicht werden.\nAuch die Verbindung eines Bezahlterminals mit einem Ausleihautomaten ist möglich. Hier sind dann alle Prozesse für die Nutzer*innen an einem Gerät abwickelbar. Es sind Ausleihautomaten am Markt, die bargeldlose Bezahlung mit verschiedenen Bezahlsystemen (spezielle Debit-Karten, ec-Karten …) und/oder Bargeldzahlungen ermöglichen.\nDie Systeme für Bezahlung und Gebühren sind meist an die jeweiligen Finanzstellen einer Universität oder einer Stadt(verwaltung) angebunden. In Universitäten kann auch das angeschlossene Studierendenwerk als Clearingstelle verwendet werden.\n\n\nZugangskontrolle\nEine Zugangskontrolle zur Bibliothek ist relevant, wenn Nutzer*innen die Bibliothek oder Teile davon auch ohne Anwesenheit von Bibliothekspersonal vor Ort nutzen können sollen (Open Library). Im Regelfall sieht eine technische Umsetzung der Zugangskontrolle so aus, dass das Schließsystem des Gebäudes an das Identifikationssystem für die Nutzer*innen angebunden ist. Dies lässt sich beispielsweise mit Kartenlesegeräten am Eingang oder mit einem RFID-Terminal lösen, welches die Bibliotheksausweise einlesen und dem Schließsystem nach festgelegten Regeln mitteilen kann, ob die betreffende Person zur Nutzung der Ressource berechtigt ist. Die Nutzung sollte außerdem kurzzeitig im Rahmen des Datenschutzes protokolliert werden.\n\n\nAuslastungszählung\nDie Ermittlung der Auslastung einer Einrichtung ist aus verschiedenen Gründen interessant: für statistische Zwecke, für eine Anzeige auf der Webseite als Service für Nutzer*innen oder um im Notfall die ungefähre Anzahl der im Gebäude anwesenden Personen zu erfahren.\nBei der Auswahl einer geeigneten Lösung muss zuerst festgelegt werden, ob eine exakte Zählung nötig ist, oder eine Approximation der Anwesenheitszahlen ausreichend ist.\nFolgende Umsetzungsmöglichkeiten sind für eine exakte Zählung in Erwägung zu ziehen: Auswertung der Zugangskontrollsysteme; Einsatz einer Drehschranke bzw. Vereinzelungsanlage, die nur Einzelpersonen durchlässt; Zählung durch Personal und Zählung durch Kameras mit CV-Software (computer vision). Andere, relativ exakte, aber doch mit kleinen Ungenauigkeiten bei der Zählung behaftete Systeme sind die Zählung durch eine Lichtschranke, durch ein Radar etwa von einem Sicherungsgate (siehe Zugangskontrolle) und IR-Durchgangszähler mit Gruppenerkennung an der Decke montiert (bspw. Produkte der Firma Irisys).\nBei allen exakten Zählsystemen ist insbesondere die Frage des Datenschutzes zu beachten, da aus der Zählung die Nachverfolgung der Nutzer*innen nicht abgeleitet werden darf. Aus Datenschutzgründen ist auch der Einsatz von aufzeichnenden (Kamera-)Systemen im öffentlichen Raum immer sorgfältig abzuwägen.\nFür die approximative Auslastungszählung eignen sich neben anonymisierten Varianten der genannten Möglichkeiten insbesondere auch die Auslastungsmessung von anderen Infrastruktursystemen, etwa dem öffentlichen WLAN. Wenn davon ausgegangen werden kann, dass ein*e Nutzer*in (im Durchschnitt) ein Gerät im öffentlichen WLAN anmeldet, dann kann dies als guter Indikator dienen. Sollen die Zahlen der im WLAN angemeldeten Geräte zur Messung der Auslastung eines Gebäudes dienen, sind allerdings umfangreiche Justierungen notwendig. So zeigte sich z.B. an der UB Dortmund, dass zu Prüfungszeiten die Studierenden in der UB pro Person jeweils mehrere Endgeräte im WLAN nutzten – vermutlich mehrheitlich Smartphone und Notebook.\nEine andere Möglichkeit ist die Auswertung der Nutzung von Arbeitsplätzen, etwa der Anteil gerade aktiv genutzter öffentlicher PCs, die Messung von Temperaturabweichungen von Innenräumen, oder die Messung der Lautstärke. In Luzern wurde erfolgreich mit Sensoren in Sitzen bzw. unter Tischplatten experimentiert.\n\n\nVor-Ort-Verlängerung\nDie Vor-Ort-Verlängerung ist ein Angebot der Bibliothek für Mitarbeiter*innen der zugehörigen Institution, also z.B. der Hochschule oder der Verwaltung. Hierbei wird der ausgeliehene Bestand nicht in der Bibliothek, sondern bei Nutzer*innen vor Ort, also im Büro, im Labor etc. erfasst und die entsprechenden Leihfristen verlängert. Technische Hilfsmittel wie RFID und an das BMS angebundene Schnittstellen erleichtern diesen Prozess.\n\n\nOpen Library\nUnter Open Library werden zwei verschiedene Phänomene zusammengefasst: einerseits ist die Open Library ein offenes Projekt zum Aufbau einer digitalen Bibliothek von frei verfügbaren digitalen Büchern oder Digitalisaten. Andererseits bezeichnet der Begriff den Zugang zu einer Bibliothek durch technische Infrastruktur ganz ohne oder mit wenigen personellen Ressourcen. Einige Bibliotheken bieten auf diese Weise einen durchgehenden Zugang, andere Bibliotheken nutzen die Möglichkeiten, um ihre Öffnungszeiten zu erweitern.\nZugang erhalten in diesen Fällen ausschließlich autorisierte Benutzer*innen innerhalb und außerhalb der Öffnungszeiten der Bibliothek. In den Eingangsbereichen wird über Lesegeräte die Zugangsberechtigung geprüft. Hier kann es Altersbeschränkungen geben und auch ein gesperrter Bibliotheksausweis gewährt keinen Einlass.\nBuchsicherungsanlagen dienen dem Diebstahlschutz von gesicherten Medien und speichern Vorfälle per Video oder verständigen einen Wachdienst. Vorgemerkte Medien liegen in Vormerkregalen, teilweise auch sogenannten intelligenten Abholregalen oder Automaten bereit. Da das Medium auf ein*e Nutzer*in vorgemerkt ist, kann es nur von diesen ausgeliehen werden.\nEine Ausleihe erfolgt durch die Benutzung der Ausleihautomaten. Ausleihregeln sind im Bibliotheksmanagementsystem hinterlegt, werden bei der Verbuchung über den Automaten geprüft und das Medium entsprechend entsichert.\nDie Open Library ermöglicht zwar grundsätzlich den Zugang zu allen Medien, aber es gibt auch immer Teile des Bestands, die von der Selbstausleihe ausgeschlossen sind, z.B. Brettspiele, Tageszeitungen und ausleihbare Geräte.\n\n\nNavigation und Lokalisierung\nDamit Nutzer*innen die im OPAC bzw. Discovery-System gefundenen (physischen) Medien auch nutzen können, müssen sie den entsprechenden Regalstandort aufsuchen. Zur Orientierung wird häufig die Aufstellungssystematik der Medien genutzt und im Rechercheergebnis steht, unter welcher Signatur, in welchem Regal, in welcher Etage ein Buch zu finden ist.\nIn kleineren Bibliotheken kann eine solche Standortangabe zusammen mit der Beschilderung des Gebäudes vor Ort ausreichen, um die Medien zu lokalisieren. Je größer jedoch der Bestand und die Räume einer Bibliothek sind, desto schwieriger ist diese Aufgabe.\nFür das einfachere Auffinden von Regalstandorten gibt es verschiedene Visualisierungsmöglichkeiten. Im einfachsten Fall kann der genaue oder ungefähre Standort eines Mediums auf einer statischen oder sogar interaktiven Karte angezeigt werden. Eine solche Karte könnte hierbei direkt beim Suchtreffer im Katalog angezeigt werden, oder als dediziertes Terminal vor Ort vorhanden sein. Anhand der Karte können sich Nutzer*innen dann zum Buch bewegen.\nTechnisch komplexer ist es nun, wenn eine solche Karte nicht nur den Standort des Buches, sondern gleichzeitig auch den aktuellen Live-Standort des/der Nutzer*in visualisieren soll. In einem solchen Fall von Indoor-Navigation, der etwa einer Google-Maps Karte gleicht, muss zusätzlich technischer Aufwand betrieben werden, um den aktuellen Nutzerstandort zu ermitteln. Im Gegensatz zu Karten und Navigationslösungen in Automobilen oder unterwegs mit dem Smartphone, kann allerdings innerhalb von Gebäuden nicht auf GPS-Satelliten zur Positionsbestimmung zurückgegriffen werden, sondern es müssen andere Signale oder Ortungspunkte genutzt werden, um den Standort der Person im Raum zu ermitteln.\nMögliche Ansätze sind hierbei etwa sog. Beacons, die mit Funklösungen wie Bluetooth und Triangulation die Position eines Mobiltelefons ermitteln, oder bildbasierte (AR - augmented reality) Lösungen, die über die Smartphonekamera bestimmte Objekte oder Marker im Raum erkennen.\nDie Standorte von physischen Medien sind über eine Zuordnung von Signaturen zu Regalen, teilweise sogar auch zu einzelnen Regalbrettern, einfach möglich. Dies bedingt allerdings eine systematische Aufstellung, die in Zeiten von mehrheitlich elektronischen Medien und immer weniger Printmedien nicht mehr hilfreich ist. So sollte ursprünglich eine Systematik am Regal den Bestand zu einem bestimmten Thema abbilden, man sollte über die benachbarten Bücher einen Überblick zu einem Thema bekommen können. Da aber bspw. E-Books nicht im Regal auftauchen, stellt sich die Frage nach der Aufstellung von gedruckten Medien in den Lesesälen neu.\nEine Alternative zur systematischen Aufstellung kann die dynamische Aufstellung sein. So kann man z.B. ohne Umsignierungen temporäre Sammlungen bilden. Man kann darüber nachdenken, ob die Nutzer*innen ausgeliehene Bücher selbst zurückgeben können, indem sie sie einfach an einen freien Platz im Regal stellen. Man kann die Nutzer*innen die früher so ungeliebten „Nester“, also die Konzentration von Büchern an einer beliebigen nicht systematischen Stelle bilden lassen, sodass vielleicht auch andere von dieser „eigenen“ Systematik profitieren. Dies bedingt natürlich eine technische Lösung, mittels der die Bücher anschließend auch wiedergefunden werden, also irgendwie geortet werden können (siehe auch der Abschnitt Revision).\n\n\nSortiersysteme\nMedien-Sortieranlagen sind in der Regel direkt an den Rückgabeautomaten angeschlossen. Nutzer*innen legen Medien in einen Schacht, in dem die Medien erfasst, zurückgebucht und gleichzeitig gesichert werden. Fließbänder oder Rollen transportieren das Medium entsprechend vorher definierter Ziele, z.B. bestimmte Signaturbereiche, andere Zweigstellen oder vorgemerkte Medien.\nDie Anschaffung einer Sortieranlage ist mit hohen Kosten verbunden, sowohl für die eigentliche Anschaffung und Installation als auch für die Wartung. Eine Kosten-Nutzen-Analyse (Anzahl Ausleihen/Rückgaben, Personaleinsatz, Platzbedarf) sollte daher vor der Entscheidungsfindung unbedingt durchgeführt werden. Insbesondere in öffentlichen Bibliotheken sollte außerdem immer bedacht werden, dass nicht alle Medien über das Sortiersystem zurückgenommen werden können (siehe Medienarten). Auch sehr kleine, leichte oder besonders große, schwere Medienarten müssen i.d.R. gesondert zurückgebucht werden.\n\n\nRevision\nDie Aufstellung der Medien in einer Systematik ist die Grundlage des Auffindens dieser Medien. Durch die übliche Zirkulation (Ausleihe, Rückgabe, Wiedereinstellen) und selbst schon beim Stöbern vor Ort ist die korrekte Aufstellung latent gefährdet – es entsteht Unordnung.\nTechnische Mittel zur Unterstützung bzw. Vereinfachung von Inventur und Revision sind daher gern verwendete Hilfen bei der Kontrolle oder Wiederherstellung der korrekten Aufstellung.\nFür die Auswahl von geeigneten Werkzeugen können zwei Fragestellungen herangezogen werden:\n\nIst das (nicht ausgeliehene) Medium im Haus?\nIst das (nicht ausgeliehene) Medium am richtigen Platz?\n\nDie zweite Frage ist eine Konkretisierung der ersten und bereits ein Hinweis auf die unterschiedliche Leistungsfähigkeit der Hilfsmittel bzw. auf das geeignete Vorgehen bei Inventur und Revision.\nDas klassische Vorgehen ist das genaue Auffinden und Prüfen der Medien mit Hilfe einer Liste. Dies ermöglicht im besten Fall die Wiederherstellung der korrekten Aufstellung. Durch Digitalisierung der Liste und, sofern vorhanden, eine Anreicherung mit Cover-Bildern, kann eine Beschleunigung dieses Prozesses erreicht werden. Mobile Geräte wie Tablets mit geeigneten Apps oder Laptops auf Rollwagen beschleunigen den Prozess nochmals.\nRFID kann bei der Beantwortung der ersten Frage, ob Medien generell im Haus sind, sehr gut eingesetzt werden. Für die zweite Frage, ob Medien am richtigen Platz und auch noch in der richtigen Reihenfolge stehen, ist der Einsatz von RFID aufgrund der räumlichen Auflösung jedoch nur bis zu einem gewissen Grad geeignet. Hierbei ist die verwendete RFID Frequenz ein entscheidender Faktor. Durch die Lesereichweite von 0 bis ca. 35 cm bei der HF-Frequenz ist HF zwar gut geeignet, um ein konkretes Medium in einem kleinen Suchfeld zu finden. Durch Grenzen beim gleichzeitigen Lesen von vielen HF-Transpondern, die sich – wie in Regalen üblicherweise der Fall – nah beieinander befinden, ist es jedoch praktisch nicht besonders hilfreich.\nEine elektronisch unterstützte Inventur in HF-Bibliotheken kann auf zwei Arten durchgeführt werden. Die erste Möglichkeit ist das „Entlangwandern“ an den Regalen mit einer Antenne, die an den Buchrücken entlanggeführt wird. Die Genauigkeit dieser Art der Inventur ist bei weitem nicht absolut, da die Lesegenauigkeit stark vom Abstand/Winkel von Transponder zu Lesegerät abhängt (idealerweise sind Antenne und Transponder parallel ausgerichtet). Zudem stören Metallgegenstände (z.B. Metallregale) den Empfang.\nDie zweite Möglichkeit ist der Einsatz sogenannter Smart-Shelves. In diesen sind aktive RFID-Komponenten verbaut, die die Detektion der auf ihnen befindlichen Medien erlauben. Nachteilig sind hier die Kosten der Regale, die umgerechnet bei mehreren Euro pro detektiertem Medium liegen. Ein großes Manko bei den oben genannten Inventursystemen ist zudem die mangelnde Integration mit dem BMS bzw. eine ausgereifte Benutzungsoberfläche.\nAnders sieht es im UHF-Frequenzbereich aus. Durch die potentielle große Lesereichweite von bis zu ca. 12 Metern bei praktisch keiner Beschränkung der Anzahl der Transponder im Erfassungsfeld ist die Frage der prinzipiellen Präsenz eines Mediums gut zu beantworten. Gleichzeitig ist durch die große Lesereichweite die korrekte Aufstellung an einer konkreten Stelle nur durch einen hohen technischen Aufwand automatisiert zu prüfen.\nScan-Roboter, die regelmäßig den Bestand überprüfen (beispielsweise an der TU Dortmund), können derzeit mit einer Genauigkeit von ca. 1 Kubikmeter den Standort eines Transponders bestimmen. Die zur Zeit im Einsatz befindlichen Roboter der Firma Metralabs aus Ilmenau scannen den Bestand in sogenannten „Runs“, die etwa 30.000 Tags pro Stunde finden. Nach einem abgeschlossenen Run werden die Ergebnisse in einer CSV-Datei über einen Fileshare oder einfach per E-Mail geliefert. Enthalten sind darin u.a. RFID-Tag-Inhalt und x,y,z-Koordinaten ausgehend von einem einmal festgelegten Nullpunkt. Diese Informationen können (und müssen) dann weiterverarbeitet werden. Denkbar ist eine Integration in den Katalog, sodass in einem Lageplan der Standort des Mediums angezeigt wird, sobald man darauf klickt. Eine Inventur über alle Daten ist möglich, wenn man die Daten mit dem ausgeliehenen Bestand abgleicht und die Fehlmenge ausweist. Eine Stellrevision ist etwas aufwändiger, da der Bestand dazu über längere Zeit überwacht werden muss und Abweichungen bestimmt werden müssen – wenn der genaue Soll-Standort jedes Mediums nicht in einer Tabelle erfasst ist, mit der die aktuelle Roboterdaten verglichen werden können.\nFestzuhalten ist, dass auch die von den Robotern erhobenen Daten keine absolute Genauigkeit haben, sondern immer etwas „Schwund“ enthalten. Die Gründe für die Ungenauigkeit sind vielfältig: Reflexionen durch Metall-Anhäufungen im Suchfeld, verstellte Gänge zwischen den Regalen, die die Roboter am Durchfahren und Scannen hindern, sehr eng zusammenstehende Bücher, die verhindern, dass beide Tags gescannt werden usw. Mit einer Fehlerrate von ca. 1% ist beim Robotereinsatz zu rechnen.\n\n\nSmart Library\nAngelehnt an neologistische Komposita wie „smart home“, „smart industries“ (zu Deutsch ‚Industrie 4.0‘) oder „smart cities“ hielt ebenfalls die Kombination „smart“ und „library“ Einzug in die Welt der Informationseinrichtungen. Einzelne Produkte wie „smart shelves“ warben mit dem Label für neue Dienstleistungen über den Einsatz von RFID-Technologie. Insgesamt bezieht man das Konzept der „smart library“ auf Bibliotheken, die Informationstechnologien im Rahmen der Digitalisierung und Automatisierung auch in neuen Bereichen wie physischen Räumen, Nachhaltigkeit und Vernetzung einsetzen (Freyberg und Wolf 2019; Seeliger 2019).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html#rfid",
    "href": "infrastruktur.html#rfid",
    "title": "Technische Infrastruktur",
    "section": "Objektidentifikation physischer Medien mit RFID",
    "text": "Objektidentifikation physischer Medien mit RFID\nDer folgende Abschnitt gibt einen tieferen Einblick in die Thematik „RFID“, die für viele Teile der allgemeinen Infrastruktur (Verbuchung, Mediensicherung, Sortiersysteme …) relevant ist. Insbesondere im Hinblick auf Fragestellungen einer Migration oder Einführung von RFID-basierten Technologien für einen Bibliotheksbestand werden Grundlagen und Hintergründe erläutert, Speicherung und Datenmodelle diskutiert und Unterschiede zwischen Verschiedenen RFID-Technologien aufgezeigt. Weitere Informationen zum Einsatz von RFID in Bibliotheken siehe Seeliger (2014) und Kern (2011).\n\nRFID-Grundlagen\nRFID (radio frequency identification) ist eine Technologie, um Objekte mittels Funksignalen zu erfassen und zu orten. Die Objekte werden hierbei durch RFID-Tags (auch Etiketten oder Transponder genannt) markiert. Ein RFID-Tag enthält zumeist einen Schaltkreis (Speicher + Prozessor) und eine Antenne. Um mit einem RFID-Tag zu kommunizieren, wird ein Sende-/Empfangsgerät benötigt („RFID-Reader“), welches die Antenne über Funkwellen mit Strom versorgt („Induktion“) und danach mit dem Schaltkreis kommuniziert. Die meisten RFID-Tags sind „passiv“, da sie über die Antenne aus der Ferne mit Strom versorgt werden. Es existieren aber auch „aktive“ Tags, die eine Batterie enthalten. Aufgrund der dafür notwendigen Größe sowie der Kosten sind aktive Tags in Bibliotheken nicht im Einsatz. Die Kosten für passive Tags bewegen sich im Bereich von wenigen Cent pro Stück, natürlich stark abhängig von der Bestellmenge.\n„RFID“ ist nicht gleich „RFID“! Die Identifikation mittels Radiofrequenzen wird in verschiedenen Frequenzbereichen betrieben. Die Frequenzbereiche sind untereinander inkompatibel, Tags sind nur in einem Frequenzbereich betreibbar, die Schreib-/Lesegeräte sind ebenfalls dediziert auf den Frequenzbereich. Selten gibt es Geräte, die mehrere Frequenzbereiche lesen können, diese enthalten beide notwendigen Technologien. In europäischen Bibliotheken ist meistens ein sogenanntes RFID-HF im Einsatz, welches im Frequenzbereich von 3-30 MHz arbeitet. In der Industrie, in Bibliotheken in China und USA sowie in einzelnen Bibliotheken in Europa kommt RFID-UHF mit einer Frequenz von 868 MHz in Europa (andere Frequenzbereiche in anderen Gebieten) zum Einsatz. Der Hauptunterschied in der Anwendung zwischen den beiden Frequenzbereichen ist die Schreib-/Lesereichweite (siehe UHF oder HF?).\nNeben dem Beschreiben und Auslesen des Speicherchips aus der Ferne ist auch eine mehr oder minder präzise Ortung der Tags möglich. Die Ortung ist mit Ungenauigkeiten versehen, da in dem Frequenzbereich von RFID-UHF Reflexionen der Normalfall sind. Eine Lokalisierung ist also nur über die Kombination verschiedener Verfahren in sehr vielen Messungen und einiger Statistiken möglich und dennoch mit einer Unsicherheit versehen.\nNeben Bibliotheken finden sich RFID-Tags im Alltag auch oft im Einzelhandel, hier meist als Diebstahlsicherung, zur Inventur oder auch zum Erkennen der Artikel an der Kasse. Auch hier kommt bei neueren Installationen häufig RFID-UHF zum Einsatz. Kontaktlose Zahlungsmöglichkeiten auf Bankkarten (NFC) sind ebenfalls eine Anwendungsform von RFID.\n\n\nUHF oder HF, was sind Tags?\nIm deutschsprachigen Raum ist zur Zeit (2022) der Einsatz von RFID-HF verbreitet. Einzelne Bibliotheken setzen auch schon RFID-UHF ein (Beispiele: Bibliothek der Wirtschaftsuniversität Wien oder auch die Universitätsbibliothek Dortmund sowie die Bibliothek der Burg Giebichenstein Kunsthochschule Halle). In der Industrie ist RFID-HF praktisch nicht im Einsatz, was vermuten lässt, dass diese Technologie in einigen Jahren aussterben könnte. Die größere Reichweite von RFID-UHF ermöglicht zum Beispiel eine automatisierte Inventur / Stellrevision (siehe auch Abschnitt Revision). Auf der anderen Seite muss man die großen Reichweiten auch einschränken, um etwa bei Ausleihautomaten nicht das zu verbuchen, was sich in einem größeren Abstand befindet. Dies erfordert Kenntnisse in der Justage der Leistung und Signalstärke der eingesetzten Antennen (Dortmund) oder Abschirmung derselben (Wien).\nAn Personalplätzen können auch die aus Metall bestehenden Verstärkungen unter den Tischen zu großen Störungen führen und es sollten daher unbedingt abgeschirmte Antennen eingesetzt werden.\nEin sichtbarer Unterschied der Transponder ist bedingt durch die beiden Funkfrequenzen HF (High Frequency: 13,56 MHz) und UHF (Ultra High Frequency: 860 bis 960 MHz). Höhere Frequenzen erfordern eine andere Bauform der Empfangsantennen in den Transpondern. HF Transponder haben durchschnittlich die Ausmaße einer Chipkarte, damit auf einer Fläche von ca. 10 bis 20 Quadratzentimetern eine spiralförmige Antenne in einem quadratischen oder rechteckigen Layout untergebracht werden kann. In der Mitte der Spirale ist der Mikrochip platziert. Je größer die Antennen sowohl in den Transpondern als auch in den Antennen der Geräte sind, desto besser funktioniert der Kommunikationsprozess.\nUHF Transponder hingegen haben üblicherweise Antennen in einer eher länglichen Bauform, bei der zwei symmetrische Antennenteile von einem Zentrum, in dem der Mikrochip platziert ist, in gegenüberliegende Richtungen zeigen. Hier ist weniger die Größe, sondern das für den jeweiligen Anwendungsfall am besten geeignete, konkrete Layout der Antennen für eine zuverlässige Kommunikation wichtig. Die Ausmaße eines typischerweise in Bibliotheken eingesetzten UHF RFID Tags sind ca 15 mm × 94 mm, es gibt aber auch kleinere Ausführungen.\nEs existieren ebenfalls Transponder, die sowohl mit UHF, als auch mit HF ansprechbar sind, was die Kosten pro Transponder deutlich erhöht, da zwei verschiedene Antennen vorhanden sein müssen. Beide Frequenzen gleichzeitig in einer Bibliothek zu verwenden, bringt keinen Vorteil, der diesen Aufwand rechtfertigen würde.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 2.1: HF Transponder und UHF Transponder\n\n\n\nDas unterschiedliche Layout der Transponder ist für das Einkleben in Büchern wenig relevant. Beide Arten werden bei der Herstellung auf Folie aufgebracht. Man unterscheidet verschiedene Ausprägungen der Weiterverarbeitung:\n„dry naked“ bezeichnet einen fortlaufenden Folienstreifen, auf dem Chips und Antennen in gewissen Abständen aufgebracht sind. Hier gibt es keine Stanzung und keine Klebefläche.\n„wet inlay“ hingegen bezeichnet eine Weiterverarbeitung, die Klebemasse auf Trägerfolie enthält und gestanzt ist.\nWeitere Verarbeitung sorgt auf Wunsch für Papieroberfläche, sodass die Transponder auch bedruckt werden können. Hersteller von RFID-Transpondern (in der klebenden Version auch „Tags“ genannt) liefern oft nur „dry naked“, weiter verarbeitete Tags beschafft man von sogenannten „Konvertierern“ (nicht zu verwechseln mit dem manchmal so genannten Prozess der „Konvertierung“ des Medienidentifikators von Barcode zu RFID). Der verwendete Klebstoff genügt keiner hohen Anforderung, die in Bibliotheken mit seltenen Werken denkbar wäre. Langzeithaftung oder garantierte Unschädlichkeit des Klebstoffes für das Papiermaterial sind kein Thema für die Hersteller und nur für sehr viel Geld bei speziellen Konvertierern zu bekommen.\nRelevant für die Sichtbarkeit der technischen Komponenten in einer Bibliothek ist jedoch die Gegenseite der Transponder: die Schreib-/Lese- Geräte bzw. genauer gesagt deren Antennen. Die maximale Lesereichweite wird bei HF maßgeblich durch die Größe der Antennen, bei UHF hingegen hauptsächlich durch die Feldstärke bestimmt. Dementsprechend haben HF Antennen, die für die Buchsicherung beispielsweise an Ausgängen verwendet werden, die ungefähre Größe einer mittelgroßen Person und es werden üblicherweise zwei sich gegenüberstehende Antennen zu einem „Gate“ kombiniert, nicht zuletzt um die Leseentfernung auf ca. 80-100 cm zu verdoppeln und der Kontrolle der Transponder beim Passieren einer Person durch das Gate zu ermöglichen. Ein solches Gate ist sichtbar, muss in die Architektur des Eingangsbereiches einer Bibliothek integriert werden und kann ggf. auch ein Hindernis darstellen (Rollstuhlfahrer, Fluchtwege). Für andere Anwendungen ist der Größenunterschied der Antennen der beiden Frequenzbereiche weniger relevant, wenn auch nicht komplett zu vernachlässigen: Eine Inventur mit HF bedingt eine gewisse Nähe der Sende/Empfangsantenne zu den Medien, ansonsten müssten die Antennen Ausmaße von mehreren Metern haben.\nBei UHF hingegen werden auch Medien in größerer Entfernung erkannt (siehe auch im Abschnitt Revision). Typische Bibliotheks-Vorgänge wie die Ausleihe oder Rückgabe, egal ob an einer Theke oder an einer Selbstbedienungsstation, erfordern eine Lesereichweite von 0 bis maximal ca. 30 cm. Da sich bei beiden Frequenzen für diese Entfernung eine in etwa ähnliche Größe der Antennen ergibt, ist bei diesen Anwendungen der optische Unterschied der Geräte nicht sehr offensichtlich. Bedeutsam für diese Anwendungen ist allerdings die Anzahl der gleichzeitig im Stapel gut lesbaren Medien: bei HF sind es unter optimalen Voraussetzungen ca. 3 bis 8 Medien (je nach Größe bzw. Dicke), bei UHF ist diese Zahl nicht wirklich beschränkt. Beispielsweise werden im Extremfall problemlos 20 bis 30 Medien in einem Stapel gleichzeitig erkannt.\nUHF und HF unterscheiden sich außerdem durch den Speicherplatz, der auf dem Chip des Tags vorhanden ist. Faustformel ist: Je weiter gefunkt werden muss, desto energieintensiver ist der Prozess und desto weniger kann übertragen/gespeichert werden. Demzufolge ist auf einem UHF Tag im Regelfall weniger Platz als auf einem HF Tag. So wenig sogar, dass bei vielen üblichen Tags nicht beliebig lange Informationen auf einen Tag geschrieben werden können, sondern sich explizit darüber Gedanken machen muss, welche Daten gespeichert werden sollen. Es existieren allerdings auch spezielle Tags mit großem Speicher, auf denen man etwa das in Bibliotheken weit verbreitete sogenannte Dänische Datenmodell finden kann.\nRFID-Tags können in verschiedenen Dingen eingebracht werden, so gibt es Tags in Büchern, Schlüsselanhängern sowie auch mobile devices (NFC, Smartphones). Nicht alle Arten sind untereinander kompatibel, so kann man mit NFC in Smartphones zwar RFID-HF-Tags in Bibliotheken auslesen, nicht aber RFID-UHF-Tags.\nZu der Lebensdauer der Tags lässt sich bisher recht wenig sagen – was auch eine gute Nachricht ist, da sich im Zeitraum des Praxiseinsatzes von ca. 20 Jahren bisher keine nennenswerte Abnutzung der elektronischen Eigenschaften der Tags gezeigt hat. Den Klebstoff betreffend, mit dem die Tags in die Medien eingeklebt werden, gilt in Bibliotheken Altbekanntes. Schädigung von Papier und Langzeithaltbarkeit dieser Klebstoffe ist im Zusammenhang mit Barcodeetiketten gut erforscht (Kern 2011).\n\n\nMigration RFID\nDie Migration von einer Technologie zur anderen führt unweigerlich zu drei Fragen:\n\nAuf welche Technologie soll ich migrieren?\nWas genau muss migriert werden?\nWie führe ich den Prozess der Migration durch?\n\nIn den letzten Abschnitten schon angeklungen ist der Unterschied von HF und UHF, wodurch sich in den meisten Fällen die Frage der Technologiewahl als erste stellt. Als Faustregel kann hier festgestellt werden, dass die schon langjährige Verbreitung von HF im DACH-Raum (DACH == D-A-CH == Deutschland - Österreich - Schweiz) dazu geführt hat, dass einige Anbieter mit Standardlösungen am Markt sind und die meisten Anwendungsfälle abgedeckt sind, inkl. dem Migrationspfad von Barcodes.\nBei der Migration an sich geht es dann zumeist darum, das Identifikationsmerkmal eines Mediums auf einem RFID-Tag zu speichern. Hier existieren mehrere Ansätze: Es kann ein RFID-Tag „von der Stange“ ohne Beschreiben in ein Buch eingebracht werden, dessen eindeutiger Identifikator ausgelesen und – zumeist im BMS – mit dem jeweiligen Medium verknüpft werden. Eine andere Möglichkeit wäre das Beschreiben des RFID-Tags mit einem Identifikator, etwa der Mediennummer, einer eindeutigen Signatur oder einer komplexeren Datenstruktur wie dem Dänischen Datenmodell (siehe folgender Abschnitt).\nSchließlich muss der Prozess der Migration gewählt und durchgeführt werden. Das Einkleben und zumindest Auslesen, ggf. aber auch Beschreiben eines RFID-Tags ist eine Aufgabe, die entweder von der Belegschaft einer Bibliothek oder als eingekaufte Dienstleistung durchgeführt werden kann. Erfahrungen zeigen, dass die eingekaufte Dienstleistung in drastisch kürzerer Zeit mit der Aufgabe fertig wird, als wenn dafür nur eigenes Personal eingesetzt wird. Eine typische Zeit für das Taggen eines Buches mit außen angebrachtem Barcode (also Einkleben eines leeren RFID-Tags, Auslesen des Barcodes, Schreiben der Infos des Barcodes auf das RFID-Tag) sind 20 Sekunden inklusive Verbringungsarbeiten (Erfahrungswert der UB Dortmund, Mittelwert beim Taggen von 860.000 Medien an vier Standorten durch einen externen Dienstleister). Es bietet sich an, im gleichen Atemzug auch eine Inventur des Bestands durchzuführen.\nDer Wechsel von RFID-HF auf RFID-UHF ist noch nicht erprobt, sollte aber keine größeren Probleme darstellen als der Wechsel von Barcode zu RFID. Ein RFID-HF-Tag und ein RFID-UHF-Tag können nebeneinander in einem Buch geklebt und sicher ausgelesen werden. Einige Hersteller von RFID-Hardware bieten auch hybride Geräte an, die beide Arten von Tags auslesen können.\nEin Wechsel von RFID-UHF zu RFID-HF erscheint nicht sinnvoll und wird daher hier nicht betrachtet. Grundsätzlich gilt dabei aber das gleiche wie im vorherigen Absatz.\n\n\nDatenmodelle\nFür die Inhalte von HF-Tags in Bibliotheken wurde das sogenannte „Dänische Datenmodell“ spezifiziert, welches später in der ISO-Norm ISO 28560 Teil 3 aufgegangen ist. Generell wird das Datenmodell in der ISO 28560 spezifiziert. Sie enthält heute drei Teile mit folgendem Inhalt: ISO 28560-1 enthält eine Beschreibung vielfältiger, für Bibliotheken denkbarer Datenfelder. Dies sind neben der Mediennummer auch der Titel von Büchern und weitere Daten, welche eventuell offline verfügbar auf dem Chip sein sollten. Aus den Elementen kann für jedes Land ein „Profil“« zusammengestellt werden. ISO 28560 Teil 2 basiert wiederum auf ISO 15962 und den oben genannten OIDs (object identifier). Er wird in den angelsächsischen Ländern stark propagiert. In diesen Ländern sind bisher vorwiegend proprietäre Datenmodelle im Einsatz, das Dänische Modell ist kaum verbreitet. ISO 28560 Teil 3 entspricht zu fast hundert Prozent dem Dänischen Datenmodell. Es ist im Vergleich zum Teil 2 zwar fest kodiert, aber deutlich einfacher strukturiert Kern (2014).\nAls Referenz für das Datenmodell auf einem Tag dient die Norm ISO 28560 in drei Teilen:\n\nTeil 1 beschreibt die erforderlichen und optionalen Datenfelder, die länderspezifisch in Profilen zusammengestellt werden können. Die Felder werden hierbei als OID bezeichnet.\nTeil 2 beschreibt die Anordnung dieser Felder (OIDs) in einem flexiblen bzw. fließenden Speicherlayout und wird primär in angelsächsischen Ländern angewandt.\nTeil 3 der ISO 28560 entspricht dem immer noch häufig im Sprachgebrauch genutzten Begriff des Dänischen Datenmodells und beschreibt die feste Struktur der Daten in verschiedenen anwendungsspezifischen Blöcken.\n\nDiese Norm sollte ursprünglich einerseits die Interoperabilität zwischen Bibliotheken ermöglichen. Es sollte also möglich sein, ein Buch einer fremden Bibliothek mit dem eigenen System zu verbuchen. In Deutschland oder in Österreich wird dies nicht flächendeckend (vorsichtig ausgedrückt) angewendet. In der Schweiz ist das hingegen üblich und wegen der zentralen Magazinbibliothek auch notwendig. Andererseits sollte eine Interoperabilität zwischen BMS und RFID-Infrastruktur ermöglicht werden, damit einzelne Komponenten des Systems problemlos ausgetauscht werden können, ohne die Funktionalität zu gefährden.\nDie Informationen, die im Dänischen Datenmodell in einem RFID-Tag gehalten werden, sind teilweise Daten, die auch schon im BMS über ein Medium vorgehalten werden. Das Dänische Datenmodell ist von seinem Format so komplex bzw umfangreich, dass es wegen des geringeren zur Verfügung stehenden Speichers nicht auf beliebige UHF-Tags geschrieben werden kann, sondern explizit UHF-Tags mit großen Speicher genutzt werden müssen. Der geringere Speicher von den meisten UHF-Tags erlaubt lediglich die Speicherung einer Bibliotheks-ID, einer ID des Mediums (auch „Mediennummer“ oder „Strichcode“ genannt) und natürlich eines Bits, welches den Sicherungsstatus speichert.\n\n\nExkurs: Speicherung von Informationen auf Strichcodes und RFID-Tags\nUm Informationen, wie etwa eine Mediennummer, eine Signatur oder eine komplexere Datenstruktur – wie das Dänische Datenmodell – auf einem Identifikationsmerkmal, wie einem Barcode, einem QR-Code oder einem RFID-Tag in maschinenlesbarer Form abzuspeichern, muss die Information entsprechend dem jeweiligen Zielformat encodiert werden. Im Folgenden wird beispielhaft für Barcodes und RFID-Tags erläutert, wie so etwas funktioniert.\nBarcodes sind weit verbreitet, beispielsweise auf Produkten im Supermarkt. Meistens steht unter einem Barcode im Klartext, welche Nummern- oder Buchstabenfolge sich hinter einem Barcode verbirgt. Allerdings ist nicht jeder Barcode wie ein anderer. Es gibt verschiedene Formate, die sich darin unterscheiden, durch welche Strich- und Leerplatzfolge jeweils einzelne Zeichen dargestellt werden. Je weniger Zeichen von einem Barcodeformat unterstützt werden sollen, desto weniger Striche werden pro Zeichen benötigt und desto kompakter wird der Barcode (siehe Strichcode in Wikipedia).\nAbbildung 2.2 stellt einen Barcode im Code39-Format dar, welches in Bibliotheken verbreitet ist und Zahlen, Großbuchstaben und ein paar wenige Sonderzeichen darstellen kann.\n\n\n\n\n\n\nAbbildung 2.2: Beispiel für einen Code39 Barcode\n\n\n\nQR-Codes funktionieren analog, erweitern allerdings die Darstellung um eine zweite Dimension, sodass auf weniger Platz mehr Informationen dargestellt werden können. Gleichzeitig enthalten QR-Codes eine Prüfsumme, die Fehler bei der Erkennung, etwa bei Beschädigungen am Code, ausgleichen können.\nEin RFID-Tag enthält einen Speicherchip und hält daher seine Informationen in Bits und Bytes, genau so, wie Daten und Dateien auch auf einem Computer gespeichert werden. Wie sich jeweils eine Information aus Bits und Bytes interpretieren lässt (ganz so, wie man aus den Balken eines Barcodes Buchstaben und Zahlen interpretiert), hängt von der jeweiligen Encodierung ab, wobei der Speicher eines Tags auch in unterschiedliche Encodierungsformate aufgeteilt werden kann, um Platz zu sparen.\nAbbildung 2.3 zeigt einen Speicherbereich, wie er auf dem Chip eines Tags vorkommen könnte. Die 32 Bit kodieren im Beispiel mit dem Hexadezimalwert 19E9B6EA die Zeichenfolge „DE2“ als Text für eine Bibliothekskennung und die Zahl 46826 als Medienkennung.\n\n\n\n\n\n\nAbbildung 2.3: Beispiel für einen Speicherbereich auf einem RFID-Tag\n\n\n\nIn Bibliotheken enthält das RFID-Tag auch ein sog. AFI-Bit (application family indicator), das zur Medien- bzw. Buchsicherung verwendet wird. Komplexere Datenmodelle, wie das Dänische Datenmodell, brauchen deutlich mehr Speicherplatz, um auf ein Tag zu passen.\n\n\nSteuerkommandos, Arten des Zugriffs\nRFID-Geräte kommunizieren mit RFID-Tags über verschiedene sog. Steuerkommandos. Analog zur Barcodes, die von einem Drucker gedruckt werden (WRITE) und von einer Lesepistole eingelesen werden (READ), so gibt es ähnliche Steuerkommandos auch für RFID-Tags, wobei diese entsprechende Kommunikation zwischen RFID-Gerät und Tag auslösen. Je nach Technologie (HF, UHF bzw. sogar nach technischer Spezifikation der Tags, SLI, SLI-S, SLI-X) existieren verschiedene Kommandos, die verschiedene Aktionen mit einem Tag auslösen. Ohne in technische Details zu gehen, lassen sich diese Kommandos grob wie folgt abstrahieren: READ, WRITE, SECURE, UNSECURE, KILL, PROTECT. Dies entspricht der natürlichen Interaktion mit den Tags etwa beim Inventarisieren neuer Bücher (WRITE), dem Verbuchen (READ, dann UNSECURE) oder dem Durchschreiten eines Gates (prüfen auf SECURED).\nBei der Nutzung von Bibliotheksgeräten, die RFID einsetzen, kommt man im Normalfall nicht mit den Steuerkommandos von RFID-Hardware in Berührung. Sollte man aber mit der Hardware selber experimentieren oder testen, wird man manchmal auch roh auf die Tags schreiben müssen.\n\n\nSicherheit und Datenschutz\nRFID-Tags können auf zwei Arten manipuliert werden: Einerseits kann man mittels geeigneter Hardware (im Falle von RFID-HF genügt ein Smartphone) den Inhalt eines nicht schreibgeschützten Tags verändern. Dies betrifft sowohl das Sicherungsbit (das Gate schlägt also nicht mehr an, wenn ein so manipuliertes Medium herausgetragen wird), als auch den gesamten Tag Inhalt, sodass das Medium von der Infrastruktur der Bibliothek nicht mehr verarbeitet werden kann. Andererseits kann man die meisten UHF-Tags und manche HF-Tags mit einem einfachen Befehl zerstören, also dauerhaft und endgültig stummschalten. Beide Arten der Manipulation kann man mit einem Passwortschutz wirkungsvoll verhindern (fun-fact: die meisten in Deutschland eingesetzten HF-Systeme enthalten diesen Passwortschutz nicht, sind also nicht vor einfachsten Manipulationen geschützt).\nBei der Einführung von RFID werden häufig Diskussionen zum Thema Datenschutz geführt. Wenn allerdings die Tags lediglich mit einer nur intern bekannten ID beschrieben werden, also einer ID, die nicht öffentlich im Katalog des BMS einsehbar ist, besteht diese Gefahr nicht. Selbst wenn jemand Medien im Rucksack eine*r Nutzer*in scannen würde (was technisch nicht unaufwändig ist), könnte man daraus keine Rückschlüsse auf das betreffende Medium schließen.\n\n\nAnbindung von RFID-Infrastruktur an BMS\nDamit ein Gerät einer RFID-Infrastruktur mit dem Rest der Systeme einer Bibliothek, insbesondere dem BMS kommunizieren kann, muss es über entsprechende Schnittstellen angebunden werden. Bei der Anschaffung sollte daher darauf geachtet werden, dass die Anlage etwa die standardisierten Schnittstellen wie SIP2 oder NCIP nutzt, um Dienstleistungen wie Rückgabe, Sortierung und Ausleihe mit dem Bibliothekssystem abwickeln zu können (Michaelis 2014).\nDie Anbindung von lokalen RFID-Readern an Computerarbeitsplätzen von Mitarbeiter*innen erfolgt im Regelfall durch das Anschließen eines solchen Gerätes direkt am Arbeitsplatz, zumeist über USB. Es existieren allerdings auch Reader, die über einen Netzwerkanschluss direkt mit dem Netz der Einrichtung verbunden werden können und dadurch weitere Flexibilität ermöglichen, da kein Gerät an einen lokalen Computer angesteckt werden muss.\nIm Vergleich zu Barcode-Lesern, die den Inhalt eines gelesenen Barcodes meist einfach als Tastatureingaben an die Position des Cursors am Bildschirm ausgeben, können RFID-Geräte aufgrund der komplexen Inhalte von RFID-Tags (siehe Datenmodelle) auch mittels Programmierschnittstellen aus dem BMS oder anderen Systemen angesprochen werden. Die Logik der Interpretation des Taginhalts liegt hierbei dann beim BMS oder bei einer Zwischensoftware („Proxy“), die zwischen Reader und BMS vermittelt.\n\n\nMedienarten\nRFID-Transponder sind nicht für alle Medienarten geeignet. Aufgrund der Tatsache, dass RFID-Tags über Funkwellen mit Strom versorgt werden und kommunizieren, gilt, dass die Tags durch das Vorhandensein von Metall, Wasser, o.ä. beeinflusst oder abgeschirmt werden können. Selten vorkommende metallisch beschichtete Einbände von Bücher zum Beispiel verhindern effizient die Nutzung von RFID. Konkret bedeutet das, dass die Antenne eines RFID-Tags im Regelfall nicht mehr funktioniert, wenn sie in direkter Nähe zu metallischen Oberflächen ist: Eine CD, ein Laptop im Rucksack, oder sogar ein anderen RFID-Tag in einem dünnen Buchstapel können die Reichweite und Lesbarkeit einschränken. Beim Einbringen von Tags ist daher darauf zu achten, dass sie nicht auf solche Materialien aufgebracht werden. Ebenfalls hilft es nicht, ein Tag auf die Außenseite eines metallischen Gegenstands aufzubringen.\nGenerell sollte der Gegenstand, auf den ein Tag aufgebracht wird, von Funkwellen durchdrungen werden können, zumindest aber von der Seite, an der das Tag aufgebracht wurde.\nGleichzeitig bedeutet das auch, dass RFID-Tags durch das Vorhandensein von Wasser, also Menschen, ebenfalls abgeschirmt werden können. Eine hunderprozentige Erkennungsrate in einem Sicherheitsgate ist somit unrealistisch.\nRFID-Transponder sind natürlich nicht geeignet für Medien, bei denen eine Unwucht störend ist (Schallplatten, CDs), sie sollten dabei auf der Außenhülle angebracht werden. Bei CDs ist zusätzlich der Metallanteil der CD störend.\nDazu kommt, dass RFID-Tags unterschiedlicher Hersteller und Arten unterschiedlich auf verschiedene Umgebungen reagieren. Für UHF wird vom Unternehmen EECC eine Studie herausgegeben, die die physikalischen Eigenschaften von verschiedenen Tags untersucht („UHF RFID Almanach 2022“ 2022). Um diese Studie nutzen zu können, sind allerdings fundierte physikalische Kenntnisse notwendig, alternativ kann die Firma mit der Aussprache einer Empfehlung beauftragt werden, wie 2019 in der UB Dortmund geschehen.\nNach Einführung von RFID ist das Weiterführen von Barcodes zur Identifikation zwar nicht mehr zwangsläufig erforderlich, es ergeben sich aber zwei Vorteile, insofern weiterhin der Barcode mit am oder im Medium angebracht wird. Durch den Barcode kann das Medium weiterhin maschinenlesbar identifiziert werden. Falls RFID Komponenten ausfallen sollten, kann mit dem Barcode traditionell weiter gearbeitet werden. Wenn außerdem (wie meist üblich) unter dem Barcode auch die im Barcode codierten Zeichen mit zu sehen sind, können auch Menschen das Medium bzw. den Band problemlos eindeutig identifizieren. Barcodes können dabei auch auf ein mit Papier beschichtetes RFID-Tag aufgebracht werden.\nAuf die Barcodes kann beim Einsatz von RFID auch verzichtet werden, wenn in Kauf genommen wird, dass man für die Identifikation eines Buches eine Recherche im BMS durchführen muss, wenn das Tag unlesbar oder falsch beschrieben ist.\n\n\nAlternativen zu RFID in Bibliotheken\nFortgeschrittene Entwicklungen im Bereich OCR und generell CV (computer vision) können es ermöglichen, ganz ohne technisch lesbare Identifikationsmerkmale ein Buch zu erkennen und zu verarbeiten. Bei einer solchen Lösung wird mittels einer Kamera das Äußere eines Mediums aufgenommen und erkannt und kann somit weiterverarbeitet werden. Um das Medium einer Bibliothek zuzuordnen kann weiterhin etwa ein Aufkleber auf dem Buch, z.B. ein Signaturetikett auf dem Rücken, erkannt werden, um das Buch der Bibliothek zuordnen zu können und ggf. mehrere Exemplare auseinander halten zu können.\nBisher ist kein solches System aktiv im Einsatz; die Erkennung von Objekten ist jedoch ein aktives Forschungsthema.\nVollständige Automatisierung ist in Bibliotheken auch möglich mit Barcodeverbuchung und EM-Sicherung (EM = elektro-magnetisch). Wenn die Barcodes vorn außen am Buch angebracht sind, können sie auch von Ausleihautomaten und Rückgabeautomaten gelesen und verarbeitet werden. EM-Sicherung wird realisiert mittels magnetisierbaren Streifen, die mit doppelt klebendem Film in den Falz der Bücher geklebt wird. Diese Streifen sind über separate Geräte magnetisierbar bzw. entmagnetisierbar. Der Status ist detektierbar über spezielle Gates, so dass darüber die Buchsicherung realisierbar ist. Auch diese Sicherung ist natürlich fehleranfällig und nicht 100%ig. Schon metallene Gegenstände in derselben Tasche wie gesicherte Bücher verhindern die Erkennung. Es gibt bei dieser Methode also keinen Vorteil gegenüber eines RFID-Betriebs – im Gegenteil, diese Technologie ist veraltet, wird immer seltener genutzt und insofern immer teurer.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html#dienste-für-nutzerinnen",
    "href": "infrastruktur.html#dienste-für-nutzerinnen",
    "title": "Technische Infrastruktur",
    "section": "Dienste für Nutzer*innen",
    "text": "Dienste für Nutzer*innen\n\nWebsite\nEine Internetpräsenz ist für eine öffentliche Einrichtung mittlerweile unabdingbar und dient vielen Nutzer*innen als Erstkontaktmöglichkeit, Arbeitsmittel und Informationsplattform gleichermaßen. Die Infrastruktur hierfür wird im Kapitel Kommunikation genauer behandelt.\n\n\nInternetzugang\nDer kostenlose Zugang zum Internet ist für viele Nutzer*innen ein Grund die Bibliothek als Lern- und Arbeitsort zu nutzen. PC-Arbeitsplätze und freies WLAN gehören deshalb in den meisten Bibliotheken mittlerweile zum Standard. Letzteres ist Voraussetzung, um mit eigenen Geräten wie Notebook, Handy und Tablet arbeiten zu können. Damit in allen relevanten Bereichen WLAN mit angemessener Bandbreite verfügbar ist, sollten Bibliotheken Anforderungen an die Ausstattung des Gebäudes mit einer ausreichenden Anzahl an WLAN-Access-Points bestimmen. Das öffentliche Netz sollte vom internen Netz für Mitarbeiter*innen der Bibliothek getrennt sein, um das Risiko eines Angriffs auf die Infrastruktur zu minimieren. Bei öffentlichen PCs sind zusätzlich Datenschutz-Maßnahmen zu treffen.\nGrundsätzlich sind für die Bereitstellung von Internet zwei Fragen zu klären:\n\nIst der Zugang offen oder sind ein Passwort und ggf. Registrierung notwendig?\nErfolgt der Zugang direkt durch die Bibliothek oder per Roaming über Dritte?\n\nIn Hochschulen und Forschungseinrichtungen bietet sich die WLAN-Roaming-Infrastruktur eduroam, die in Deutschland vom DFN koordiniert wird und auch international an vielen Bildungseinrichtungen verfügbar ist. Der Betrieb von eduroam wird in der Regel durch das universitäre Rechenzentrum verantwortet. Nutzer*innen müssen allerdings an einer Einrichtung registriert sein und ihre Zugangsdaten kennen, um eduroam zu verwenden.\nFür den offenen Zugang kann im Idealfall mit der Trägereinrichtung z.B. der eigenen Kommune zusammengearbeitet werden. Eine weitere Möglichkeit ist das ehrenamtliche Freifunk-Projekt. Je nach Rahmenbedingung gibt es verschiedene Leitfäden und Fördermöglichkeiten zur Einrichtung offenen Internetzugangs.\n\n\n\nInternet-Nutzungshinweise in den Städtischen Bibliotheken Leipzig\n\n\nAls Anbieter von öffentlichem WLAN sollten Bibliotheken auf Gefahren und mögliche Sicherheitsvorkehrungen hinweisen. Bei der Nutzung von öffentlichem WLAN muss beachtet werden, dass die Verbindungen in der Regel nicht verschlüsselt sind und somit alle, die sich im gleichen Netzwerk befinden, potenziell die übertragenen Daten mitlesen können. Zur Minimierung des Risikos sollten Webseiten möglichst nur verschlüsselt per HTTPS aufgerufen werden und Datei- und Verzeichnisfreigaben deaktiviert sein, um zu verhindern, dass andere Teilnehmer im Netzwerk auf eigene Dateien zugreifen können.\n\n\nGruppen- und Einzelarbeitsplätze\nZur Ausstattung von Gruppen- und Einzelarbeitsplätzen gehört auch angemessene Informationstechnik. Wesentlich sind zunächst ein stabiler Internetzugang und Steckdosen. Dabei sollte die Absicherung mit genügend Sicherungen und getrennten Stromkreisen geplant werden, denn nicht selten fliegt wegen eines defekten Laptop-Netzteils sonst die Sicherung eines gesamten Lesesaals raus. Vor allem sollte in solchen Fällen nicht plötzlich das gesamte Bibliothekspersonal stromlos dastehen.\nAusstattung, Verwaltung und Unterhalt von Räumen mit Technik ist insbesondere für öffentliche Bibliotheken ressourcen- und kostenintensiv. Entsprechend sollten sich Bibliotheken an der tatsächlichen Nachfrage ihrer Nutzer*innen orientieren und nur die Technik anschaffen, die sie selbst verwalten können.\nEs gibt auch einige Bibliotheken, die in ihren öffentlichen Arbeitsbereichen sogenannte Smartboards zur Verfügung stellen. Damit sind große Monitore gemeint, die Computer enthalten. Die gewohnten Funktionen wie Webbrowsing, Textverarbeitung und andere Programme sind über Bildschirmtastatur/Touchscreen erreichbar, man kann aber auch eine Art digitales Whiteboard nutzen und die darauf erarbeiteten Ergebnisse digital weiternutzen.\nIn der Praxis zeigt sich jedoch bisher, dass Smartboards meistens nur als großer Monitor mit extern angeschlossenem Notebook oder integriertem Computer genutzt werden. Ein Angebot von derartigen Projektionsflächen lässt sich auch kostengünstiger realisieren.\nIn einigen Bibliotheken werden auch komplette Workspaces für große Gruppen angeboten. Für die gezielte Nutzung von speziell ausgestatteten Arbeitsplätzen kann das Angebot einer Sitzplatz- bzw. Raumbuchungslösung nützlich sein. Das Buchungssystem lässt sich auf der Homepage oder in einer Service-App einbinden und kann auf diesem Weg ebenso wie andere Dienstleistungen genutzt werden. Hier werden inzwischen auch gehostete Lösungen als Cloud-Service angeboten. Oft unterstützen diese Dienste die Anmeldung via Shibboleth und OpenID-Connect, ggf. sogar die Nutzung innerhalb einer Föderation. Die Fragen des Datenschutzes sind natürlich bei einem solchen Einsatz im Vorfeld zu klären.\n\n\nÖffentliche PC-Arbeitsplätze\nSowohl wissenschaftliche als auch öffentliche Bibliotheken bieten PC-Arbeitsplätze für ihre Nutzer*innen an. Auch wenn der Trend zu eigenen Geräten geht, bleiben die Nutzungszahlen bei den PC-Arbeitsplätzen besonders in öffentlichen Bibliotheken stabil.\nDienste wie die Nutzung des Internets und Textverarbeitung mit oder ohne Gebühren sind die häufigsten Einsatzzwecke für PC-Arbeitsplätze und sollten weiterhin niedrigschwellig angeboten werden. Die Gebührenabrechnung für angemeldete Nutzer*innen erfolgt über Bezahlsysteme, die an das Bibliotheksmanagementsystem angegliedert sind. Geräte mit Münz- oder Kartenzahlung bieten außerdem die Möglichkeit, diese auch ohne Bibliotheksmitgliedschaft zu nutzen.\nIn der Regel stehen PC-Arbeitsplätze angemeldeten Nutzer*innen zur Verfügung. Anhand der Benutzergruppe können altersbedingte Einschränkungen vorgenommen werden. So müssen Eltern z.B. der Internetnutzung von minderjährigen Kindern zustimmen.\nBei kostenfreier und nicht reglementierter Nutzung muss der Jugendschutz besonders beachtet werden. Denkbar ist dabei das Whitelisting von Internetseiten, d.h. eine Freischaltung von Seiten, die aus Jugendschutz-Sicht als unbedenklich eingestuft wurden. Ein Blacklisting hingegen ist nicht empfohlen, da keinesfalls alle bedenklichen Seiten bekannt sein können.\nAuch der Datenschutz spielt im öffentlichen Bereich eine große Rolle. PCs müssen so konfiguriert werden, dass Nutzer*innen ausschließlich ihre eigenen Dateien sehen und keinen Zugriff auf Dateien von anderen Personen erhalten. Dies wird z.B. durch persönliche Nutzerprofile (gebunden an das Benutzerkonto) oder systemseitige Rücksetzung aller Einstellungen (Gastzugänge) erreicht. Zum Betrieb solcher „Kiosksysteme“ gibt es entsprechende Software. Der Einsatz von Thin-Clients ist in diesem Bereich sinnvoll.\nAuch bei der Freigabe der Nutzung von Speichermedien sollten Sicherheitsvorkehrungen getroffen werden, beispielsweise vorgeschaltete automatische Virenprüfungen. Eine Sperre von externen Speichermedien wäre ebenfalls denkbar, jedoch spricht die aktuelle Nachfrage dagegen.\nHinweise zur Einrichtung eines öffentlichen WLAN in Bibliotheken gibt der Abschnitt zum Internetzugang.\n\n\nDigitalisieren/Scannen, Digitalisierung on demand\nScannen, Kopieren und Drucken sind weitere häufig genutzte Angebote in Bibliotheken. Zur Unterstützung von Nachhaltigkeit und Digitalisierung von Studium und Lehre könnte das Ausdrucken auf Papier reduziert oder gar nicht angeboten werden und stattdessen das Einscannen auf Datenträger oder Speichersysteme befördert werden.\nUm Nutzer*innen das Digitalisieren von Medien zu ermöglichen, kann eine Bibliothek Scanner zur Verfügung stellen. Im einfachsten Fall sind das Multifunktionsgeräte (MFGs), die sowohl Kopier- als auch Scan- und Druckfunktionen anbieten. Meistens sind Druck- und Kopierfunktionen kostenpflichtig, Scannen oft kostenfrei. Höherwertige Scans von größeren Vorlagen und ergonomisches Scannen sind mit sogenannten Kopfscannern möglich. Bei diesen Scannern liegt das Medium offen, mit dem Druckbild nach oben auf der Vorlagefläche. Mit Fingerdruck oder Fußschalter wird der Scanvorgang ausgelöst, anschließend kann ohne Umdrehen der Vorlage, wie es bei einem herkömmlichen Kopierer notwendig wäre, umgeblättert werden.\nEin weiterer Anwendungsfall ist das Einscannen von Einzelblattvorlagen. z.B. Vorlesungsmitschriften. Hier sind Scanner mit automatischem Papiereinzug ideal, die Vorder- und Rückseite gleichzeitig einscannen können.\nDas Digitalisat kann in der Regel auf einem USB-Stick gespeichert werden. Komfortabler sind eine Netzwerkverbindung und eine Anmeldemöglichkeit für Nutzer*innen. Alternativ kann auch die Eingabe einer Mailadresse mit anschließendem Versand eines Links auf das Dokument angeboten werden. Das Digitalisat selbst per E-Mail zu verschicken ist i.d.R. aufgrund der Dateigröße nicht möglich.\n\n\nAusleihbare Geräte\nAls „Bibliothek der Dinge“ wird die Möglichkeit bezeichnet, in Bibliotheken auch Gegenstände wie Werkzeuge, Sportgeräte und Musikinstrumente ausleihen zu können. Für die Ausleihe von Kunstwerken oder Spielen sind auch die Begriffe „Artothek“ bzw. „Ludothek“ üblich. Für diese Gegenstände ist in der Regel eine besondere Form der Mediensicherung notwendig. Für die Ausgabe von Tablets gibt es beispielsweise spezielle Automaten. Durch Verbindung mit dem Bibliotheksmanagementsystem ist es auch möglich, die Freigabe an ein Mindestalter zu knüpfen und verschiedene Profile auf den Tablets anzulegen.\n\n\nMakerspace\nEin Makerspace ist ein Bereich in einer Bibliothek, in dem Hardware und Software zum Ausprobieren zur Verfügung gestellt wird. Ziel ist das Angebot eines niedrigschwelligen Zugangs zu neuen (technischen) Entwicklungen, die der breiten Öffentlichkeit – in der Regel aus Kostengründen – sonst nicht zur Verfügung stehen. Beispiele für Angebote in Makerspaces sind:\n\n3D-Drucker\nGeräte zur Holz/Metallverarbeitung\nStickmaschine/Nähmaschine\nTon- und Videotechnik\nRepaircafé\n\nZusätzlich zur Bereitstellung der Technik bieten viele Bibliotheken Einführungs- und Expertenkurse an, die jedoch auch stark von den vorhandenen Personalressourcen abhängig sind. Makerspaces sind vor allem in größeren öffentlichen Bibliotheken verbreitet. Auch in einigen wissenschaftlichen Bibliotheken gibt es inzwischen entsprechende Angebote, wobei hier der Fokus mehr auf dem Einsatz in Lehre und Lernen liegt, zum Beispiel das Dortmunder Hybrid Learning Center (hylec).\n\n\nBibliotheks-App\nBei Überlegungen zum Einsatz einer App für die Dienstleistungen der Bibliothek sollten verschiedene Aspekte betrachtet werden. Eine App wird um so häufiger installiert, je mehr wichtige und häufig genutzte Funktionalitäten damit nutzbar sind. Eine Integration der Dienstleistungen der Bibliothek in eine bestehende App der übergeordneten Institution ist also der Eigenentwicklung vorzuziehen – sofern das möglich ist.\nIn einer App können grundsätzlich alle Dienstleistungen der Bibliothek angeboten werden. Ein Mehrwert entsteht dann, wenn man den mobilen Charakter des Endgerätes berücksichtigt. Beispiele: Navigation in der Bibliothek mit Wegweisung am Endgerät, Buchung des Gruppenarbeitsraums, vor dem man gerade steht (z.B. über einscannbare QR-Codes).\nEine vollständige Nutzung der Dienstleistungen der Bibliothek ist nur dann möglich, wenn man sich auch digital anmelden kann. Es sollte also eine Form eines digitalen Ausweises geben, Accountname/Passwort im einfachsten Fall, komplett digitaler Ausweis über die App im besten Fall.\nWildau als Beispiel mit UNIDOS hat eine Integration des Bibliothekskontos, mit der Möglichkeit der Anzeige aller entliehenen Medien und der Verlängerung dieser. Bewerkstelligt wird die Funktion via SIP2-Schnittstelle. Zusätzlich können Discovery-Systeme sowie eine Raumbuchung verlinkt bzw. direkt via App ermöglicht werden. Die Kosten für die Entwicklung einer nativen App für Android und iOS sind eher hoch. Daher kann stattdessen auch eine responsive Web-App zum Einsatz kommen. Apps machen meist nur dann wirklich Sinn, wenn man Zugriff auf interne Komponenten der Smartphones wie NFC, Kamera (zum Scannen), Bluetooth (z.B. zur Detektion von Beacons) etc. benötigt.\n\n\nTechnische Beratung und Schulungen\nTechnische Beratung erfolgt oft in dem Umfang, der für lokale Bibliotheksdienste sinnvoll ist. Bietet eine Bibliothek z.B. die Onleihe als Dienst an, werden sich Nutzer*innen bei Fragen direkt an die Bibliothek wenden und nicht an den Dienstleister.\nSomit müssen sich auch die Mitarbeiter*innen in der Bibliothek stetig fortbilden, um ihren Nutzer*innen einen guten Service zu bieten.\nBeispiele:\n\nErklärung und Dokumentation zu Diensten, z.B. Ebook-Leihe, Streaming-Dienste, E-Learning-Ressourcen\nEbook–Reader Beratung zur Unterstützung der Ebook-Ausleihe\nBeratung zu App-Nutzung, die als digitale Inhalte angeboten werden\n\nWerden neue Dienste eingeführt, bedarf es neben der Werbung auch einer Einführung oder dem Angebot einer Schulung, in erster Linie für Mitarbeiter*innen. Viele Anbieter unterstützen dabei mit eigenem Schulungsmaterial, was unter Umständen je nach Zielgruppe angepasst werden muss.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html#dienste-für-mitarbeiterinnen",
    "href": "infrastruktur.html#dienste-für-mitarbeiterinnen",
    "title": "Technische Infrastruktur",
    "section": "Dienste für Mitarbeiter*innen",
    "text": "Dienste für Mitarbeiter*innen\nDie folgenden IT-Dienstleistungen dienen der Unterstützung der täglichen Arbeit, insbesondere im Hinblick auf verteilte Arbeitsumgebungen und mobiles Arbeiten. In vielen Fällen werden sie von der übergeordneten Einrichtung einer Bibliothek bereitgestellt. Neben dem Zugang zu Arbeitsmitteln dienen die Dienste vor allem der internen Kommunikation und dem Wissenmanagement.\n\n\n\n\n\n\nInfo\n\n\n\nWissensmanagement besteht aus Prozessen zur Erfassung und Weitergabe von Wissen innerhalb einer Organisation, das oft nur implizit in den Köpfen der Mitarbeitenden vorliegt. Werkzeuge hierfür werden im Kapitel Kommunikation vorgestellt.\n\n\n\nMobiles Arbeiten\nFür mobiles Arbeiten müssen Endgeräte transportabel sein (Notebook, Tablet …) und zum anderen müssen die Dienste, die für das Arbeiten notwendig sind, vom jeweiligen Standort aus erreichbar sein (siehe VPN). Für dauerhaftes Arbeiten von anderer Stelle als dem Büro (Homeoffice) ist aus Ergonomiegründen ein fester Arbeitsplatz mit Tastatur, Maus, Bildschirm und ggf. Anschlussmöglichkeit für mobile Geräte („Dock“) vorzuziehen oder vorgeschrieben.\nDer Begriff bring your own device (BYOD) bezeichnet die Nutzung von privaten Endgeräten in der Infrastruktur des Arbeitgebers. Dies ist allerdings mit einigen Herausforderungen verbunden. So kann nicht zentral sichergestellt werden, dass das Endgerät frei von Schadsoftware ist. Auch die Sperrung des Gerätes aus der Ferne ist nicht möglich (auf eigenen Geräten des Arbeitgebers kann eine solche Software installiert werden, die im Falle eines Diebstahls aktiviert werden kann). Letztendlich liegt also die Verantwortung dafür, ob dienstliche Daten über das private Gerät in falsche Hände gelangen, beim Arbeitnehmer.\n\n\nVPN\nEin „Virtuelles Privates Netzwerk“ dient dazu, über einen authentifizierten Zugriff das Endgerät der Mitarbeiter*innen bzw. Nutzer*innen virtuell in das interne Netzwerk (Intranet) der Institution einzubinden. Das ermöglicht die Nutzung von Diensten, die auf der Basis der Netzwerkadresse (IP-Adresse) entscheiden, ob der Zugriff ermöglicht wird. Viele Dienste einer Institution werden über eine Firewall aus Sicherheitsgründen dem gesamten Internet verborgen und sind nur über ein VPN auch außerhalb der Institution zum Beispiel aus dem Homeoffice zugreifbar.\n\n\nWerkzeuge zur Kommunikation\nWie im Kapitel zur Kommunikation beschrieben, gibt es verschiedene Werkzeuge zur synchronen und asynchronen Kommunikation von Telefon über E-Mail und Chats bis zu Videokonferenzsystemen. Neben der reinen Kommunikation dienen Werkzeuge wie Wikis und Ticketsysteme auch der Zusammenarbeit und kollaborativen Erstellung von Inhalten.\nUm virtuelle Besprechungsräume so auszustatten, dass auch Menschen an Besprechungen teilnehmen können, die nicht anwesend sind, ist technische Infrastruktur erforderlich. Die Anwesenden müssen für die Anwesenden sichtbar und hörbar sein, ohne dass die Anwesenden Headsets tragen müssen. Dies erreicht man mit Webcams, die automatisch auf die sprechende Person fokussieren und im Idealfall auch Störgeräusche (Echos, Rauschen, Rascheln) ausblenden. Die gängigen Systeme im knapp vierstelligen Eurobereich genügen für Konferenzen mit bis zu sechs anwesenden Personen um einen Tisch herum. Sind mehr Personen anwesend, steigt der technische Aufwand stark an, wenn man häufig hybrid arbeiten und die anwesenden Personen nicht benachteiligen möchte.\n\n\nFileservices\nDienstlich genutzte Dateien sollten an zentraler Stelle abgelegt werden, damit sie in eine Backup-Lösung eingeschlossen werden können und damit die Möglichkeit besteht, sie mit anderen Menschen auszutauschen. Entweder wird dazu als Netzlaufwerk ein klassischer Fileserver wie zum Beispiel Windows Server genutzt, oder es kommt eine Cloudlösung wie Nextcloud zum Einsatz. Zusätzlich oder alternativ kann ein Dokumentenmanagementsystem genutzt werden.\n\n\nDokumentenmanagementsystem (DMS)\nDokumentenmanagementsysteme sind Multi-User Softwaresysteme mit Anbindung an eine hinreichend großen und ausfallsicheren Datenspeicher. Sie lösen in der Regel drei Anforderungen:\n\nlangfristige ggf. auch revisionssichere Ablage digitalisierter oder rein digitaler Dokumente die einer Aufbewahrungsfrist unterliegen (Archivierung, Versionierung)\nUnterstützung der Datenverarbeitung für Prozesse zwischen verschiedenen Akteuren (Workflows)\nStrukturierung und Pflege der Dokumente institutions-relevanter interner und externer Prozesse (Aktenplan)\n\nAb und an erhalten Bibliotheken den Auftrag, die Originale von digitalisierten und in einem DMS abgelegten Dokumenten physisch zu archivieren oder auch den Digitalisierungsprozess zu verantworten. Hierbei ist es empfehlenswert, zwischen Unternehmens- bzw. institutionskritischen Dokumenten, die nicht für die Bibliotheksnutzer verfügbar sein sollen, und Dokumenten mit bibliothekarischem Bezug zu unterscheiden, denn eine Bibliothek ist im Allgemeinen kein Archiv.\n\n\nSocial Intranet\nAls Kombination des klassischen Intranets mit sozialen und kommunikativen Funktionen ermöglicht ein Social Intranet den Mitarbeitenden so den einfachen Austausch von Informationen und Ideen und wird genauer im Kapitel zur Kommunikation beschrieben.\n\n\nPlanungs- und Koordinationstools\nDie in diesem Absatz beschriebenen Werkzeuge dienen der Aufgaben-Koordination in der Bibliothek. Je mehr Personen in einem Team an gemeinsamen Aufgaben arbeiten, desto empfehlenswerter ist der Einsatz folgender Instrumente für die Aufgaben-Koordination in der Bibliothek.\nTicketsystem: Ticketsysteme dienen der strukturierten, regelbasierten (z.T. automatischen) Abarbeitung von Anfragen, Wünschen, Fehlermeldungen von Nutzenden. Diese Systeme können für alle anfallenden Aufgaben innerhalb der Bibliothek verwendet werden, z. B. in der Softwareentwicklung bei der Bearbeitung von Kundenanfragen oder internen Prozessen.\nEin Vorteil ist die transparente Bearbeitung von Vorgängen, da sowohl der Status als auch die Bearbeitenden jederzeit einsehbar und nachvollziehbar sind. Statusänderungen werden dem Autor je nach Einstellung des Ticketsystems automatisiert mitgeteilt. Alle Bearbeiter:innen haben zu jeder Zeit Einblick und können bei Bedarf übernehmen. Eine Vertretungsregelung oder E-Mail-Weiterleitung wird damit obsolet. Nach abschließender Bearbeitung werden Tickets in der Regel archiviert und können später zu Statistik- und Dokumentationszwecken ausgewertet werden. Auch eine Art FAQ kann automatisiert durch entsprechend markierte Tickets erstellt werden.\nBeispiele für Ticketsysteme:\n\nRedmine (Open Source)\nJira (kommerziell)\nIntegrierte Issue-Tracker im Rahmen der Softwareentwicklung, beispielsweise in GitLab (Open Source) und GitHub (kommerziell)\n\nKanban-Board: Ein Kanban-Board visualisiert jegliche Arbeitsabläufe anhand einer in Spalten unterteilten Tafel (z. B. Backlog, in Arbeit, fertig). Physisch ist dies mit Post-Its und Whiteboard machbar. Ein Kanban-Board kann auch als Grundlage für ein Projektmanagement nach SCRUM dienen.\nBeispiele für Kanban-Software:\n\nDeck (Open Source Plugin für NextCloud)\nKanboard (Open Source)\nTrello (kommerziell)\n\nInterne und externe Kalender: Kalender können unterschiedliche Aufgaben erfüllen. Intern helfen zwischen Mitarbeitenden geteilte Kalender bei der Terminplanung und Abstimmung.\nAuf der Bibliothekswebsite veröffentlichte Kalender können z. B. buchbare Veranstaltungen für Kund*innen und Besucher*innen der Bibliothek enthalten. Ebenso ist eine Reservierung von Räumen oder Technik denkbar.\nVerbunden mit einem Ticketverkauf oder einer Ticketreservierung können sie die umständliche Kommunikation über E-Mails ablösen und ersparen damit einen großen Verwaltungsaufwand.\nBeispiele für Kalender:\n\nSuperSaaS (kommerziell)\n\nSocial Media Planung: Betreibt eine Bibliothek mehrere Social Media Kanäle, kann ein einziges Tool zur Planung und Veröffentlichung von Beiträgen sinnvoll sein. Damit kann der Content zeitlich und inhaltlich vorgeplant werden. Integrierte Analysewerkzeuge helfen zudem bei der Zeitplanung, indem die vergangenen Veröffentlichungen ausgewertet werden und der beste Zeitpunkt für die Zielgruppe ermittelt wird.\nBeispiele:\n- Hootsuite, (kommerziell, in der Basisversion frei nutzbar) - Buffer, (kommerziell, in der Basisversion frei nutzbar) - Trello (kommerziell, in der Basisversion frei verfügbar) - Fedica (kommerziell, in der Basisversion frei verfügbar)\nWissenslandkarte: Wissenslandkarten, auch Wissenskarten (Knowledge Maps), sind grafische Darstellungen von Wissen in Organisationen. Als Wissenslandkarten werden im Wissensmanagement graphische Verzeichnisse von Wissensträgern, Wissensbeständen, Wissensquellen, Wissensentwicklung, Wissensstrukturen oder Wissensanwendungen bezeichnet. Sie dienen vor allem der Identifikation von Wissen in Unternehmen, um Arbeitsabläufe effektiver und effizienter zu gestalten und referenzieren auf Expertenwissen, Teamwissen, Wissensentwicklungsstationen sowie organisationale Fähigkeiten und Abläufe. Bei dieser Methode wird lediglich der Verweis auf das verankerte Wissen geliefert und nicht das Wissen selbst dort abgelegt.\n\nQuelle: https://de.wikipedia.org/wiki/Wissenslandkarte\nBeispiele: MindManager (kommerziell)\nTerminfindung: Abseits von Kalendern in E-Mail-Programmen können Terminfindungstools sinnvoll sein, insbesondere bei der Planung mit externen Beteiligten.\nBeispiele: DFN terminplaner, nuudel",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "infrastruktur.html#zusammenfassung-und-ausblick",
    "href": "infrastruktur.html#zusammenfassung-und-ausblick",
    "title": "Technische Infrastruktur",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nDie technische Infrastruktur bildet die Grundlage für die Dienste einer Bibliothek. Während sich die grundlegenden Dienste für Mitarbeiter*innen einer Bibliothek nicht wesentlich von denen anderen Einrichtungen unterscheiden, sind viele Dienste für die Nutzer*innen von Bibliotheken an die Verwaltung physischer Medien gekoppelt.\n\n\n\n\nFreyberg, Linda, und Sabine Wolf, Hrsg. 2019. Smart Libraries: Konzepte, Methoden und Strategien. b.i.t.online Innovativ 76. Wiesbaden: b.i.t. Verlag.\n\n\nKern, Christian. 2011. RFID für Bibliotheken. Springer.\n\n\n———. 2014. „Die Datenmodellstandardisierung und ihre Auswirkungen auf RFID-Bibliotheken“. In RFID für Bibliothekare: ein Vademecum, herausgegeben von Frank Seeliger, 3. Auflage. Verlag News & Media. https://core.ac.uk/download/pdf/33985396.pdf.\n\n\nMichaelis, Barbara. 2014. In RFID für Bibliothekare: ein Vademecum, herausgegeben von Frank Seeliger, 3. Auflage, 145–50. Verlag News & Media. https://doi.org/10.15771/RFID_2014_13.\n\n\nSeeliger, Frank, Hrsg. 2014. RFID für Bibliothekare: ein Vademecum. 3. Auflage. Verlag News & Media. https://doi.org/10.15771/978-3-936527-32-2.\n\n\n———. 2019. „Smart Services als Marketinginstrument“. In Praxishandbuch Informationsmarketing: Konvergente Strategien, Methoden und Konzepte, herausgegeben von Frauke Schade und Ursula Georgy, 343–57. De Gruyter Saur. https://doi.org/10.1515/9783110539011-023.\n\n\n„UHF RFID Almanach 2022“. 2022. EECC. https://eecc.info/rfidalmanach.html.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Technische Infrastruktur</span>"
    ]
  },
  {
    "objectID": "management.html",
    "href": "management.html",
    "title": "IT-Management",
    "section": "",
    "text": "Einleitung\nIT-Systeme sind selten statisch, sondern folgen einem Lebenszyklus von der Planung ihrer Einführung bis zu ihrer Ablösung. Während des Betriebs der Systeme müssen mögliche Risiken beachtet und rechtliche Rahmenbedingungen eingehalten werden. In Bibliotheken sind daher entsprechende IT-Kompetenzen und ein organisatorischer Rahmen für die Digitale Souveränität gefordert. Um diesen Anforderungen begegnen zu können, gibt es Möglichkeiten zur Aus- und Weiterbildung.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IT-Management</span>"
    ]
  },
  {
    "objectID": "management.html#it-lebenszyklus",
    "href": "management.html#it-lebenszyklus",
    "title": "IT-Management",
    "section": "Lebenszyklus von IT-Systemen",
    "text": "Lebenszyklus von IT-Systemen\nAlle IT-Systeme folgen einem Lebenszyklus, der mit ihrer Einführung beginnt und irgendwann mit ihrer Abschaltung endet (Abbildung 3.1). Die wesentlichen Phasen im klassischen Lebenszyklus eines IT-Systems werden im Folgenden näher betrachtet. Darüber hinaus wird erläutert, wie Änderungen an IT-Systemen in Institutionen im Rahmen des Change Managements begleitet werden sollten.\nGrundsätzlich unterscheidet sich die Implementierung von Informationssystemen in und für Bibliotheken nicht von der Softwareentwicklung in anderen Bereichen. Die konkrete Abfolge vor allem der ersten Phasen kann je nach der angewendeten Projektmanagement-Methode (agil vs. klassisch) variieren. Eine Diskussion von agilen und klassischen Methoden liegt außerhalb des Fokus dieses Handbuchs.\n\n\n\n\n\n\nAbbildung 3.1: Softwareentwicklungs-Lebenszyklus\n\n\n\n\nPlanung und Analyse\nGrundlage für die Umsetzung eines Softwareprojekts, egal ob es sich um individuell erstellte Software oder die Anpassung eines existierenden IT-Systems handelt, ist ein gemeinsames Verständnis für das Ziel und die Anforderungen an das Projekt. Dieses gemeinsame Verständnis, insbesondere der Anforderungen, sollte bei allen Projektmitgliedern und den weiteren Stakeholdern vorhanden sein. Die Anforderungen werden idealerweise vor und während der Entwicklung unter Einbeziehung von Nutzer*innen ermittelt und angepasst.\nZur Planungs- und Analysephase gehören neben einer grundsätzlichen Machbarkeitsanalyse des Projekts die Zusammenstellung eines geeigneten Teams, die Bestimmung der Stakeholder sowie die Klärung finanzieller und rechtlicher Rahmenbedingungen.\n\\(\\Rightarrow\\) Siehe auch ausführlicher zum Entscheidungsprozess bei der Einführung eines Bibliotheksmanagementsystem\n\n\n\n\n\n\nInfo\n\n\n\nZuweilen kommt es vor, dass die Entscheidung für ein IT-System bereits getroffen ist, bevor geklärt wurde, welches Problem damit gelöst werden soll. Auch in diesem Fall ist es sinnvoll, die Einführung mit einer offenen Planung und Anforderungsanalyse zu beginnen, und danach zu prüfen, welche Anforderungen das System tatsächlich abdecken kann.\n\n\n\n\nDesign/Prototyping\nWährend der Design- bzw. Prototyping-Phase entwickeln Designer*innen und Entwickler*innen erste Prototypen des geplanten IT-Systems. Ziel ist es dabei, Feedback der verschiedenen Stakeholder zu erhalten, um gemeinsam ein besseres Verständnis der Anforderungen zu erhalten bzw. diese zu präzisieren. Das Kapitel Anforderungen an Bibliotheks-IT geht gesondert auf die Bedeutung dieser Einbeziehung und die damit verbundenen Methoden ein.\n\n\nImplementierung\nAufbauend auf einem gemeinsamen Verständnis der Anforderungen überführen Entwickler*innen Prototypen in lauffähigen Code. Wird im Rahmen des Projekts ein bestehendes System implementiert, werden die Prototypen zunächst in eine Testinstanz und in der Folge in die produktive Instanz des Systems überführt.\nIn klassischen Projekten sieht man in dieser Phase zuerst ein Produkt mit idealerweise schon möglichst vielen der gewünschten Features, während nutzer*innenorientierte Vorgehensmodelle (siehe Kapitel Anforderungsanalyse) hier auf einen iterativen Prozess setzen, welcher Produktiterationen häufiger bereitstellt und evaluiert.\nGrundsätzlich unterscheidet sich die Implementierung von Informationssystemen in und für Bibliotheken nicht von der Softwareentwicklung in anderen Bereichen. Unabdingbar sind der Einsatz eines Versionskontrollsystems, ein Issue-Tracker und möglichst automatische Tests und Deployment (kontinuierliche Integration), sodass Änderungen am Quellcode direkt zu einer Aktualisierung der Test- und/oder Produktiv-Instanz der installierten Software führen.\n\n\nTest und Integration\nAls letzte Lebensphase vor der Produktivschaltung werden Abnahmetests und die Integration des entwickelten bzw. erworbenen Systems in die Zielumgebung durchgeführt. Im Falle der Inanspruchnahme eines Dienstleisters wird hier auch dessen Leistung final abgenommen, wenn das System erfolgreich produktiv in Betrieb genommen werden kann.\nWährend der Tests wird korrekte Umsetzen der Anforderungen sowie die Umsetzung der Anforderungen geprüft.\n\n\nWartung\nDie Wartungsphase folgt auf die Produktivsetzung des IT-Systems. In dieser Lebensphase wird das System nicht mehr grundlegend weiterentwickelt, es werden jedoch Fehler (Bugs) entfernt und Anpassungen der Funktionsweise im Sinne der Parametrisierung oder eine Optimierung der Programmabläufe vorgenommen.\nTypischerweise verbleiben IT-Systeme, die grundlegende Geschäftsprozesse abbilden oder die nach individuellen Anforderungen erstellt wurden, viele Jahre in dieser Phase.\n\n\n\n\n\n\nBeispiel\n\n\n\n\n\nAbbildung 3.2 illustriert die Lebensspanne einiger ausgewählter Nachweissysteme der Staatsbibliothek zu Berlin, die zum Zeitpunkt der Erstellung dieses Textes erst teilweise abgelöst wurden.\n\n\n\n\n\n\nAbbildung 3.2: Lebenszeit (in Jahren) von Bibliothekssystemen in der Wartungsphase am Beispiel der Staatsbibliothek zu Berlin (Stand 2022)\n\n\n\n\n\n\n\n\nAblösung\nDie Ablösung eines Systems kann eine Vielzahl an Gründen haben. So entwickeln sich die technischen Möglichkeiten und die Anforderungen der Nutzer*innen kontinuierlich weiter. Eine Ablösung kann aber auch durch technische Obsoleszenz erzwungen werden, wenn zugrundeliegende Software-Komponenten wie das Betriebssystem oder ein Datenbankmanagementsystem nicht mehr sicher betrieben werden können.\nDie konkrete Ablösungsplanung sollte mit genügend zeitlichem Vorlauf begonnen werden. Dies gewährleistet die Arbeitsfähigkeit in der Ablösungsphase. So können in der Vorphase beispielsweise notwendige Daten migriert werden, die vom Altsystem vorgehalten werden.\nMit dem frühzeitigen Beginn der Ablösungsplanung noch in der Wartungsphase können zudem vermeidbare Risiken minimiert werden. Dies ist insbesondere deshalb wichtig, da man als Betreiber eines IT-Systems nicht alle Faktoren kontrolliert, welche eine kurzfristig notwendig werdende Ablösung des Systems verursachen können. Darunter fallen zum Beispiel:\n\ndie Abschaltung wegen technischer Obsoleszenz (s.o.),\nder Ausfall des Systems durch Hardware-Ausfälle,\ndie Ankündigung von Wartungsarbeiten und Sicherheits-Patches durch den Hersteller oder die Insolvenz des Herstellers (insb. bei proprietärer Software) oder\ndie De-Facto-Unwartbarkeit durch den Wegfall geeigneten Personals mit Spezialkenntnissen (z.B. veralteter Programmiersprachen), siehe dazu auch den Abschnitt Ressourcenplanung\n\nLetztlich führen fast alle diese Punkte zur Abschaltung eines IT-Systems aufgrund von IT-Sicherheitsproblemen, da diese Einbrüche in die Systeme (Hacks) begünstigen. Hinzu kommt das Risiko von Datenverlusten, entweder durch physischen Verlust im Falle eines Hardware-Defekts oder durch den logischen Verlust, da z.B. proprietäre Datenformate nicht mehr gelesen werden können.\nDer Weiterbetrieb eines IT-Systems ohne Ablösungsplanung birgt hohe Risiken in sich und kann eine Organisation folglich in ernsthafte Schwierigkeiten bringen, insbesondere wenn geschäftskritische Prozesse betroffen sind.\n\n\nChange Management\nDie Umgestaltung von Prozessen und Tools in Bibliotheken ist ein Wandel, der die Organisation und Kultur des Hauses in ihrer Ganzheit berührt. Das Change Management – also die planvolle Steuerung von Veränderungsprozessen – spielt dabei eine zentrale Rolle. Es gilt, sowohl technologische Aspekte als auch menschliche Faktoren sorgsam in Betracht zu ziehen, um nachhaltige, akzeptierte und effektive Lösungen zu implementieren. Dies betrifft nicht nur die Auswahl und Einführung von Werkzeugen und Plattformen, sondern auch die Entwicklung von Kompetenzen, die Gestaltung von Arbeitsprozessen und die Förderung einer Kultur der Offenheit und Zusammenarbeit:\n\nRealen Bedarf ermitteln, Ziele und Zielgruppen definieren, keine „Solutions looking for a Problem“ einführen: welches (kommunikative) Problem will ich lösen?\nEinsatz bestehender Tools prüfen: brauche ich wirklich etwas Neues, oder kann ich mit gewissen Abstrichen leben und dafür ein bestehendes Tool kreativ einsetzen? Die Gefahr beim Einsatz einer großen Vielzahl von Tools ist, dass Information / Wissen zu stark in verschiedene Systeme fragmentiert wird. Die Gefahr beim Einsatz von zu unspezifischen Tools ist, dass die Lösung dem eigentlichen Problem nicht gerecht wird.\nAuf Interoperabilität sowie offene Standards achten und bereits mit der Einführung Exit-Strategien entwickeln, um einen Vendor Lock-in zu vermeiden.\nDie angestrebten Veränderung frühzeitig und offen kommunizieren, Unterstützung in Form von Dokumentation, Schulungen und Support anbieten, Ängste nehmen (sowohl bei Kolleg*innen als auch Gremien). Der Trend hin zu kollaborativen Tools sorgt für eine höhere Sichtbarkeit, das ist vielen nicht ganz geheuer. Oft ist es eine große Hemmschwelle, auf einmal für alle sichtbar in ein Wiki oder Forum zu schreiben.\nBereit sein, Verfahren und Werkzeuge auch wieder abzuschaffen („alte Zöpfe abschneiden“). Die Einführung zusätzlicher Werkzeuge sorgt sonst auch oft für Unverständnis, Frust und eventuell sogar Mehraufwände.\nAkzeptanz regelmäßig prüfen (siehe Evaluation)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IT-Management</span>"
    ]
  },
  {
    "objectID": "management.html#betriebsmodelle",
    "href": "management.html#betriebsmodelle",
    "title": "IT-Management",
    "section": "Betriebsmodelle",
    "text": "Betriebsmodelle\nInsbesondere serverbasierte Software, wie zum Beispiel das Bibliotheksmanagementsystem, kann auf verschiedene Arten betrieben werden. Die Betriebsarten unterscheiden sich bezüglich Installation, Kosten, Pflege und Wartung sowie Backup und Support. Die Wahl eines Betriebsmodells ist nicht nur eine betriebswirtschaftliche Entscheidung, sondern beeinflusst auch strategisch die Digitale Souveränität der Einrichtung.\n\nLokale Installation\nBis etwa 2010 war diese Betriebsart der Normalfall: Eine Einrichtung erwarb die Lizenz für eine (Server-)Software, entweder als Einzelkauf oder im Abo, und installierte diese auf eigenen Servern, z.B. im Serverraum der Bibliothek. Im Fachjargon spricht man auch von einer „On-premise“-Installation.\nIn diesem Modell kümmert sich die Einrichtung selbst um Installation und Updates. Folglich erfordert dieses Modell hohen Personaleinsatz und kann dazu führen, dass bei einem personellen Engpass eine Software länger betrieben bzw. nicht aktualisiert wird, als eigentlich ratsam wäre. Auch muss sich die Einrichtung um grundlegende Dinge, wie Backups und Ausfallsicherheit selbst Gedanken machen.\nAuf der anderen Seite bietet dieses Modell der Einrichtung potentiell den höchsten Grad an Kontrolle über die eingesetzte Software – etwa hinsichtlich nötiger Erweiterung oder Anpassung – und macht die Einrichtung damit weitgehend unabhängig von äußeren Einflüssen.\n\n\nHosting\nIn diesem Betriebsmodell wird die Ebene der Rechenkapazität bzw. Serverhardware an einen Dienstleister ausgelagert. Der Dienstleister kann hierbei etwa das Rechenzentrum einer Universität oder des jeweiligen Bibliotheksverbundes sein oder ganz allgemein jeder kommerzielle Betreiber eines Rechenzentrums, bei dem Kapazitäten erworben werden.\nSämtliche Betriebsfragen, wie Backups und Ausfallsicherheit der eingesetzten Hardware können an diesen Anbieter delegiert werden. Im Falle des Hostings durch einen Bibliotheksverbund entfallen möglicherweise auch Einrichtung, Installation und Upgrades. Die Betriebskosten müssen beim Verbund kalkuliert werden, was jedoch durch das Hosting für mehrere Einrichtungen besser skaliert.\n\n\nCloud\nBei diesem Betriebsmodell, das manchmal auch als SaaS (Software as a Service) bezeichnet wird, liegt der technische Betrieb beim Anbieter bzw. Dienstleister der Software und die Einrichtung nutzt eine für sie bestmöglich vorkonfigurierte Installation („Instanz“). Dies ist insbesondere bei webbasierten Anwendungen die bevorzugte Betriebsart, stellt aber erhöhte Anforderungen an die Anbindung lokaler Endgeräte wie z.B. Ausleihautomaten), weil dabei eine sichere und stabile Verbindung zwischen den lokalen Automatisierungsgeräten und dem entfernt gehosteten System hergestellt werden muss.. Die Einrichtung ist weder für die Wartung der eingesetzten Hardware noch für die Pflege der genutzten Software zuständig.\nIn der Praxis kann sich ein solches Betriebsmodell als komfortabel erweisen, da keine Personalressourcen für allgemeine Tätigkeiten des IT-Betriebs oder spezielle Bibliotheks-IT-Tätigkeiten benötigt werden. Gerade für kleine Einrichtungen kann dies ein guter Weg sein, möglichst personalsparend serverbasierte Software einzusetzen. Auf der anderen Seite fallen für dieses Betriebsmodell Abonnementkosten beispielsweise nach Größe der Einrichtung oder nach Anzahl der Endnutzer*innen an. Es hängt daher vom Einzelfall und von der Art der Berechnung ab, ob eine Kostenersparnis zu erwarten ist.\n\n\nDigitale Souveränität\nDigitale Souveränität bedeutet die selbstbestimmte Gestaltung und Kontrolle über die eigenen digitalen Ressourcen, Technologien und Daten, um unabhängige, nachhaltige und nutzerzentrierte Services gewährleisten zu können.\nSie bietet Bibliotheken die Möglichkeit, sowohl ihren Service als auch ihre interne und externe Kommunikation auf eine nachhaltige, unabhängige und innovative Weise zu gestalten und weiterzuentwickeln, während gleichzeitig die Prinzipien der Offenheit und Gemeinschaftlichkeit im Umgang mit Wissen und Technologie gefördert werden.\nDigitale Souveränität repräsentiert für Bibliotheken damit nicht nur einen ethischen Imperativ, sondern führt auch zu einer überzeugenden strategischen Ausrichtung:\n\nUnabhängigkeit von Dienstleistern: Durch die Selbstverwaltung und -steuerung der eingesetzten digitalen Werkzeuge können Bibliotheken spezifische Anpassungen vornehmen und sind nicht auf externe Anbieter angewiesen, wodurch auch die Datenkontrolle intern bleibt.\nAufbau von Know-how im Haus: Die kontinuierliche Arbeit mit und Entwicklung von digitalen Werkzeugen befähigt das Personal, ein vertieftes technisches Verständnis und Fertigkeiten zu entwickeln, was langfristig zur Selbstständigkeit und Innovationskraft beiträgt.\nStärkung von Kooperation durch Open Source: Gemeinsame Entwicklungsprojekte im Open-Source-Bereich unterstützen nicht nur den Wissensaustausch und die Innovationskraft unter Bibliotheken, sondern fördern auch die Entstehung robuster und anpassbarer Technologien, die gemeinschaftlich weiterentwickelt werden können.\nÖffentliches Geld für öffentliche Software: Diese Philosophie unterstreicht das Bestreben, mit öffentlichen Mitteln finanzierte Software auch der gesamten Gemeinschaft zugänglich zu machen, wodurch der Grundsatz der Transparenz und des gemeinschaftlichen Nutzens betont wird.\nFinanziell attraktiv: Durch die Vermeidung von Lizenzkosten und die Möglichkeit, Software nach Bedarf anzupassen und weiterzuentwickeln, eröffnen Open-Source-Werkzeuge nicht nur kosteneffiziente, sondern auch zukunftssichere Investitionspfade.\nDatenschutz und -sicherheit: Eigenkontrolle über die Systeme kann zu besseren Sicherheits- und Datenschutzpraktiken führen, da Bibliotheken direkten Einfluss auf die Verwaltung und den Schutz von Benutzerdaten haben.\nInklusion und Zugänglichkeit: Die Fähigkeit, digitale Werkzeuge selbst zu gestalten und anzupassen, ermöglicht es Bibliotheken, gezielt auf inklusive Praktiken und die Schaffung barrierefreier Dienste und Ressourcen hinzuwirken.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IT-Management</span>"
    ]
  },
  {
    "objectID": "management.html#betriebssicherheit-und-risikomanagement",
    "href": "management.html#betriebssicherheit-und-risikomanagement",
    "title": "IT-Management",
    "section": "Betriebssicherheit und Risikomanagement",
    "text": "Betriebssicherheit und Risikomanagement\nNeben den Problemen der Ablösungplanung gibt es weitere Risiken des Betriebs von IT-Systemen, von denen einige im nachfolgenden Abschnitt vorgestellt werden.\n\nVendor Lock-in\nEin nicht zu unterschätzendes Risiko, welches sich aus der Einführung eines proprietären IT-Systems ergibt, ist der sogenannte Vendor Lock-in. Dieser beschreibt die Abhängigkeit von Produkten oder Dienstleistungen eines Anbieters, durch die der gleichzeitige Einsatz von anderen Produkten oder der Wechsel zu anderen IT-Systemen erschwert wird. Durch den Einsatz von Systemen mit etablierten Standards, offenen Datenformaten und Schnittstellen sowie geeigneter Ablösungsstrategien kann das Risiko eines Vendor Lock-ins verringert werden.\nDer Begriff des Vendor Lock-ins kann noch auf den Bereich der Fehlerbehebung und die Wartung von Software ausgedehnt werden. Im Fall von proprietärer Software, welche ohne Zugriff auf den Quellcode betrieben wird, ist die Fehlerbehebung ausschließlich Sache des Herstellers. Fällt dieser, wie oben beschrieben, aus, kann ein Betrieb aus IT-Sicherheitsperspektive nicht mehr verantwortet werden. Hinzu kommt, dass das sogenannte Reverse Engineering bzw. das Dekompilieren dieser Software in der Regel verboten ist. Mit einem Grundsatzurteil des EuGH aus dem Jahr 2021 wird dieses Verbot jedoch aufgeweicht. So ist es nun rechtmäßigen Erwerbern erlaubt, Fehler in einem Computerprogramm zu beheben und dafür auch proprietäre Software zu dekompilieren. In der Praxis sollte dieses Notfallszenario aber nicht in die Planung einbezogen werden, da die Fehlerbehebung innerhalb fremder Software unter dem Rückgriff auf Dekompilierung besondere Kenntnisse seitens des zuständigen IT-Personals voraussetzt. Selbst wenn, wie bei Freier Software, der Quelltext vorliegt, dauert es einige Zeit sich soweit darin einzuarbeiten, bis Fehler selbst behoben oder gar neue Funktionen hinzugefügt werden können.\n\n\nSoftware-Abhängigkeiten\nSowohl der Betrieb von proprietärer als auch von Open-Source-Software ist vom Funktionieren einer Vielzahl weiterer Software-Komponenten abhängig. Diese Abhängigkeit lässt sich mit einem vereinfachten Schichtmodells des Betriebs eines IT-Systems illustrieren:\n\n\n\n\n\n\nAbbildung 3.3: Schichtmodell-Bild (Platzhalter)\n\n\n\nAus Abbildung 3.3 wird deutlich, dass moderne Software-Systeme zum Beispiel auf einem Betriebssystem oder weiteren Subsystemen wie einem Datenbankmanagementsystem basieren. Um das gesamte IT-System betreiben zu können, müssen die Einzelkomponenten zusammenspielen. Fällt eines der Systeme, beispielsweise das Betriebssystem, aufgrund von Obsoleszenz aus, so ist es unter Umständen möglich, die darüber liegenden Schichten auf ein neues Betriebssystem zu migrieren, jedoch ist dies nicht garantiert.\nDas Risiko erhöht sich, wenn im Rahmen eines Wartungsvertrags durch den Hersteller festgelegt wurde, dass zum Beispiel nur bestimmte Kombinationen aus Betriebssystem und weiterer Komponenten zugelassen sind. In diesem Fall kann ein IT-System aus der Wartung fallen, obwohl es vorerst betreibbar bleibt. Mit dem Ausfall der Wartung entfallen auch Software-Updates etc. Damit ist der mittel- bis langfristige Weiterbetrieb des Systems ohne Gefährdung der Betriebssicherheit aller IT-Systeme der Organisation nicht möglich.\n\n\nRechtliche Rahmenbedingungen\nDie meisten Bibliotheken befinden sich in öffentlicher Hand und sind deshalb bestimmten Gesetzen und Verordnungen unterworfen. Von besonderer Bedeutung sind dabei Anforderungen an die Software-Ergonomie und die Barrierefreiheit (Accessibility) von IT-Systemen.\n\n\nSoftware-Ergonomie\nDie gesetzliche Unfallversicherung fordert z.B. die Berücksichtigung ergonomischer Grundsätze bei der Entwicklung von Software. Moderne grafische Anwendungen müssen ebenso wie Internetseiten diese Anforderungen erfüllen:\n\nDie Software muss gebrauchstauglich sein, das heißt, sie sollte gewährleisten, dass Benutzer festgelegte Ziele in einem bestimmten Nutzungskontext effektiv, effizient und zufriedenstellend erreichen können. Dies setzt voraus, dass die Grundsätze der Dialoggestaltung nach DIN EN ISO 9241-110, wie Aufgabenangemessenheit, Selbstbeschreibungsfähigkeit, Steuerbarkeit, Fehlertoleranz, Erwartungskonformität, Individualisierbarkeit, Lernförderlichkeit beachtet und realisiert werden.\n– (Gesetzliche und Unfallversicherung e.V. (DGUV) 2019)\n\nDie Erreichung dieser Ziele wird im Kapitel Anforderungsanalyse thematisiert.\n\n\nBarrierefreiheit\nNeben dem Befolgen der Anforderungen an ergonomisch bedienbare Software, liegt es auf der Hand, dass IT-Systeme für eine Vielzahl von Anwender*innen nutzbar sein sollte. Diese grundlegende Anforderung bezeichnet man als Barrierefreiheit bzw. Accessibility.\nWährend Barrierefreiheit häufig mit einem sehr engen Behinderungsbegriff assoziiert wird, wie z.B. die Rampe für Rollstuhlfahrer*innen, ist dieser Begriff mittlerweile aufgrund der gesetzlichen Grundlagen in Deutschland wesentlich weiter zu fassen (siehe §3 Behindertengleichstellungsgesetz). So leiten sich z.B. auch Anforderungen an Angebote in Leichter Sprache o.ä. aus diesem weiten Behinderungsbegriff ab.\nAufbauend auf der einschlägigen Gesetzgebung regelt die BITV 2.0 (Verordnung zur Schaffung barrierefreier Informationstechnik nach dem Behindertengleichstellungsgesetz) die konkrete Gestaltung barrierefreier IT-Systeme und Webangebot. Hierbei greift sie auf die aktuell gültigen Web Content Accessibilty Guidelines (WCAG) zurück, welche die Anforderungen der Barrierefreiheit anschaulich mit vielfältigen Beispielen illustriert.\nDie gesetzliche Anforderung Barrierefreiheit umsetzen zu müssen trifft dabei nicht nur auf Organisationen der öffentlichen Verwaltung, wie es viele Bibliotheken sind, zu. Vielmehr müssen sich alle Stellen, die europäisches Vergaberecht anwenden müssen (siehe EU Richtlinie 2016/2102), z.B. im Rahmen von Drittmitteln oder Zuwendungen, nach diesen Vorgaben richten.\nWichtig ist hierbei zu beachten, dass die BITV 2.0 nicht zwischen internen und externen Nutzer*innen unterscheidet. Das heißt, dass sowohl rein bibliotheksintern genutzte Systeme als auch nach außen gerichtete Anwendungen, wie z.B. Discovery-Systeme, barrierefrei zu gestalten sind.\nNeben der Ermöglichung der digitalen Teilhabe für den Großteil der Bevölkerung sollen abschließend noch drei weitere positive Aspekte der Beachtung von Barrierefreiheitsanforderungen genannt werden:\n\nViele Umsetzungen der Grundsätze der Barrierefreiheit, erhöhen auch die Usability für Menschen ohne Behinderung,\nebenso wird zumeist die Nachnutzbarkeit auf Mobilgeräten erhöht und\ndie Gestaltung barrierefreier Anwendungen schlägt sich positiv im Suchmaschinenranking nieder, da Suchmaschinen diesen Aspekt zur Bewertung heranziehen.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IT-Management</span>"
    ]
  },
  {
    "objectID": "management.html#management-der-bibliotheks-it",
    "href": "management.html#management-der-bibliotheks-it",
    "title": "IT-Management",
    "section": "Management der Bibliotheks-IT",
    "text": "Management der Bibliotheks-IT\nDie Auswahl und Implementierung sowie der Betrieb von digitalen Diensten ist ein stetig wachsender Aufgabenbereich für Bibliotheken. Durch den Verlust ihres früheren Monopols auf die Versorgung mit Informationen ist ab Ende der 1990er Jahre in den Bibliotheken ein starker Innovationsdruck entstanden. In der Folge wurden neue Dienstleistungen wie fachliche Portale, Dokumentenserver etc. im Rahmen von Projekten realisiert. Die notwendigen Kenntnisse haben vielerorts eigens dafür eingestellte Mitarbeiter*innen eingebracht. Eine systematische Ausweitung von Kenntnissen zu IT-Systemen, Metadatenmanagement, Web-Standards, Usability und User Experience bleibt jedoch für die meisten Bibliotheken eine große Herausforderung.\n\nKompetenzen\nDer Einsatz von IT-Systemen erfordert dezidierte Kenntnisse und Fähigkeiten. Veränderungen der Systemlandschaft, z.B. durch einen Systemumstieg oder die Einführung neuer Systeme, erfordern daher eine regelmäßige Analyse der vorhandenen und benötigten IT-bezogenen Kompetenzen.\nBei der Analyse sind folgende Bereiche zu unterscheiden:\n\nBasisinfrastruktur für Hard- und Software: Infrastruktur Netze/Hardware, Netzdienste, Server, Basis-Software, Storage, Backup\nSystemadministration: Installation und Betrieb\nInbetriebnahme und Individualisierung: Metadatenmanagement, Konfiguration und Parametrisierung, Software-Entwicklung\nProjektmanagement\n\nFür die Systemadministration werden Kenntnisse zu grundsätzlichen Architekturen webbasierter Systeme benötigt. In der Regel handelt es sich dabei um die Installation und Administration von Datenbanken, PHP- und Java-Systemen auf Linux-Servern, einschließlich der jeweiligen Wartung durch Minor und Major Updates, Patches etc.\nFür die Inbetriebnahme und Individualisierung von Systemen sind zunächst Kenntnisse bei der Umwandlung von Daten in unterschiedliche Formate notwendig, wenn eine Datenmigrationen aus Altsystemen durchgeführt wird. Die Systeme müssen dann konfiguriert und parametrisiert werden, das heißt auf die konkreten Nutzungsszenarien angepasst. Hier ist zwischen einer so genannten Out-of-the-Box-Verwendung, bei der das System im Rahmen seiner Möglichkeiten genutzt wird, und einer weitergehenden Individualisierung durch eigene Software-Entwicklung zu unterscheiden. In beiden Fällen ist ein Zusammenspiel von bibliotheksfachlichen und Software-technischen Kompetenzen erforderlich, um das Verständnis von bibliothekarischen Geschäftsgängen und den Prozessen und Funktionalitäten des Systems zusammen zu bringen. Daher werden in der Regel Implementierungsteams aus Anwender*innen und Software-Betreuer*innen bzw. Entwickler*innen gebildet.\nDie Arbeit dieser Implementierungsteams sollte idealerweise nach Grundlagen des Projektmanagements und des Lebenszyklus von IT-Systemen erfolgen, also unter Berücksichtigung klarer Strukturen für die Planung, die Kommunikation und die Kontrolle. Siehe dazu auch das Kapitel [IT-Entwicklung].\n\n\n\n\n\n\nNiemand kommt als IT-Expert*in auf die Welt und es ist unmöglich, bei allen Entwicklungen auf dem Laufenden zu bleiben. Versuchen Sie ihre Kompetenzen realistisch einzuschätzen und scheuen Sie sich nicht Kolleg*innen um Rat zu fragen!\n\n\n\n\n\nOrganisation\nGrößere Bibliotheken haben in der Regel eigene IT-Abteilungen, die folgende Aufgaben übernehmen:\n\nAnwendungsbetreuung bei Hard- und Software sowie Peripheriegeräten,\nBetreuung von eigenen Servern,\nKonfiguration und Parametrisierung von Systemen,\nggf. Software-Entwicklung.\n\nAlle Punkte außer der Konfiguration und Parametrisierung von Systemen werden in der Regel von informatisch ausgebildetem Personal vorgenommen. Die Konfiguration und Parametrisierung von Systemen ist klassischerweise eine Aufgabe so genannter Systembibliothekar*innen, die teilweise in Ergänzung zu ihrer bibliothekarischen Ausbildung auch über formalisiert erworbene Zusatzqualifikationen verfügen und auf diese Weise eine ideale Schnittstelle zwischen den bibliothekarischen Anwender*innen und den Systemen sind.\nDie Betreuung eigener Server wird häufig von übergeordneten Einrichtungen wie universitären, städtischen oder regionalen Rechenzentren oder externen Dienstleistern übernommen. Bei kleinen Bibliotheken werden oftmals auch andere Tätigkeiten von externen Dienstleistern durchgeführt.\nDer Umgang mit dem Mangel an IT-Fachkräften wird für die Ressourcenplanung des IT-Managements in Bibliotheken zur Herausforderung werden. Dabei wird auch Open-Source-Software, die in der Community entwickelt und unterstützt wird, eine größere Rolle spielen, ebenso wie externe Dienstleister und Software as a Service. Eine umfassende Bedarfsanalyse bei IT-Systemen wird daher zukünftig noch stärker berücksichtigen müssen, wie eine längerfristige Betreuung von eingesetzten Systemen ggf. auch ohne eigenes Personal gewährleistet werden kann.\n\nRessourcenplanung\nVeränderungen des Status Quo durch einen Systemwechsel, neue Funktionalitäten oder auch personelle Änderungen durch neue Mitarbeitende, Berentung o.ä. beeinflussen die Personal- und Ressourcenplanung. Eine kontinuierliche Beschäftigung mit diesem Thema ist notwendig, um das Betriebsrisiko von IT-Systemen zu minimieren. Die Benutzung von gut etablierter und weit verbreiteter freier Software verursacht Kosten durch Kompetenzaufbau oder -einkauf, macht unabhängig in Bezug auf Datenhoheit, Datensicherheit und vor allem die Weiterentwicklung. Die Lizenznahme eines kommerziellen Produktes lässt grundsätzlich weniger Individualisierung zu und verlagert die Verantwortung für die Verfügbarkeit und Betriebssicherheit auf den jeweiligen Anbieter.\nFür die Planungen muss entsprechend regelmäßig der Bedarf analysiert werden:\n\nSysteme: Welche Kompetenzen erfordert der Betrieb der Systeme?\nPersonal: Wie viel Personal steht mit welchen Kompetenzen zur Verfügung?\nWie verteilen sich die Kompetenzen auf das vorhandene Personal?\nWie hoch ist die Übereinstimmung bei vorhandenen und benötigten Kompetenzen?\nWelche Weiterbildungen sind erforderlich?\n\nDas Thema Aus- und Weiterbildung sowie die Personalgewinnung wird im Folgenden ausführlicher betrachtet.\n\n\n\nAus- und Weiterbildung\nIn der Einleitung wird Cody Hanson (2015) zitiert: „Most importantly, all library staff must understand that our software is our library, and is everyone’s responsibility.“ Bezogen auf Einarbeitung und Weiterbildung bedeutet das, dass sich Mitarbeitende mit der (Weiter-)Entwicklung von Software ebenfalls weiterbilden und weiterentwickeln. Nur so kann die Verantwortung von allen Mitarbeitenden mit Bezug zur Bibliotheks-IT gemeinsam getragen werden.\nNachfolgend werden aktuelle Beispiele zur Aus- und Weiterbildungen mit Bezug zum bibliothekarischen IT-Bereich aufgeführt. Nicht betrachtet werden Szenarien wie die Einarbeitung von Anwender*innen von IT-Systemen bei der Einführung oder dem Wechsel von Systemen.\n\nAusbildungsmöglichkeiten und Zusatzqualifizierung\nHistorisch gibt es keine formalisierte Ausbildung für die erwähnten Systembibliothekar*innen. Die notwendigen Kenntnisse werden klassischerweise im Rahmen von „Training on the Job“ erworben.\nAllgemeine Ausbildungen und Studiengänge im Bereich IT und Data Science bieten eine gute Grundlage, decken aber bibliotheksspezifische IT-Themen nur unzureichend ab. Stand 2022 gibt es mehrere spezielle Ausbildungsangebote für die Arbeit in der Bibliotheks-IT mit unterschiedlichen Schwerpunkten:\n\nberufsbegleitende Master-Studiengänge:\n\nBibliotheksinformatik an der TH Wildau\n\nVollzeit-Studiengänge:\n\nMasterstudiengang Digitales Datenmanagement an der FH Potsdam und HU Berlin\n\nKurse\n\nZertifikatskurs Data Librarian an der TH Köln\nZertifikatskurs Forschungsdatenmanagement an der TH Köln\n\n\nDiese Zusatzqualifizierungsmöglichkeiten sind eine sehr gute Möglichkeit, um vorhandene Mitarbeiter*innen systematisch weiterzuentwickeln und die IT-Kenntnisse in der Bibliothek zu verbreiten. Der Erwerb dieser Abschlüsse ist jedoch zeitaufwändig und passt nur für wenige Lebenssituationen.\n\n\nWeiterbildung\nBibliothekarische Ausbildungsstätten sowie Verbundzentralen sind wichtige Akteure bei der Weiterbildung von Bibliothekspersonal und machen teilweise entsprechende gezielte Weiterbildungsangebote. Dabei handelt es sich in der Regel um ein- oder halbtägige Angebote, die durchaus im Einzelnen Hilfestellung bieten. Für Mitarbeitende mit Bezug zur Bibliotheks-IT sollten ausdrücklich zeitliche und ggf. finanzielle Ressourcen für die Nutzung dieser Angebote bereitgestellt werden.\nAuch der Besuch von Konferenzen ist eine wichtige Säule der Weiterbildung. Im Kontext der Bibliotheks-IT hervorzuheben sind hier\n\nJahrestagungen der Verbundzentralen\nBibliotheksferenzen wie BiblioCon und Österreichischer Bibliothekskongress\nTagung der European Library Automation Group (ELAG)\nCode4Lib-Konferenzen in den USA\nAccess Conference in Kanada\nUKSG Annual Conference in Großbritannien\nJahrestreffen der UXLibs in Großbritannien\n\nDie wichtigste Rolle bei der Qualifizierung für die Aufgaben im Bereich Bibliotheks-IT dürfte die informelle Weiterbildung spielen. Informelle Weiterbildungsformen sind\n\nAnwendungstreffen: z.B. jährlich für DSpace, VuFind, Koha, FOLIO, Kitodo, OPUS …\nMailinglisten, Foren und andere Kommunikationskanäle wie zum Beispiel das deutschsprachige Metadaten-Forum https://metadaten.community/.\npersönliche Kontakte, Gruppen wie UX Roundtable\nLibrary Carpentries\nFachpublikationen: Code4Lib Journal, Weave Journal\nSoziale Medien: Weblogs, Instagram, Mastodon, Discord…\n\n\n\nPersonalgewinnung\nDie Gewinnung von Personal für die Aufgaben im Bereich der digitalen Dienste ist neben der Aus- und Weiterbildung eine zweite Herausforderung. Die Gehaltsstruktur im öffentlichen Dienst ist für informatisch ausgebildetes Personal nicht unbedingt wettbewerbsfähig, so dass viele ausgewiesene IT-Stellen nur schwer besetzt werden können. Eine unmittelbare Reaktion darauf kann sein, die Vorteile der Beschäftigung im öffentlichen Dienst besser herauszuarbeiten (nicht-kommerzielles Umfeld, gesellschaftliche Relevanz der Tätigkeiten).\nDennoch ist es erwartbar, dass Aufgaben im Bereich Bibliotheks-IT künftig stärker an Verbundzentralen oder externe Dienstleister outgesourct werden müssen.\nDie Ausrichtung der bibliothekarischen Studiengänge wird die Bedarfe bei den digitalen Diensten noch stärker berücksichtigen und Studierende mit einem erhöhten Interesse an den Aufgaben in der Bibliotheks-IT rekrutieren müssen.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IT-Management</span>"
    ]
  },
  {
    "objectID": "management.html#zusammenfassung-und-ausblick",
    "href": "management.html#zusammenfassung-und-ausblick",
    "title": "IT-Management",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nAuch nach Auswahl eines Systems ist eine permanente Beobachtung des Lebenszyklus erforderlich. Es empfiehlt sich immer eine frühzeitige Reaktion auf sich ändernde Anforderungen. Das Wissen um das System und um seine Anwendung müssen ebenfalls aktuell gehalten werden, z.B. durch entsprechende Fortbildungen oder Schulungen. Sollte sich ein System-Umstieg abzeichnen, sind vor allem die internen Arbeits-Prozesse zu berücksichtigen: das Wissen der Systemanwendenden und -betreuenden ist somit unverzichtbar, denn nur dadurch kann auf eine Ablösung bzw. Anpassung des Systems effektiv reagiert werden.\n\n\n\n\nGesetzliche, Deutsche, und Unfallversicherung e.V. (DGUV). 2019. „Bildschirm- und Büroarbeitsplätze: Leitfaden für die Gestaltung“. https://publikationen.dguv.de/widgets/pdf/download/article/409.\n\n\nHanson, Cody. 2015. „Opinion: Libraries are Software“. 2015. https://www.codyh.com/writing/software.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>IT-Management</span>"
    ]
  },
  {
    "objectID": "anforderungen.html",
    "href": "anforderungen.html",
    "title": "Anforderungsanalyse",
    "section": "",
    "text": "Einleitung\nIm Kapitel Management von IT-Systemen wurde bereits auf Themen wie Barrierefreiheit und software-ergonomische Anforderungen sowie den permanenten Anpassungsbedarf an Systeme im Laufe ihrer Lebenszeit eingegangen.\nBetrachtet man sein persönliches Nutzungsverhalten im digitalen Bereich, wird klar, dass sich auch die eigenen Präferenzen bezüglich der Nutzung von Apps oder Webseiten ändern. Ursachen dafür sind beispielsweise Veränderungen an Lebens- oder Arbeitskontexten, Erwartungen an die Bedienbarkeit von Systemen oder neue Nutzungsformen von Medien, die durch technischen Fortschritt und digitale Transformation möglich gewordene sind.\nIT-Entwicklung in Bibliotheken sollte sich daher in erster Linie an den Bedürfnissen von Nutzer*innen ausrichten. Es gibt verschiedene Methoden, die entsprechenden Bedarfe und Anforderungen zu ermitteln und sie in die Entwicklung einzubeziehen. Dazu gehören unter anderem der Einsatz von Personas, Use Cases oder Storyboards. Weitere Methoden sind zum Beispiel Storyboards, Wireframes oder auch Prototypen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anforderungsanalyse</span>"
    ]
  },
  {
    "objectID": "anforderungen.html#nutzerinnenorientierte-gestaltung",
    "href": "anforderungen.html#nutzerinnenorientierte-gestaltung",
    "title": "Anforderungsanalyse",
    "section": "Nutzer*innenorientierte Gestaltung",
    "text": "Nutzer*innenorientierte Gestaltung\nNutzer*innenorientierte Gestaltung heißt, die Bedürfnisse von Nutzenden in den gesamten Entwicklungsprozess einzubeziehen. Das bedeutet, dass deren Bedarfe nicht nur als Quelle von initialen Anforderungen dienen, sondern kontinuierlich in den Entwicklungsprozess einbezogen werden. Hierbei ist es besonders wichtig, die Fähigkeiten und Bedürfnisse der Nutzenden sowie ihre Arbeitskontexte und -aufgaben in den Entwurf von IT-Systemen einzubeziehen. Diese Aspekte finden sich auch in den zugrunde liegenden Definitionen, wie der Usability (siehe Abschnitt Was beeinflusst den Nutzungseindruck?) wieder.\nBeim nutzer*innenorientierten Design oder dem User-Centered Design (UCD) handelt es sich nicht im formale Methoden im engeren Sinn, sondern um eine Sammlung i.d.R. empirisch abgesicherter Techniken mit drei Kernideen (Gould und Lewis 1987):\n\nFokussierung auf Nutzer*innen und deren Aufgaben von Beginn der Entwicklung an\nderen kontinuierliche Einbeziehung und Auswertung von Nutzer*innen-Feedback sowie Performance-Messung\nNutzung eines iterativen Design-Prozesses\n\nKling und Star (1998) ergänzen, dass die ganz individuellen Fähigkeiten der Nutzenden in Betracht gezogen werden müssen, was allein schon aus Gründen der digitalen Teilhabe sinnvoll erscheint.\nGenerell zielt UCD darauf ab, interaktive Systeme zu entwickeln, welche einfach zu nutzen und nützlich sind. Hierbei wird ein Fokus auf Aspekte wie Effektivität, Effizienz und Zufriedenheit gelegt (DIN 2020). Diese Aspekte werden im Abschnitt Was beeinflusst den Nutzungseindruck? weiter erläutert.\nDas Central Digital and Data Office des Vereinigten Köngreichs fasst die zentral zu bearbeitenden Arbeitspunkte im nutzer*innenzentrierten Gestaltungsprozess und den Weg dahin prägnant in seinen „Government Design Principles“ zusammen „Government Design Principles. GOV.UK“ (2012):\n\nStart with user needs\nDo less\nDesign with data\nDo the hard work to make it simple\nIterate. Then iterate again\nThis is for everyone\nUnderstand context\nBuild digital services, not websites\nBe consistent, not uniform\nMake things open: it makes things better\n\nUnter der oben genannten Website des Central Digital and Data Office finden sich auch umfangreiche Hinweise, wie sich die einzelnen Punkte praktisch umsetzen lassen.\n\nWas beeinflusst den Nutzungseindruck?\nGut bedienbare, interaktive Systeme sollen Zufriedenheit auslösen und zugänglich sein. Die Erreichung dieser Ziele und zentrale Begriffsdefinitionen sind Teil des Arbeits- und Forschungsgebiets der Software-Ergonomie und finden sich in den einschlägigen Normen wie der DIN EN ISO 9241-11 wieder (DIN 2020).\nVon zentraler Bedeutung sind dabei zwei Kernbegriffe: die Usability (Gebrauchstauglichkeit) und die User Experience (Nutzer*innenerfahrung)\n\n\n\n\n\n\nUsability ist das „Ausmaß, in dem ein System, ein Produkt oder eine Dienstleistung durch bestimmte Benutzer in einem bestimmten Nutzungskontext genutzt werden kann, um festgelegte Ziele effektiv, effizient und zufriedenstellend zu erreichen“ DIN EN ISO 9241-11\nUser Experience bezeichnet die „Wahrnehmungen und Reaktionen einer Person, die aus der tatsächlichen und/oder der erwarteten Benutzung eines Produkts, eines Systems oder einer Dienstleistung resultieren“ DIN ISO 9241-210:2011\n\n\n\nBei Usability handelt es sich um eine Eigenschaft eines Systems, die während der konkreten Interaktion mit diesem relevant wird und beispielsweise angibt, inwiefern Hürden bei der Bedienung auftreten (Abbildung 4.1). Es ist also keine Produkt- sondern eine Gebrauchsqualität, die sich erst während der Nutzung zeigt.\nZur Vermeidung von Usability-Problemen existieren ein Vielzahl von Heuristiken, die in den einschlägigen Normen skizziert werden bzw. durch Autoren wie Shneiderman und Plaisant (2005) in ihren „8 golden rules“ oder Nielsen mit seinen „10 Heuristics“ benannt werden.\nDie User Experience hingen bezieht sich auf die Wahrnehmung der Nutzenden sowohl vor, nach und auch während der Interaktion. Sie bezeichnet sozusagen die Positionierung gegenüber einem System und hat damit Auswirkungen darauf, ob Nutzende ein System erneut benutzen werden oder, z. B. aufgrund von schlechter Bedienbarkeit, d. h. schlechter Usability, vor einer zukünftigen Nutzung zurückschrecken. Es reicht folglich nicht aus, einzelne Aspekte einer Nutzer*innenschnittstelle zu optimieren. Vielmehr muss der gesamte angebotene Service aus Sicht der Nutzenden optimiert werden, damit sich ein positives Nutzungserlebnis einstellt. Diese Optimierung beschränkt sich dabei nicht nur auf die digitalisierten Anteile eines Services sondern bezieht alle Arbeitsschritte, egal ob analog oder digital, mit ein.\n\n\n\n\n\n\nAbbildung 4.1: Zusammenhang zwischen Usability und User Experience",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anforderungsanalyse</span>"
    ]
  },
  {
    "objectID": "anforderungen.html#einbeziehung",
    "href": "anforderungen.html#einbeziehung",
    "title": "Anforderungsanalyse",
    "section": "Wie beziehen wir unsere Nutzer*innen ein?",
    "text": "Wie beziehen wir unsere Nutzer*innen ein?\nMit der Einführung einer neuen IT-Lösung werden bestimmte strategische Ziele verfolgt wie die Ablösung eines veralteten Systems, die Einführung einer neuen Dienstleistung und dergleichen. Die konkrete Ausgestaltung dieser strategischen Ziele sollte unter Einbeziehungen der beabsichtigten Nutzenden erfolgen. Die konsequente Bedarfsorientierung sichert die Qualität der Dienste und verhindert, dass eigene Bedürfnisse und Einschätzungen von Expert*innen die Entwicklung dominieren. Für die Einbeziehung von Nutzer*innen gibt es verschiedene Methoden, die im Folgenden kurz dargestellt werden sollen.\n\nBedarfsermittlung\n\nKlassische Methoden zur Bedarfsermittlung\nZu den in Bibliotheken auch jenseits der Entwicklung von digitalen Diensten häufig genutzten Methoden der Bedarfsermittlung gehören qualitative und quantitative Befragungen sowie Beobachtungen. Diese Methoden sind aus der empirischen Sozialforschung entlehnt. Für viele Software-Projekte sind groß angelegte Befragungen zu aufwändig, allerdings ist es empfehlenswert, sich über Studien aus vergleichbaren Projekten zu informieren und daraus nach Möglichkeiten Ableitungen für eigene Zielsetzungen zu entwickeln.\nBeobachtungen können sehr flexibel angelegt und geplant werden. Dadurch können valide Ergebnisse mit vertretbarem Aufwand produziert und die Studie bei Bedarf gut skaliert werden. Der Fokus bei solchen Studien liegt darauf, Nutzer*innen in ihrem Arbeitsalltag zu beobachten, um ihre Herangehensweise bei der Lösung von Aufgaben und Problemen zu ermitteln. Übertragen auf digitale Dienste kann das zum Beispiel im Rahmen eines Usability-Tests passieren, in dem eine oder mehrere Personen ein System nutzen. Typischerweise werden während des Tests nicht nur Notizen oder Aufnahmen gesichert, sondern die Tester*innen nutzen die Think-Aloud-Methode. Dabei sollen Nutzende in Echtzeit laut kommentieren, was sie denken, sehen und tun (siehe Abschnitt Methoden).\nFokusgruppen dagegen sind eine qualitative Methode, in der Vertreter*innen verschiedener Zielgruppen gemeinsam an einem bestimmten, vorher formulierten Thema arbeiten. Das können sowohl Diskussionen über Anforderungen und Wünsche an ein bestimmtes System sein, als auch die Planung von Einsatzszenarien oder Workflows. Durch die freie Wahl von Themen und Mitgliedern, z. B. Nutzende ohne Vorerfahrungen und/oder Expert*innen, sind Fokusgruppen ebenfalls eine sehr flexible, breit anwendbare Methode.\n\n\nBedarfsermittlung mit Personas und Use Cases\nPersonas sind fiktive Persönlichkeiten, die stellvertretend für einzelne Zielgruppen eines Dienstes entwickelt werden. Die Beschreibungen enthalten vielfältige Informationen über die Persona und laden damit dazu ein, den zu entwickelnden Dienst aus der Perspektive der jeweiligen Persona zu beurteilen, jenseits von abstrakten Anforderungen. Darüber hinaus helfen Personas dabei, Prioritäten zu setzen und die Zielerreichung zu überprüfen. Es empfiehlt sich, für jedes strategische Ziel eine Persona zu erstellen, mindestens drei bis fünf Personas insgesamt.\nAbgeleitet von solchen Personas fällt es häufig leicht, konkrete Use Cases für die Interaktion mit einem System zu definieren. Ein Use Case beschreibt dabei eine Reihe von Aktionen, die eine Person in bzw. mit einem System durchführen kann. Das kann beispielsweise in einem Fließtext passieren, in dem ein Szenario beschrieben wird.\n\n\n\n\n\n\nAbbildung 4.2: Bild einer Personabeschreibung aus einem Vortrag zum Szenario basierten Design\n\n\n\nAußerdem kann es sich lohnen, solche Use Cases zu visualisieren. Dabei können Start, Ende, mögliche Verzweigungen, alternative Aktionen und mehr mit verschiedenen Formen modelliert werden. Dafür können formalisierte Systeme wie die Unified Modelling Language (UML) zum Einsatz kommen. Sie bietet ein Set verschiedener Formen, um Start, Ende, Verzweigungen, Alternativen und mehr visuell zu beschreiben. Aber auch Skizzen können Nutzungsszenarien bereits verdeutlichen und als Diskussionsgrundlage dienen, z. B. in Form von Storyboards, die in einem eigenen Unterkapitel zu dieser Methode noch beschrieben werden.\nUse Cases können sowohl als Grundlage für den Entwicklungsprozess dienen als auch für die Evaluation eines Systems (siehe Abschnitt Evaluierung). Für die Nutzenden-Personas einer Bibliothek kann eine breite Palette von Use Cases existieren. Manche sind dabei eher allgemein zu verstehen, andere bibliotheksspezifisch und natürlich sind alle je nach Einrichtung bzw. Anforderungen beliebig erweiterbar. Zu beachten ist, dass sowohl Personas als auch Use Cases zwingend auf der Grundlage vertrauenswürdiger Daten wie denen aus der Bedarfsermittlung erstellt werden sollten. Solche Methoden ohne Kenntnisse der Zielgruppen anzuwenden kann nur zur Reproduktion der eigenen Meinung führen.\n\n\n\nMethoden\nTestaufgaben für Usability-Tests werden erstellt, um typische Nutzungsszenarien mit Hinblick auf die Usability des Systems hin zu überprüfen. Die folgenden Methoden können relativ einfach umgesetzt werden, generieren jedoch bereits wertvolle Erkenntnisse.\n\nThink-Aloud-Methode\nDie zentrale Idee der Think-Aloud-Methode ist, dass Proband*innen während der Interaktion mit dem zu evaluierenden System ihre Meinungen, Gedanken und Gefühle laut aussprechen.\nDadurch wird es den Beobachter*innen ermöglicht, zuvor unsichtbare, kognitive Prozesse der Proband*innen zu beobachten sowie einen Einblick in typische Nutzungsweisen zu gewinnen. Durch die Verbalisierung und Beschreibung des Systems durch die Nutzenden lernt man zeitgleich die Nutzer*innenterminologie für bestimmte Sachverhalte kennen, die teils erheblich von der Fachsprache abweichen wird. Die Ergebnisse der Methode können z. B. durch Notizen oder Audioaufnahmen festgehalten werden.\n\n\nCo-Discovery Learning\nDie Kernherausforderung bei der Erstellung von Think-Aloud-Protokollen ist es, die Proband*innen kontinuierlich zu motivieren, selbst kleinste Gedanken zu verbalisieren. Beim Co-Discovery Learning arbeiten zwei Testpersonen gleichzeitig an einem System und helfen sich gegenseitig bei der Erfüllung der Aufgaben. Dadurch entstehen Gespräche und gewissermaßen automatisch ein Think-Aloud-Protokoll beider Personen.\nDie Methode bildet einerseits eine realistische Arbeitssituation des gegenseitigen Helfens ab und normalisiert andererseits das laute Aussprechen von Gedanken innerhalb einer Dialogsituation.\n\n\nQuantitative Methoden\nBeobachtungsmethoden generieren primär qualitative Daten, ebenso wie viele Inspektionsmethoden. Aus Managementsicht werden jedoch oft Entscheidungen auf Grundlage von quantitativen Daten bevorzugt, da diese häufiger als Fakten wahrgenommen werden.\nEinfache, relativ leicht zu erhebende quantitative Metriken im Rahmen von Usability-Tests sind z.B.:\n\nNutzungsfehler pro Zeiteinheit,\nAnzahl nicht benötigter Befehle (Menüs, Icons, Links)\nBenötigte Zeit für den Abschluss einer Arbeitsaufgabe (insbesondere im Vergleich mit einer vorherigen Iteration)\nBenötigte Anzahl an Klicks/Links, um an ein bestimmtes Ziel zu kommen.\n\nDer „Benutzungsfragebogen ISONORM 9241/10“ bietet einen interessanten Kompromiss zwischen qualitativen und quantitativen Daten, da er qualitative Aussagen bezüglich der Usability eines Systems (z.B. Aufgabenangemessenheit und Selbstbeschreibungsfähigkeit) mithilfe einer siebenstufigen Likert-Skala abbildet. Der Fragebogen ist frei im Internet verfügbar. Beachtet werden muss, dass für belastbare quantitative Daten die Größe der Testgruppe deutlich steigen muss, um Verfälschungen durch Einzelpersonen zu vermeiden.\n\n\n\nEinbeziehung von Nutzenden in die Entwicklung\nAls Grundlage für Personas oder Use Cases und alle weiteren Schritte ist die Einbeziehung von tatsächlichen Nutzenden in die Entwicklung also bereits in einem frühen Stadium möglich und sinnvoll. Diese Einbeziehung sichert ab, dass wesentliche Ziele der Nutzenden erreicht werden und in mitunter komplexen Entwicklungsprozessen die richtigen Schwerpunkte gesetzt werden. Dafür stehen verschiedene Methoden zur Verfügung.\nNachfolgend werden drei Ansätze vorgestellt:\n\nStoryboards - Skizzierung von Interaktionskonzepten\nWireframes und Mock-Ups - Skizzen der Oberflächen\nPrototypen - erste funktionsfähige Iterationen\n\n\nStoryboards als frühe Methode\nEin Storyboard illustriert, wie ein User Interface (UI, Nutzer*innenoberfläche) auf Eingaben reagiert, ohne das Interface visuell perfekt darzustellen. Es kann genutzt werden, um in Use Cases bestimmte Aktionen zu illustrieren.\nDie Visualisierung von Interaktionsideen kann Beteiligten helfen, mögliche Abläufe nachzuvollziehen. Storyboards sind dabei oft leichter verständlich als z. B. technische Diagramme mit der oben genannten UML. Trotzdem ist darauf zu achten, dass Ideen und Konzepte für Stakeholder und Nutzende klar beschrieben werden, um Missverständnisse zu vermeiden. In dieser Form lassen sich Storyboards nutzen, um z.B. verbale Beschreibungen oder Nutzungsszenarien zu ergänzen.\nDurch die noch vage Darstellung der Idee können dann Diskussionen angeregt werden. Beispielsweise können Storyboards in Fokusgruppen vorgestellt und diskutiert oder auch in Einzelgesprächen mit verschiedenen Stakeholdern analysiert werden. Möglichst alle Fragen und Ideen sollten dabei ohne Limitierungen behandelt werden können und die Ergebnisse festgehalten werden.\nVor- und Nachteile von Storyboards im Überblick:\n\n\n\n\n\n\n\nVorteile\nNachteile\n\n\n\n\nleicht verständlich, für alle Stakeholder geeignet\nnicht jeder Use Case oder jede Interaktionsmöglichkeit ist darstellbar\n\n\nbereits im frühen Entwurfsprozess einsetzbar\ndigitale, nichtlineare Produkte (z.B. Websites) sind schwer darstellbar\n\n\nschnelle Erstellung ohne Vorkenntnisse möglich\nggf. Unklarheiten bei der Nutzung (z.B. durch unklare Symbole)\n\n\n\n\n\nWireframes und Mock-Ups\nWireframes und Mock-Ups sind Prototypen und werden vor allem dazu genutzt, erste Skizzen für Struktur, Layout und Funktionalitäten eines Interface vorzustellen. Ähnlich wie Storyboards dienen sie als einfach zu erstellende Diskussionsgrundlage, mit deren Hilfe ein Abgleich der Vorstellungen von einem System und der Gestaltungsmöglichkeiten durchgeführt werden kann.\n\n\n\n\n\n\nAbbildung 4.3: Bild eines Papierprototypen für den Relaunch des Digitalisierungssystems Kitodo\n\n\n\nEin Wireframe („Drahtmodell“) ist eine noch undetaillierte („low-level“) Ausarbeitung eines Interfaces, v. a. darauf ausgerichtet, die Positionierung der einzelnen Elemente zu planen. Daher sind z.B. Bilder oder Buttons als Kästchen dargestellt und Texte als Striche und ähnliches (siehe Abbildung 4.3). Ein Mock-Up ist, im Kontext Design, eine ausgereifte („high-level“) Version des Interfaces mit realistischen Farben, Schriftarten und Elementen. Sowohl Wireframes als auch Mock-Ups sind also rein statische Entwürfe des zukünftigen Produkts im Gegensatz zu interaktiven Prototypen, die echte Funktionalitäten enthalten.\n\n\nPrototypen\nInteraktive Prototypen besitzen als nächsthöhere Form eines geplanten Produkts bereits erste Funktionen des geplanten Interfaces. Auch hier gibt es eine Spanne von rudimentären, low-level bis hin zu ausgereiften, high-level Prototypen, die durch Iterationen schrittweise erreicht werden. Üblich ist außerdem die Unterteilung in „vertical slice“, die qualitativ hochwertige Umsetzung nur eines bestimmten Teils des Produkts, und „horizontal slice“, die prototypische Umsetzung einer möglichst großen Bandbreite des späteren Systems.\nErste Prototypen müssen dabei noch nicht zwingend programmiert werden, sondern können durch entsprechende Prototyping-Software, wie Figma oder Axure, umgesetzt werden. Diese besitzen eine Art Bausystem für Interfaces mit mehreren Ansichten, die über Aktionen wie den Klick auf einen Button verbunden werden können. So kann Nutzenden gewissermaßen ein Produkt vorgetäuscht werden, das dann mit rudimentären Funktionen bereits getestet werden kann.\nWährend des eigentlichen Softwareentwicklungsprozesses wird der anfängliche Prototyp mit jeder Iteration hochwertiger und nimmt mehr den Charakter eines vollen Systems an. Es empfiehlt sich, nach Iterationen regelmäßig zu evaluieren, ob neue Funktionen oder Änderungen noch für die Zielgruppen geeignet sind.\n\n\n\nEvaluierung\nDie vorangegangenen Abschnitte haben herausgestellt, wie wichtig es ist, regelmäßig Feedback der Nutzenden zu erhalten. Eine zentrale Datenquelle dafür ist die Begleitung eines Projekts durch Evaluierungen. Ein Beispiel für eine lebendige Evaluierungskultur ist das “User Research Center” der Harvard Library, das regelmäßig verschiedene Methoden anwendet, um Angebote gemeinsam mit Nutzenden zu evaluieren und diese öffentlich in einem Wiki teilt.\nIm Rahmen der Usability-Evaluierung entscheidet man dabei grob zwei Methoden: Beobachtungs- und Inspektionstests (Abbildung 4.1). Während erstgenannte unter Einbeziehung von Nutzer*innen durchgeführt werden, werden Inspektionstests häufig durch Usability-Expert*innen realisiert.\nAls Vorteil der Beobachtungstests erweist sich aus der Praxissicht, dass diese auch ohne eine formale Usability-Ausbildung durch engagierte Mitarbeiter*innen durchgeführt werden können. Im Folgenden soll deshalb das prinzipielle Vorgehen bei einem Beobachtungstest skizziert werden.\n\nTestgruppen\nDie Testgruppe muss die potentielle Nutzungsgruppe bestmöglich repräsentieren, jedoch nicht sehr groß sein. Die Erfahrung zeigt, dass ca. fünf Testpersonen ausreichen, um die wichtigsten Usabilityprobleme eines Systems zu identifizieren Jakob Nielsen (2000). Statt eines einzigen Tests mit vielen Teilnehmenden bieten sich daher schnell durchzuführende Tests mit wenigen Teilnehmenden an, um ein Produkt iterativ zu verbessern. Möchte man jedoch verschiedene Typen von Nutzer*innen analysieren oder quantitative Ergebnisse sammeln, muss die Gruppengröße entsprechend wachsen.\nNeben den typischen Streuungsmerkmalen wie demographischen und kulturellen Faktoren (z.B. Bildungshintergrund) bietet es sich an, Nutzer*innen auszuwählen, die über ein unterschiedliches Maß an Vorwissen über das zu entwickelnde oder verwandte Produkte verfügen. Außerdem sollten Personen integriert werden, welche von Einschränkungen betroffen sind, die im Abschnitt Accessibility thematisiert wurden.\n\n\nTestablauf und Vorbereitungen\nNach der Rekrutierung repräsentativer Nutzer*innen und der Vorbereitung der benötigten Materialien und der Testumgebung bietet sich ein Pilottest mit Proband*innen an. Dieser dient der Validierung der eigenen Annahmen über die Testaufgaben (s.u.) und die Machbarkeit des Ablaufs.\nDie Testumgebung sollte eine entspannte und natürliche Arbeitsumgebung vermitteln. Diese ist in jedem Fall einer künstlichen Laborumgebung vorzuziehen. Während der Beobachtungstests ist sicherzustellen, dass keine Unterbrechungen, z.B. in Form von Telefonanrufen erfolgen, damit die Proband*innen das zu evaluierende System konzentriert testen können.\nNach dem Beobachtungstest sollte es den Proband*innen ermöglicht werden, die Testergebnisse zu erhalten. Außerdem ist es neben dem obligatorischen Dank für die Teilnahme üblich, eine Aufmerksamkeit - je nach Dauer z. B. Kaffee, Süßes, Gutscheine - auszuhändigen, um die eigene Wertschätzung für das zeitliche Investment der Proband*innen auszudrücken. In einer Erklärung zum Datenschutz ist die anonyme Datennutzung zuzusichern.\n\n\nTestaufgaben\nWie die Testgruppen müssen auch die Testaufgaben repräsentativ für den späteren Einsatzzweck des Systems sein. Die von den Proband*innen zu bearbeitenden Testaufgaben müssen realistische Aufgaben in bzw. mit dem System sein und in der gegebenen Zeit absolvierbar sein. Dabei ist zu beachten, dass sich die Arbeitsaufgaben an tatsächlichen Use Cases orientieren und nicht trivial sind.\nDie Formulierung der Arbeitsaufgaben muss unmissverständlich für die Proband*innen sein und auf deren (mitunter variierendes) Vorwissen eingehen. Ein Pilottest hilft, dies zu überprüfen.\nDie Gestaltung der einzelnen Aufgaben sollte einer Dramaturgie folgen, um die Proband*innen während des gesamten Tests zu motivieren. Das heißt konkret, dass die ersten Teilaufgaben leicht zu lösen sein sollten und deren Schwierigkeit dann kontinuierlich zunimmt, um durch komplexere Aufgaben belastbare Aussagen zu erhalten.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anforderungsanalyse</span>"
    ]
  },
  {
    "objectID": "anforderungen.html#zusammenfassung-und-ausblick",
    "href": "anforderungen.html#zusammenfassung-und-ausblick",
    "title": "Anforderungsanalyse",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nEs gibt verschiedenste Methoden mit denen Bedarfe ermittelt und Nutzende in die Entwicklung von Software einbezogen werden können - je nach Umfang des Produkts und des Anwender*innenkreises. Usertests erfordern ein anderes Zeitmanagement als die Entwicklung von Personas. Auch der Anwendungsfall nimmt Einfluss auf die Methodenauswahl. So kann für die Entwicklung eines neuen Designs die Verwendung von Wireframes und Mockups bei der Bedarfsermittlung hilfreich sein. Wird ein Portal mit neuen Interaktionsmöglichkeiten entwickelt, empfehlen sich Prototypen, mit denen auch die Interaktionen getestet werden können.\n\n\n\n\nDIN. 2020. „DIN EN ISO 9241-110 Ergonomie der Mensch-System-Interaktion - Teil 110: Interaktionsprinzipien (ISO 9241-110:2020)“. https://www.din.de/de/mitwirken/normenausschuesse/naerg/veroeffentlichungen/wdc-beuth:din21:320862700.\n\n\nGould, J. D., und C. Lewis. 1987. „Designing for usability: Key principles and what designers think“. In Human-computer interaction: a multidisciplinary approach, 528–39. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\n„Government Design Principles. GOV.UK“. 2012. 2012. https://www.gov.uk/guidance/government-design-principles.\n\n\nJakob Nielsen. 2000. „Why You Only Need to Test with 5 Users. Nielsen Norman Group“. 2000. https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/.\n\n\nKling, Rob, und Susan Leigh Star. 1998. „Human centered systems in the perspective of organizational and social informatics“. ACM SIGCAS Computers and Society 28 (1): 22–29. https://doi.org/10.1145/277351.277356.\n\n\nShneiderman, Ben, und Catherine Plaisant. 2005. Designing the user interface. Strategies for effective human-computer interaction. 4th Aufl. Pearson.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Anforderungsanalyse</span>"
    ]
  },
  {
    "objectID": "sicherheit.html",
    "href": "sicherheit.html",
    "title": "Sicherheit & Datenschutz",
    "section": "",
    "text": "Einleitung\nIn den letzten Jahren ist die Zahl der Angriffe auf Bildungseinrichtungen, insbesondere Hochschulen und ihre Bibliotheken, deutlich gestiegen. Laut der Hochschulrektorenkonferenz (HRK) waren bis Januar 2023 insgesamt 24 Hochschulen bzw. Universitäten solchen cyberkriminellen Angriffen ausgesetzt und nahmen Schaden (MDR 2023). Informationseinrichtungen aller Sparten werden zudem mit sicherheits- und datenschutzrelevanten Aufgaben konfrontiert.\nDie Entwicklung hin zur Digitalisierung, Automatisierung und Virtualisierung zieht nicht nur die Technisierung eigener Geschäftsgänge und Dienstleistungen nach sich. Diese Entwicklung erfordert auch eine größere Sensibilität und Aufmerksamkeit hinsichtlich der Sicherheit der eigenen Systeme. Safety und Security sind hierfür wichtige, zu unterscheidende Grundprinzipien.\nBibliotheken verstehen sich als Orte, die ihre Informationen und vielschichtigen Dienstleistungen i.d.R. einer großen Nutzendenschaft zur Verfügung stellen. Sie tragen durch ihre Arbeit zur Umsetzung der Openness-Strategie in Gesellschaft und Wissenschaft bei. Offenheit und Transparenz sind schützenswerte Haltungen von Bibliotheken, welche gleichzeitig aber auch eine größere Angriffsfläche ermöglichen.\nDoch was macht Bibliotheken und Hochschulen so interessant für Angriffe? An diesen Orten werden für Cyberkriminelle interessante Daten verwaltet. Dazu gehören personenbezogene Daten von Nutzenden und Beschäftigten, schützenswerte Forschungsdaten, Bewegungsdaten, Lizenzdaten, Daten zur Nutzung von Literatur, etc. Die dabei abgegriffenen Daten können u.a. für Identitätsdiebstahl, Offenlegung privater Informationen oder für die Erstellung eines detaillierten Nutzerprofils herangezogen werden (Holländer 2023).\nIT-Sicherheit und Datenschutz sind auch immer eine Frage der Zuständigkeit. Je nach Größe, Art und Organisationsstruktur der Bibliothek muss die IT-Sicherheit mit den beteiligten Instanzen wie Stadtverwaltung, Verbundzentrale, den Rechen- und IT-Zentren der Institution und den IT-Sicherheitsbeauftragten und Datenschutzbeauftragten in Bezug auf die Verantwortlichkeiten abgestimmt werden. Grundsätzlich liegt die Aufgabe der Sensibilisierung und Schulung des Bibliothekspersonals im direkten Verantwortungsbereich der Bibliothek.\nWenig ist zum Thema IT-Sicherheit in Bibliotheken veröffentlicht worden (Kost u. a. 2022). Deshalb soll dieses Kapitel zur Vorbeugung nicht nur für das Thema IT-Sicherheit sensibilisieren, eine Beschreibung des Sicherheitsvorfalls und einen Überblick über die Richtlinien und Vorgaben geben, sondern auch für Präventivmaßnahmen werben und Handlungsempfehlungen geben. Gleiches gilt für den Datenschutz.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "sicherheit.html#einleitung",
    "href": "sicherheit.html#einleitung",
    "title": "Sicherheit & Datenschutz",
    "section": "",
    "text": "Security beinhaltet alle Maßnahmen zum Schutz vor Diebstahl oder Beschädigung von Soft- und Hardware. Safety meint den sicherheitsbewussten Umgang mit Netzwerken und Daten (Holländer 2023).\n\n\n\n\n\n\n\n\n\nAbgrenzung Datenschutzbeauftragte*r IT-Sicherheitsbeauftrage*r (Rehm (2023), Schmidt (2023))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "sicherheit.html#sicherheitsvorfall",
    "href": "sicherheit.html#sicherheitsvorfall",
    "title": "Sicherheit & Datenschutz",
    "section": "Sicherheitsvorfall",
    "text": "Sicherheitsvorfall\n\nEinfallstore\nProminente Vorfälle in verschiedenen wissenschaftlichen Einrichtungen haben in den letzten Jahren Schwachstellen in IT-Systemen offenbart, die ernstzunehmende Sicherheitslücken darstellen. Dabei ist zu beachten, dass sowohl Maschinen als auch Menschen verantwortlich für diese Lücken sein können. Folgende sind die häufigsten Angriffsmethoden:\n\nSocial Engineering: Unter Social Engineering versteht man Methoden, die zum Vertrauensgewinn eingesetzt werden, um anschließend an sensible Informationen, Zugangsdaten für Systeme oder finanzielle Mittel zu gelangen. Social Engineering-Attacken können über persönliche Kontakte oder über Webdienste wie E-Mails oder Webseiten durchgeführt werden.\nPhishing: Hierbei werden seriös wirkende E-Mails versendet, um den Empfänger*innen Kennwörter oder Zahlungsinformationen zu entlocken, um so an sensible Daten zu gelangen. Die „Qualität“ derartiger gefälschter E-Mails variiert stark. Es zeigt sich jedoch, dass sie kontinuierlich professioneller umgesetzt werden und schwieriger zu identifizieren sind. Häufig werden Phishing-Attacken mit Links auf präparierte Webseiten (sog. Watering Holes) kombiniert. In diesem Fall dienen sie etwa auch als Haupteintrittstore für Ransomware, Malware, Viren und Würmer.\nRansomware: Bei dieser Art des Angriffs wird der Zugriff auf Daten eingeschränkt oder unterbunden, z.B. durch Verschlüsselung. Für die Wiederherstellung des Zugriffs wird ein Lösegeld gefordert. Ransomware hat von allen Angriffsmethoden das größte Schadenspotential. Neben Lösegeldforderungen entstehen hohe Ausfalls- und Wiederherstellungskosten.\nAls Beispiel: Für die direkten Kosten des Ransomware-Angriffs 2019 auf die JLU Gießen werden mit Stand 2023 ca. 1,7 Mio. EUR für Schadensanalyse und -behebung kalkuliert. Zusätzliche Kosten für Aufwände und Workarounds sind in dieser Analyse nicht berücksichtigt und lassen sich schwer beziffern (Kost u. a. (2022)).\nDenial-of-Service-Attacken (DoS): Bezeichnet einen Angriff, um die Verfügbarkeit eines Netzwerkes zu beeinträchtigen und gleichzeitig Daten abzugreifen und umzuleiten.\n\n\n\nBisherige Erfahrungen\nBisherige Erfahrungen zeigen die weitreichenden Auswirkungen eines cyberkriminellen Angriffs auf Hochschulbibliotheken. In den meisten Fällen ist jedoch nicht nur die Bibliothek alleine betroffen, sondern die gesamte Hochschule. Kommt es zu einem Angriff, ist es meist notwendig, alle IT-Dienste herunterzufahren. Eine Abschottung einzelner Dienste kann kaum vorgenommen werden. Man kann sich dies als einen „harten Cut“ und ein Herunterfahren aller eigenen Server und virtuellen Maschinen zur Eingrenzung des schadhaften Fremdzugriffs vorstellen. Diese harte Maßnahme wird vorgenommen, da man nicht abschätzen kann, wo es im System schon zu welchen Schäden gekommen ist. Die Folgen: es funktioniert schlimmstenfalls NICHTS mehr. (W)LAN, Netzlaufwerke, Identity-Management und Anmeldedienste, E-Mail-Dienste, Terminverwaltungstools, Personal-Verwaltungssysteme, Türschließmechanismen, Zeiterfassung, IP-Telefonie, Lüftungs- und Beleuchtungssysteme, etc. Alle netzbetriebenen Dienste sind ggf. für mehrere Tage, Wochen oder sogar Monate außer Betrieb.\nIm Ernstfall können die vorhandenen Dienstgeräte und Kommunikationsdienste nicht mehr verwendet werden. Aufgrund dessen kann man zur Kommunikation auf private Geräte (Notebooks, Smartphones, etc., sofern die Mitarbeiter*innen bereit dazu sind) und alternative E-Mail-Dienste ausweichen. Hier ist zu bedenken, dass diese Adhoc-Lösungen nicht unbedingt den datenschutzrechtlichen Ansprüchen entsprechen. Von besonderer Bedeutung ist in diesem Fall auch das Identity Management System (IDMS), das nach einem Angriff gegebenenfalls alternativ aufgebaut werden muss.\nHinsichtlich der Bibliotheksnutzung kann das Bibliotheksmanagementsystem (BMS) z.B. auf geprüften Laptops verwendet werden, die ggf. über alternative (mobile) Netzwerke (sofern das BMS wie z.B. im GBV üblich, zentral gehostet wird und nicht ebenfalls vom Angriff betroffen ist), sodass die Ausleihe und Rückgabe wieder zeitnah ermöglicht werden können.\nZu guter Letzt darf auch nicht unterschätzt werden, welche zeitlichen Ausmaße ein Angriff einnehmen kann und welche psychischen und sozialen Auswirkungen er verursacht. Mitunter muss man mit monatelangen Einschränkungen rechnen und man kann auch nicht davon ausgehen, dass Daten überhaupt oder gar vollständig wiederhergestellt werden können.\n\n\nWo bleibt die Künstliche Intelligenz der IT-Sicherheit?\nIm Zuge der heutigen digitalen Entwicklungen wirft dieses Thema natürlich auch die Frage auf, welche Auswirkungen KI auf die IT-Sicherheit hat. Nach jetzigen Erfahrungen hat KI bislang noch nicht dazu geführt, dass Systeme anfälliger für Angriffe werden. Denkbar ist, dass Phishing-Angriffe vereinfachen kann. Im Gegenzug könnten cyberkriminelle Angriffe durch Monitoring unserer Systeme mit KI erschwert werden.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "sicherheit.html#richtlinien-und-vorgaben",
    "href": "sicherheit.html#richtlinien-und-vorgaben",
    "title": "Sicherheit & Datenschutz",
    "section": "Richtlinien und Vorgaben",
    "text": "Richtlinien und Vorgaben\nWie in jedem Bereich unseres Lebens finden sich auch in der IT-Sicherheit mehrere Richtlinien und Vorgaben sowohl auf nationaler als auch auf europäischer Ebene.\nAuf nationaler Ebene liegt die Zuständigkeit unter anderem beim Bundesamt für Sicherheit in der Informationstechnik (BSI), welches seit 1991 mit der Aufgabe betraut ist, das Regierungsnetz und die kritischen Infrastrukturen (KRITIS) zu schützen. Auf Bundesebene wurde es eine zentrale Anlaufstelle für Sicherheitsstandards, sowie Meldestelle bei IT-Krisen. Es stellt unterschiedliche Normen zur IT-Sicherung zur Verfügung. Wann welcher Standard greift, hängt von der Komplexität des Einzelfalls ab.\nFür Bibliotheken sind hierbei auch die „Checklisten zum IT-Grundschutz-Kompendium“ nach dem BSI-Standard 200-2 sehr empfehlenswert, da in diesen der aktuelle Stand der Integration des Grundschutzes überprüft werden kann. In diesem Zusammenhang wird auch auf die ISO/IEC-27000-Familie hingewiesen. In dieser Normenreihe sind neben definierten und stets aktuell gehaltenen Standards, die Anforderungen an Information Security Management Systems (ISMS), Empfehlungen für Kontrollmechanismen, als auch Best-Practices-Empfehlungen zu Aufbau und Organisation von Informationsfreiheit enthalten.\nWeitere Institutionen, die einen entscheidenden Einfluss auf die Vereinheitlichung des Datenschutzes in der Europäischen Union durch den Erlass der Datenschutz-Grundverordnung (DSGVO) von 2016 hatten, sind das Europäische Parlament und der Europäische Rat. Die DSGVO gilt seit 2018 unmittelbar in allen der EU zugehörigen Länder. Sie regelt unter anderem, dass jede Person das Recht auf Schutz der sie betreffenden personenbezogenen Daten hat. So dürfen Daten nur für einen bestimmten Zweck erhoben (Zweckbindung) und auch nicht für andere Vorhaben weiterverarbeitet werden (gem. Art 5 II DSGVO). Ferner wird in Art 37 DSGVO die Ernennung eines Datenschutzbeauftragten geregelt.\nErgänzt wird diese Grundverordnung durch die einzelnen nationalen gesetzgebenden Instanzen. In Deutschland geschieht dies sowohl auf Bundes- als auch auf Landesebene, in Form des Bundesdatenschutzgesetzes (BDSG) sowie der 16 Landesdatenschutzgesetze.\nIn Österreich wird die DSGVO ergänzt durch das Datenschutzgesetz (DSG) und das Netz- und Informationssystemsicherheitsgesetz (NISG).\nÄhnliche Gesetze wurden auch außerhalb der EU erlassen. So trat Anfang September 2023 in der Schweiz die neue Verordnung über Datenschutzzertifizierungen (VDSZ) in Kraft, welche sich an den Grundzügen der DSGVO orientiert.\nIn ihrer besonderen Rolle als Institutionen für Informationsversorgung und Bereitstellung von Wissensinfrastruktur greifen Bibliotheken auch auf die Dienste externer Anbieter zurück, z.B. durch Verträge mit Wissenschaftsverlagen über digitale Literaturangebote. Hierbei ist es wichtig, dass Bibliotheken in diesen Verträgen darauf bestehen, dass das Tracking des Nutzungsverhaltens der Forschenden ausgeschlossen wird („Tracking in der Wissenschaft: So können Bibliotheken Daten und Wissenschaftsfreiheit schützen“ (2022)), um die Wissenschaftsfreiheit und die informationelle Selbstbestimmung zu schützen (DFG 2021).\nAufgrund der stetigen Weiterentwicklung von Software und neuen Technologien hat die Europäische Kommission einen Vorschlag für ein neues Gesetz zur Widerstandsfähigkeit von Cyberangriffen, den Cyber Resilience Act, auf den Weg gebracht. Gemäß diesem Vorschlag sollen Verbraucher*innen und Unternehmen beim Kauf von Produkten und Software mit digitalen Komponenten geschützt werden, indem verbindliche Cyber-Sicherheits-Anforderungen für diese Leistungen durch die Hersteller eingeführt werden sollen. Im nächsten Schritt werden nun das Europäische Parlament und der Europäische Rat über diesen Vorschlag beraten.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "sicherheit.html#präventivmaßnahmen",
    "href": "sicherheit.html#präventivmaßnahmen",
    "title": "Sicherheit & Datenschutz",
    "section": "Präventivmaßnahmen",
    "text": "Präventivmaßnahmen\n\nPasswortsicherheit\nZu einem umfassenden IT-Sicherheitskonzept gehört, dass alle Beteiligten einen bewussten Umgang mit Passwörtern praktizieren. Passwörter sollten zum Beispiel nicht ohne Weiteres für Dritte zugänglich auf Papier oder einem anderen Medium (z. B. unverschlüsselt auf dem Computer) festgehalten werden. Bei Verdacht auf Angriff sollten alle betroffenen Passwörter umgehend geändert werden. Bei einer großen Menge an Passwörtern empfiehlt es sich, einen Passwort-Manager zu verwenden. Für die Wahl des Passworts sollten die allgemein gültigen Empfehlungen beachtet werden (beispielsweise die Passwort-Empfehlungen des BSI).\n\n\nAuthentifizierung und Autorisierung\nUnter Authentifizierung verstehen wir grundsätzlich das eindeutige Erkennen eines Zugriffs auf eine Ressource wie z.B. einen Dienst oder einen Computer, aber auch auf ein physisches Objekt wie z.B. einen Drucker.\nIn der Regel erfolgt heutzutage der Zugriff über ein Netzwerk, d. h., auch die Authentifizierung muss über das Netzwerk erfolgen. Für Geräte (Computer) kann dies durch technische Merkmale wie den Media Access Code (MAC Adresse) oder - flexibler - über die IP-Adresse durchgeführt werden. Diese Merkmale authentifizieren jedoch nur das Gerät und lassen noch keine Aussage darüber zu, wer dieses Gerät gerade benutzt.\nEine Authentifizierung einer Person erfolgt meistens über eine eindeutige Kennung und ein zugehöriges Passwort. Wenn möglich, sollten verschiedene Authentifizierungsverfahren miteinander kombiniert werden. Es wird dann auch von einer 2-Faktor-Authentifizierung (2FA) bzw. einer Mehrfaktor-Authentifizierung (MFA) gesprochen.\nIst ein Zugriff eindeutig durch entsprechende Authentifizierung erkannt, kann auf Basis verschiedener Informationen je nach Bedarf entschieden werden, ob der Zugriff auch berechtigt ist. D. h., der Zugriff muss autorisiert werden. Dies kann sehr individuell erfolgen (Konto A hat Zugriff, Konto B nicht) oder anhand der Zugehörigkeit eines Kontos zu einer bestimmten Personengruppe.\nAlle Komponenten, die zur Authentifizierung und Autorisierung in einer Einrichtung notwendig sind, werden auch als Authentifizierungs- und Autorisierungsinfrastruktur (AAI) bezeichnet. Wurde in einer Einrichtung eine AAI aufgebaut, auf welche jede Anwendung zurückgreifen kann, kann man festlegen, dass nach einmaligem Login ein Zugriff auf alle Anwendungen möglich und ein separates Einloggen nicht nötig ist. Dies wird auch als Single-Sign-On (SSO) bezeichnet.\n\n\n\nHerangehensweisen für Single-Sign-On\n\n\nLinks: Benutzer*in meldet sich auf einem Portal an und bekommt Zugriff auf alle eingebundenen Dienste. Mitte: Benutzer*in speichert alle Anmeldedaten auf einem Datenträger oder im Netzwerk. Ein lokales Programm meldet ihn*sie separat bei jedem Dienst, Portal oder Ticketing-System ein. Rechts: Benutzer*in meldet sich bei einem der Dienste an und bekommt ein Ticket für den gesamten „Kreis der Vertrauten“.\nZentrale Komponenten sind hierbei der Identity Provider (IDP), der auf Basis des dahinterliegenden IDMS eine digitale Identität inkl. notwendiger Benutzer*innenattribute bereitstellt und an dem die einmalige Anmeldung stattfindet. Die genutzten Anwendungen werden allgemein als Service Provider (SP) bezeichnet.\nEine gute Möglichkeit, das Internet sicher zu nutzen, ist das Virtual Private Network (VPN). Das VPN verschlüsselt die Identität von Internetnutzer*innen. Damit wird es Dritten erschwert, die Nutzer*innen im Internet zu verfolgen und Daten abzugreifen. VPN nutzt dabei eine Echtzeitverschlüsselung. Informationen werden in eine unleserliche Form umgewandelt und können nur mit einem Schlüssel wieder in die ursprüngliche leserliche Form gebracht werden.\n\n\nUpdates und Backups\nEine sichere Hard- und Software ist unabdingbar. Rechner sollten ausreichend geschützt sein. Dazu gehören unter anderem aktuelle Virenscanner und automatische Updates des Virenscanners, des Betriebssystems und jeder auf dem Rechner laufenden Software. Fehlende Updates können als Einfallstore genutzt werden, um Schadsoftware auf den Computer und damit auch in das Netzwerk einzuschleusen.\nFalls es zu einem Angriff oder Ausfall des Systems kommt, muss auf Backups zurückgegriffen werden. Dabei wird eine Sicherungskopie angelegt, meist auf einem anderen Medium, wie z. B. einer externen Festplatte. Server werden mittels eines RAID-Verfahrens gespiegelt, d. h. die Daten werden auf einen zweiten Server übertragen. Da es bei einem Backup auch zu Datenverlust oder -beschädigung kommen kann, empfiehlt es sich, mehr als eine Kopie anzulegen, z. B. auf Magnetbändern.\n\n\n\n\n\n\nEin Server ist ein zentraler Rechner (virtuell oder physikalisch), der Daten zur Verfügung hält und diese auf Anfrage durch einen Client (PC, mobiles Gerät) zur Verfügung stellt.\n\n\n\n\n\nSchulungen\nRegelmäßige Weiterbildungen ermöglichen es den Mitarbeiter*innen von Bibliotheken, ihre Fähigkeiten auszubauen und die Sensibilität für Themen wie IT-Sicherheit zu erhöhen. Die im Abschnitt Richtlinien und Vorgaben [interne Verlinkung] erwähnten Rahmenbedingungen schaffen dabei ein Grundgerüst für Schulungsinhalte und können für verpflichtende IT-Sicherheitsschulungen - wie sie bereits von einigen Bibliotheken durchgeführt werden - verwendet werden.\nEs sollten zwei Arten von Schulungen durchgeführt werden: für Mitarbeiter*innen und Nutzer*innen. Mitarbeiter*innen sollten dahingehend sensibilisiert werden, dass sie mit schützenswerten Daten arbeiten. Es ist bspw. wichtig, daran zu denken, dass gerade Geräte in Büros im Sichtfeld von Personen durch sinnhafte Positionierung oder Blickschutzfilter vor unbefugtem Sehen geschützt werden. Auf die Wichtigkeit sicherer Passwörter wurde bereits weiter oben in diesem Abschnitt hingewiesen. Es sollte zudem auf die Angriffsmöglichkeiten im Abschnitt „Sicherheitsvorfall“ [interne Verlinkung] aufmerksam gemacht werden. Beispielhafte Erklärungen möglicher Angriffsszenarien und Möglichkeiten (z. B. Tools) sich zu schützen, sollten aufgezeigt werden.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "sicherheit.html#handlungsempfehlungen",
    "href": "sicherheit.html#handlungsempfehlungen",
    "title": "Sicherheit & Datenschutz",
    "section": "Handlungsempfehlungen",
    "text": "Handlungsempfehlungen\nSind öffentliche Einrichtungen (Hochschulen, Kommunen) von Cyberattacken betroffen, hat dies allumfassende Folgen. Im schlimmsten Fall sind sämtliche Dienste nicht mehr verfügbar und die Ausfallzeit und Schadenshöhe weisen ins Ungewisse. Dann können Notfall- und Sicherheitskonzepte teils detaillierte Anleitungen für die Ersthilfe bieten. Generell gilt, dass Cyberattacken nicht nur eine kommunikative, sondern häufig auch eine Organisationskrise nach sich ziehen.\n\n\n\nÜbersicht über mögliche Gefahrenquellen und Maßnahmen zur Prävention und im Fall eines Schadenfalles\n\n\nZum einen muss intern (mit vorbereiteten Notfallgeräten oder ggf. privaten Endgeräten) und extern (mit Dienstleister*innen, Bibliothekspartner*innen, Bibliotheksnutzer*innen) kommuniziert werden u.a. über den Vorfall, und ggf. weiterlaufende Dienste (cloudbasierte und von Drittanbieter*innen). Besonders cloudbasierte Dienste können im Notfall evt. gute Alternativlösungen bieten, das gilt zum einen für alternative Loginverfahren z.B. bei extern gehosteten Plattformen für E-Ressourcen, wenn das IDMS nicht zur Verfügung steht.\nIst das BMS einer Bibliothek cloudbasiert, kann ein Ausleihbetrieb mit Notfallgeräten schneller wieder aufgenommen werden, da die Daten vor dem Hacker besser geschützt sind. Ähnlich kann man mit eigenen Dokumenten vorgehen, in dem das Active Directory des eigenen Systems ein tägliches Update erstellt und z.B. im CSV-Format an eine Cloud exportiert. Im einfachsten Fall betrifft es nur Notfalldokumente, die einem Krisenstab zugänglich sind. Tritt ein Notfall ein, kann man über die Standard-Benutzeroberfläche der Cloud die gesicherten Skripte starten. Der Cloudbetreiber sollte Erfahrungen mit Notfall-Lösungen und Umgebungen haben. Trotzdem sollte die Bibliothek ihre Datensicherung gut planen und cloudbasierte oder andere netzwerkunabhängige Backups länger vorhalten (mindestens 14 Tage), um auf nicht infizierte Sicherungen zurückgreifen zu können. Es ist ja nicht auszuschließen, dass die Angreifer die Zugangsdaten für die Datensicherung abgreifen konnten. Daher sollte auch über eine Verschlüsselung der Datensicherung in Erwägung gezogen werden. Genauso gilt es, die Wiederherstellung der Daten zu planen und zu üben. Unerlässlich ist auch eine gute Dokumentation der gesamten Notfall-Architektur.\nUnd zum anderen sind alternative Abläufe aufzubauen, um wieder an der digitalen Arbeitswelt teilzunehmen und den Bibliotheksbetrieb weitestgehend wieder anbieten zu können.\nDie kommunikativen Anforderungen sind wie die organisatorischen als sehr hoch einzuschätzen, da Informationseinrichtungen mit ihren analogen Begegnungsmöglichkeiten Schnittstellen für eine ganze Hochschule sein können. Für beide Aufgaben können andere Bibliotheken und externe Firmen Unterstützung bieten.\nStehen Plan und eine Notfallumgebung, ist es unerlässlich, regelmäßige Notfallübungen durchzuführen. Damit trainiert man die Abläufe und kann diese regelmäßig überprüfen und optimieren (Müller 2023). Die dadurch entstehende Routine kann Sicherheit für den Ernstfall geben. Dennoch darf man nicht unterschätzen, dass ein Angriff auch persönliche Auswirkungen auf alle Betroffenen hat.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "sicherheit.html#fazit",
    "href": "sicherheit.html#fazit",
    "title": "Sicherheit & Datenschutz",
    "section": "Fazit",
    "text": "Fazit\nLässt man die Informationen in diesem Kapitel noch einmal Revue passieren, bleibt als abschließendes Fazit frei nach Ringelnatz nur zu sagen: „Sicher ist, dass nichts sicher ist. Selbst das nicht“. Trotzdem ist man der Gefahr nicht hilflos ausgeliefert. Durch Präventivmaßnahmen, Notfall-Systeme und Pläne kann man einen Angriff wohl nicht verhindern. Man kann jedoch versuchen, den Schaden und die daraus folgenden Belastungen so gering wie möglich zu halten. In vielen Bibliotheken entstehen derzeit individuelle Sicherheitskonzepte und neue Sicherheitsmaßnahmen. Die nächsten Monate und Jahre werden zeigen, mit welchen neuen Strategien sich Bibliotheken vor Cyberangriffen schützen und sich auf diese vorbereiten können.\n\n\n\n\n\nDFG. 2021. „Datentracking in der Wissenschaft: Aggregation und Verwendung bzw. Verkauf von Nutzungsdaten durch Wissenschaftsverlage. Ein Informationspapier des Ausschusses für Wissenschaftliche Bibliotheken und Informationssysteme der Deutschen Forschungsgemeinschaft“. Deutsche Forschungsgemeinschaft. https://www.dfg.de/download/pdf/foerderung/programme/lis/datentracking_papier_de.pdf.\n\n\nHolländer, Stephan. 2023. „Cyberangriffe auf Universitäten, Fachhochschulen und deren Bibliotheken – ein unterschätztes Problem?“ B.I.T. online 26 (3). https://www.b-i-t-online.de/heft/2023-03-nachrichtenbeitrag-hollaender.pdf.\n\n\nKost, Michael, Bastian Loibl, Peter Reuter, und Matthias Stenke. 2022. „#JLUoffline. Der Cyber-Angriff auf die Justus-Liebig-Universität Gießen im Dezember 2019“. ABI Technik 42 (1). https://doi.org/https://doi.org/10.1515/abitech-2022-0005.\n\n\nMDR. 2023. „Vermehrt Hackerangriffe auf Hochschulen und Universitäten“. 25. Januar 2023. https://www.mdr.de/wissen/vermehrt-hackerangriffe-auf-hochschulen-und-universitaeten-100.html.\n\n\nMüller, Heiko. 2023. „Vorsorge für den Angriffsfall: Der Weg in die Cloud.“ iX Magazin für professionelle IT 11.\n\n\nRehm, Stefan-Marc. 2023. „IT-Sicherheitsbeauftragten bestellen. www.haufe.de“. 23. August 2023. https://www.haufe.de/compliance/management-praxis/cybersicherheit-it-sicherheitsbeauftragter_230130_447256.html.\n\n\nSchmidt, Caroline. 2023. „Wann die Bestellung eines Datenschutzbeauftragten für Ihr Unternehmen unabdingbar ist. www.e-recht24.de“. 12. Juni 2023. https://www.e-recht24.de/datenschutz/10744-datenschutzbeauftragter-dsgvo.html#.\n\n\n„Tracking in der Wissenschaft: So können Bibliotheken Daten und Wissenschaftsfreiheit schützen“. 2022. https://www.zbw-mediatalk.eu/de/2022/01/tracking-in-der-wissenschaft-so-koennen-bibliotheken-daten-und-wissenschaftsfreiheit-schuetzen.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sicherheit & Datenschutz</span>"
    ]
  },
  {
    "objectID": "metadaten.html",
    "href": "metadaten.html",
    "title": "Daten & Metadaten",
    "section": "",
    "text": "Einleitung\nFür die Sammlung und Bereitstellung von Informationen werden von Bibliotheken Ressourcen unterschiedlichster Form (Bücher, Filme, Forschungsdaten …) nachgewiesen. Zur Verwaltung der Ressourcen werden diese mit Metadaten beschrieben. Neben diesen Metadaten enthalten bibliothekarische Informationssysteme zunehmend auch die dazugehörigen digitalen Inhalte wie sogenannte Volltexte, Digitalisate und Forschungsdaten (siehe Kapitel Digitalisierung und Forschungsnahe Dienste). Viele der im Folgenden beschriebenen Grundlagen zu Eigenschaften, Arten und Verarbeitung von Daten gelten sowohl für Metadaten als auch für digitale Inhalte.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Daten & Metadaten</span>"
    ]
  },
  {
    "objectID": "metadaten.html#grundlegende-begrifflichkeiten",
    "href": "metadaten.html#grundlegende-begrifflichkeiten",
    "title": "Daten & Metadaten",
    "section": "Grundlegende Begrifflichkeiten",
    "text": "Grundlegende Begrifflichkeiten\n\nDaten\nIm Wesentlichen bestehen Daten im Sinne dieses Buchs aus einer Folge von Bits. Abgesehen von ihrer Anzahl in Bytes lässt sich auf dieser Ebene allerdings nichts weiter über Daten sagen. Uns interessiert daher mehr, wofür die Daten stehen – beispielsweise für eine Jahreszahl, ein Bild oder für den Titel eines Dokuments. Dabei besteht ein Unterschied zwischen\n\nder Struktur von Daten (Syntax)\nund der Bedeutung von Daten (Semantik).\n\nZur Interpretation von Daten dienen Kodierungen in Form von Datenformaten und Identifikatoren. Wo genau jeweils die Grenze zwischen Syntax und Semantik liegt, hängt davon ab, auf welcher Ebene und mit welcher Kodierung Daten betrachtet und verarbeitet werden (siehe Tabelle 6.1):\n\n\n\n\n\n\nStruktur\nBedeutung\nKodierung\n\n\n\n\n2024-02-24\nDer 24. Februar 2024\nISO 8601\n\n\n1100001\nDie Zahl 97 (64+32+1)\nByte als Zahl\n\n\n97\nDer Buchstabe „a“\nASCII oder Unicode\n\n\na\nUnterfeld für Haupttitel\nFeld 021A im PICA+ Format\n\n\na\nUnterfeld für Umfangsangabe\nFeld 300 im MARC21 Format\n\n\n\n\n\nTabelle 6.1: Beispiele für Syntax und Semantik von Daten auf verschiedenen Ebenen\n\n\n\nEin Großteil der Datenverarbeitung besteht darin, Daten, zum Beispiel im Rahmen von ETL-Prozessen, von einer Kodierung in eine andere Kodierung zu überführen, um sie anschließend leichter interpretieren zu können. Bei der Konvertierung von Daten von einem in ein anderes Datenformat reduziert sich das implizite Datenmodell der Konvertierung auf den kleinsten gemeinsamen Nenner beider Formate.\nZur Beschreibung von Daten dienen\n\nformale Schemata auf Ebene der Syntax\nund Datenmodelle auf Ebene der Semantik.\n\nLeider liegen beide oft nicht explizit vor, sondern müssen anhand von Beispielen, Anwendungen und Dokumentation mühsam ermittelt werden. Im Idealfall entsprechen Daten einem klar definierten Datenformat.\n\n\n\n\n\n\nInfo\n\n\n\nMit dem Resource Description Framework (RDF) kodierte Daten werden auch als „semantisch“ bezeichnet. Die Kodierung erfolgt dabei nicht mit Feldern oder Tabellen, sondern in Form von so genannten RDF-Tripeln aus Subjekt, Prädikat und Objekt. Durch Verwendung gemeinsamer Identifikatoren in mehreren Tripeln entstehen Wissensgraphen, die in speziellen Datenbanken (Triplestores) gespeichert und abgefragt werden können.\nDie Bedeutung von RDF-Daten ergibt sich allerdings, wie bei allen Kodierungen, erst aus der Dokumentation von Datenelementen und ihrer Interpretation in praktischen Anwendungen.\n\n\n\n\nDatenformate\nDatenformate definieren eine Struktur, die sich in einer oder in mehreren austauschbaren Syntax-Varianten ausdrücken lässt und deren Bedeutung durch ein Datenmodell festgelegt ist. Beispielsweise definiert der Unicode-Standard eine Menge von Schriftzeichen (Buchstaben, Sonderzeichen, Emojis …) und verschiedene Verfahren, um Zeichenketten in Bytes zu kodieren (UTF-8, UTF-16 …). Syntax-Varianten werden auch als Serialisierung bezeichnet. Die meisten Datenformate haben nur eine Serialisierung, sodass Format und Syntax meist synonym verwendet werden. Einzelne Syntax-Elemente entsprechen Bestandteilen im Datenmodell (siehe Tabelle 6.2), daher werden in der Beschreibung von Daten auch diese beiden Ebenen meist nicht sauber getrennt.\n\n\n\n\n\n\nXML-Syntax\nXML-Modell\n\n\n\n\n&lt;name /&gt; oder &lt;name&gt;...&lt;/name&gt;\nXML-Element\n\n\nname=\"Inhalt\"\nXML-Attribut\n\n\n&lt;!-- ... --&gt;\nKommentar\n\n\n\n\n\nTabelle 6.2: Einige Bestandteile des XML-Formats\n\n\n\nAls Faustregel kann gelten, dass bei statischer Betrachtung von Daten der Bezug auf ihre Syntax sinnvoll ist, während zur Verarbeitung von Daten eher auf die Bestandteile ihrer Modelle Bezug genommen werden sollte.\nDatenformate lassen sich grob in zwei Kategorien unterteilen:\n\nStrukturierungssprachen wie CSV, XML, JSON und RDF ermöglichen es, Daten in kleinere Einheiten zu unterteilen und miteinander in Beziehung zu setzen. Die Sprachen basieren auf allgemeinen Ordnungsprinzipien (Felder, Tabellen, Hierarchien, Netzwerke …) und ihre Modelle haben darüber hinaus keine eigene Semantik. Die einfachste Strukturierungssprache ist das Prinzip der Zeichenkette.\nAnwendungsformate legen die Struktur von Daten für konkrete Arten von Inhalten fest (Metadatenformate zur Beschreibung von Dokumenten, Bildformate für Bilder …). Ihre Modelle verweisen letztendlich auf reale Objekte und Eigenschaften. Viele Anwendungsformate sind ihrerseits mittels einer Strukturierungssprache kodiert, zum Beispiel basiert das DataCite-Format zur Beschreibung von Forschungsdaten auf dem XML-Modell.\n\nDarüber hinaus gibt es besondere Formate, deren Anwendung in der Verarbeitung von Daten liegt. Neben Programmiersprachen, die selbst nicht als Datenformate betrachtet werden, sind dies folgende Sprachen:\n\nSchemasprachen wie XML Schema, JSON Schema und nicht zuletzt reguläre Ausdrücke dienen der formalen Beschreibung der Syntax von Datenformaten. Dabei bezieht sich jede Schemasprache auf eine Strukturierungssprache (XML Schema für XML-Formate, Avram für feldbasierte Formate …).\nAbfragesprachen dienen dem Verweis auf einzelne Teile von Datensätzen. Sie beziehen sich ebenfalls immer auf eine Strukturierungssprache (zum Beispiel XPath für XML, JSON Path für JSON …) und sind für die Verarbeitung von Daten notwendig.\nModellierungssprachen helfen bei der Beschreibung von Datenmodellen. Die häufigsten Modellierungssprachen basieren auf dem Entity-Relationship-Modell. Da zwischen Syntax und Semantik irgendwann die reine Datenebene verlassen werden muss, sind die wichtigsten Mittel zur Datenmodellierung allerdings Diagramme und Beschreibungen in natürlicher Sprache.\n\n\n\n\n\n\n\nInfo\n\n\n\nDas Entity-Relationshop-Modell ist eine Modellierungssprache die meist für relationale Datenbanken verwendet wird, aber auch unabhängig davon verwendet werden kann. Dabei werden in einem ER-Diagramm Objekttypen (Entitäten) und ihre Beziehungsarten dargestellt (siehe Beispiel Abbildung 6.1). Eine graphische Syntax für erweitere ER-Diagramme ist die Unified Modeling Language (UML).\n\n\n\n\n\n\nAbbildung 6.1: ER-Diagramm zur Beschreibung eines Datenmodells von Sitzbänken\n\n\n\n\n\nDie Verwendung von Schema-, Abfrage- und Modellierungssprachen hilft, häufig auftretende Fehler bei der (Meta-)Datenverarbeitung zu vermeiden. Ein Beispiel hierfür ist das Resource Description Framework (RDF) mit dazugehörigen Schemasprachen (SHACL/ShEx), Abfragesprachen (SPARQL) und Modellierungssprachen (RDFS/OWL). In anderen Fällen wird aus Mangel an Werkzeugen und Kenntnissen auf spezielle Datensprachen verzichtet und stattdessen auf allgemeine Programmiersprachen zurückgegriffen.\nEine ausführlichere Beschreibung von Datenformaten mit bibliothekarischem Schwerpunkt bietet die Datenbank format.gbv.de. Grundlagen von Metadaten und Ontologien vermitteln Assfalg (2023) und Rölke und Weichselbraun (2023).\nIn der Praxis werden Daten in einem Datenformat zusätzlich durch anwendungsspezifische Auslegungen und Einschränkungen geprägt, darunter Format-Varianten, Metadatenprofile bzw. Anwendungsprofile, Erfassungsregeln und die jeweilige Erfassungspraxis.\n\n\n\n\n\n\nInfo\n\n\n\nReguläre Ausdrücke sind das gängigste Mittel zur Beschreibung der Syntax von Daten. Gleichzeitig können mit ihnen Zeichenketten nach Mustern durchsucht werden. Ein regulärer Ausdruck für die Syntax einer ISBN-13 mit optionalen Trennstrichen ist beispielsweise:\n(97[89])-?([0-9]{1,5})-?([0-9]+)-?([0-9]+)-?[0-9]\nÜblicherweise decken Schemasprachen nicht alle Aspekte eines Datenformats ab. So lässt sich die Korrektheit der abschließenden Prüfziffer ([0-9]) nicht mit einem regulären Ausdruck überprüfen.\n\n\n\n\nIdentifikatoren\nEin wesentlicher Teil von Daten besteht aus Identifikatoren (IDs). Identifikatoren sind Namen, Nummern oder Codes, die eine eindeutige Referenzierung gleicher Dinge in unterschiedlichen Kontexten ermöglichen. Daten aus unterschiedlichen Quellen können miteinander abgeglichen und kombiniert werden, wenn sie die gleichen Identifikatoren verwenden.\nNeben eher intern genutzten Datensatz-Identifikatoren (z.B. die PPN des Bibliothekssystems PICA oder die ZDB-ID der Zeitschriftendatenbank) sind vor allem international standardisierte Identifikatoren von Bedeutung. Beispiele für solche Identifier-Systeme mit Relevanz für bibliothekarische Daten sind die nachfolgenden:\n\nDie International Standard Book Number (ISBN) wird von Verlagen für Bücher vergeben. Seit 2007 ist die 13-stellige ISBN Teil des EAN-Barcode-Systems.\nDie International Standard Serial Number (ISSN) identifiziert Zeitschriften und Schriftenreihen.\nDer Digital Object Identifier (DOI) identifiziert digitale Publikationen in elektronischen Zeitschriften und Repositorien.\nDer International Standard Identifier for Libraries and Related Organisations (ISIL) referenziert Bibliotheken, Archive, Museen und verwandte Einrichtungen.\nDie Open Researcher and Contributor ID (ORCID) identifiziert Autor*innen von wissenschaftlichen Publikationen.\nDer Uniform Resource Locator (URL) dient als Adresse einer digitalen Ressource im Web und wird teilweise gleichzeitig als deren Identifikator eingesetzt.\nDas System der Uniform Resource Identifier (URI) ermöglicht die Vereinigung verschiedener Identifier-Systeme und bildet die Grundlage von RDF und Linked Open Data (LOD).\n\nGemeinsam ist den Identifikatoren, dass sie jeweils eine definierte Syntax haben (z.B. XXXX-XXXY im Falle der ISSN, wobei X für eine Ziffer und Y für eine Prüfziffer steht), deren Bestandteile hierarchisch von einer zentralen Instanz festgelegt werden. Nach dem Prinzip des Namensraums kann dabei die Vergabe von Teilen an untergeordnete Organisationen delegiert werden. Beispielsweise werden ISIL für Bibliotheken in Deutschland beginnend mit dem Präfix „DE-“ durch die ISIL-Agentur an der Staatsbibliothek zu Berlin verwaltet.\nIm Gegensatz dazu gibt es zur Identifizierung von digitalen Objekten auch dezentrale Identifikatoren in Form von Prüfsummen, die sich automatisch aus den vorhandenen Daten berechnen lassen (SHA-Summe, IPFS-Adresse, Prüfziffer …).\n\n\nNormdaten\nEinfache kontrollierte Vokabulare bestehen aus normierten Listen von eindeutigen Benennungen – beispielsweise könnte in einem Gemüse-Vokabular festgelegt sein, dass immer „Karotte“ statt „Möhre“ verwendet werden muss. Wird jeder Eintrag mit einem künstlichen Identifikator versehen, muss die Benennung selbst nicht eindeutig sein. Existiert eine Datenbank zum Nachschlagen dieser IDs, so wird diese auch als Normdatei bezeichnet. Ihre Datensätze dienen als Normdaten der eindeutigen Identifizierung von Personen, Organisationen, geographischen oder administrativen Einheiten, Themen oder anderen Entitäten an verschiedenen Stellen. Ein Normdatensatz besteht mindestens aus einem Identifikator und einer Vorzugsbenennung als primärer Name. Oft gibt es weitere identifizierende Merkmale wie alternative Benennungen, Lebensdaten von Personen, Ortsangaben u.Ä. sowie Verknüpfungen zwischen verschiedenen Entitäten.\n\n\n\n\n\n\nInfo\n\n\n\nUmfang und Komplexität von Normdateien reichen von einfachen Listen bis zu komplexen Ontologien und Wissensgraphen. Ein prominentes bibliothekarisches Beispiel einer Normdatei ist die Gemeinsame Normdatei (GND), in der neben Personen auch Körperschaften, Veranstaltungen, Geografika, Werke und Sachschlagwörter miteinander vernetzt sind.\n\n\nZur Anreicherung von Daten mit Normdaten mittels Entity Recognition (Erkennung von Entitäten in Daten) und Entity Linking (Abgleich von Entitäten mit IDs anderer Normdateien) sollte unterschieden werden zwischen:\n\nNormdateien mit Entitäten wie Personen (ORCID), Publikationen (DOI, ISBN), Sprachen (ISO 639) etc., die sich grundsätzlich eindeutig unterscheiden lassen sowie\nNormdateien, deren abstrakte Entitäten von Kontext und Modellierung abhängen (Klassifikationen, Thesauri …).\n\nZur Verwaltung von Normdaten gibt es einige Datenformate wie MARC 21 for Authority Data und ISAAR (CPF). Als gemeinsamer Nenner auch außerhalb des Bibliotheksbereichs gilt das RDF-basierte Simple Knowledge Organization System (SKOS), für das mit JSKOS auch eine JSON-Variante existiert. Vor allem kleinere oder speziellere Normdateien (z.B. Codelisten im Rahmen der Katalogisierung) liegen allerdings selten in maschinenlesbarer Form vor.\nDas Basic Register of Thesauri, Ontologies & Classifications (BARTOC) erfasst Informationen zu Normdateien aller Art, darunter auch Verfahren zum technischen Zugriff.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Daten & Metadaten</span>"
    ]
  },
  {
    "objectID": "metadaten.html#metadatenstandards",
    "href": "metadaten.html#metadatenstandards",
    "title": "Daten & Metadaten",
    "section": "Metadatenstandards",
    "text": "Metadatenstandards\nNeben allgemeinen Datenformaten sind für die Bibliotheks-IT vor allem Metadatenformate zur Beschreibung von Dokumenten relevant. Die meisten der im Folgenden beschriebenen Metadatenformate spielen außerhalb von Kultureinrichtungen keine wesentliche Rolle. Für digitale Objekte (METS/MODS, LIDO, CDWA, EN 15907, EAD …, siehe Kapitel Digitalisierung) und für Forschungsdaten (DataCite, siehe Kapitel Forschungsnahe Dienste) gibt es darüber hinaus spezielle Formate.\n\nArten von Metadaten\nFolgende Arten von Metadaten können nach ihrer Funktion unterschieden werden:\n\nDeskriptive (= beschreibende) Metadaten zur Identifizierung und inhaltlichen Beschreibung wie Titel, Verfasser*in, Schlagwörter etc.\nAdministrative (= Verwaltungs-)Metadaten wie Angaben zu Herkunft, Speicherung, Zugriffsrechten, Verwaltung etc.\nStrukturelle Metadaten über den Aufbau von Dokumenten wie die Einteilung in einzelne Kapitel, Abschnitte, eingebundene Medien etc. Die Grenze zwischen digitalen Inhalten und ihrer Struktur ist allerdings mitunter fließend.\nTechnische Metadaten zu Merkmalen wie Umfang, verwendeten Datenformaten etc.\n\nJe nach Anwendung gibt es spezielle Metadatenformate oder es werden verschiedene Arten von Beschreibungen in einem Format zusammengefasst.\n\n\nFeldbasierte bibliothekarische Metadatenformate\nMachine-Readable Cataloging (MARC) ist das älteste und noch immer wichtigste Format für den Austausch von Daten zwischen Bibliotheken. Die aktuell relevante Variante ist MARC 21, insbesondere das Format MARC 21 für bibliografische Daten. Neben der binären Kodierung kann MARC 21 auch in XML und JSON kodiert werden. Viele Eigenheiten und Probleme des Formats sind historisch bedingt, eine Alternative konnte sich bislang nicht durchsetzen.\n\n\n\nEin Datensatz im MARC 21 Format\n\n\n\n\n\nDer gleiche Datensatz in MARC-XML\n\n\nPICA ist das von MARC inspirierte Datenformat der Katalogisierungssysteme CBS und LBS (Voß 2022). Das wichtigste Anwendungsprofil ist das K10plus-Internformat.\nMAB und allegro sind ebenfalls an MARC angelehnte, feldbasierte Formate aus dem deutschsprachigen Raum, die allerdings nur noch sporadisch verwendet werden.\n\n\nXML-basierte Datenformate\nMETS und MODS sind zwei zusammen im Bereich Digitalisierung eingesetzte Formate für strukturelle und administrative (METS) sowie bibliografische Metadaten (MODS). Strukturdaten in METS ermöglichen granulare Gliederung und Verlinkung von Objekten, wobei mögliche Typen und Beziehungen in Regelsätzen definiert sind.\nEncoded Archival Description (EAD) ist der zentrale dokumentarische XML-Standard zur Beschreibung von archivischen Findmitteln.\nLIDO ist ein etabliertes Austauschformat für den Museumsbereich.\nDataCite ist ein bibliografisches Datenformat, insbesondere zur Beschreibung von Forschungsdaten (siehe Kapitel Forschungsnahe Dienste).\n\n\nDatenmodelle und Ontologien\n\n\n\n\n\n\nDefinition\n\n\n\nEine Ontologie ist ein Datenmodell, das verschiedene Klassen und Eigenschaften in RDF definiert und so die einheitliche Kodierung und Verknüpfung verschiedener Datenquellen von Linked Data bis hin zu umfangreichen Wissensgraphen ermöglicht.\nIm Gegensatz zu einfacheren Formen von Normdaten geht es bei Ontologien nicht nur um die eindeutige Identifizierung (Beispiel: ist mit „Bank“ das Gleiche wie „Sitzbank“ oder wie „Geldinstitut“ gemeint?) sondern auch um Eigenschaften und Beziehungen (Beispiel: mögliche Größen, Materialien und Orte von Bänken). Wissensgraphen enthalten neben Ontologien auch konkrete Daten über Instanzen der Ontologie-Klassen (Beispiel: Liste konkreter Sitzbänke an einem Ort).\n\n\nDublin Core bzw. das Dublin Core Metadata Element Set (DCMES) hat als kleinster gemeinsamer Nenner der meisten Metadatenstandards die größte Verbreitung. Es besteht aus 15 Basiselementen wie „creator“, „title“, „date“ und „description“ und Erweiterungen mit den DCMI Metadata Terms wie „Alternative Title“, „Date Created“ und „Date Available“.\nDie Functional Requirements for Bibliographic Records (FRBR) sind ein sehr abstraktes Metadatenmodell. Sie beinhalten insbesondere eine Einteilung von bibliografischen Entitäten in die Beschreibungsebenen „work“, „expression“, „manifestation“ und „item“.\nDie BIBFRAME-Ontologie wurde entwickelt, um MARC auf Grundlage von RDF zu ersetzen. Die wesentlichen Elemente sind „work“, „instance“ und „item“ sowie damit verbundene Eigenschaften und Entitätstypen.\nSchema.org ist eine allgemeine Ontologie für strukturierte Daten in Webseiten.\n\n\nVerlagsdaten und Literaturangaben\nDie Formate ONIX, JATS, BITS und CrossRef XML stammen aus dem Verlagsbereich zur Beschreibung von Zeitschriftenartikeln und Büchern. Sie basieren alle auf XML und sind für Bibliotheken für den Datenimport relevant. Datenformate für Literaturangaben (BibTeX, RIS, Endnote, CSL-JSON …) werden dagegen zum Export von Katalogdaten bereitgestellt. Zitationsregeln für Literaturangaben und Ansetzungsregeln von ISBD sind dagegen für den Datenaustausch eher unbrauchbar. Learning Object Metadata (LOM) dient in verschiedenen lokalen Anpassungen der Beschreibung von Lerneinheiten.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Daten & Metadaten</span>"
    ]
  },
  {
    "objectID": "metadaten.html#datenverarbeitungsprozess-in-bibliotheken",
    "href": "metadaten.html#datenverarbeitungsprozess-in-bibliotheken",
    "title": "Daten & Metadaten",
    "section": "Datenverarbeitungsprozess in Bibliotheken",
    "text": "Datenverarbeitungsprozess in Bibliotheken\n\nDatenerfassung\nTraditionell werden bibliothekarische Metadaten durch Katalogisierung erstellt. Die Verwaltung der Katalogdaten erfolgt entweder lokal oder gemeinsam in einer Verbunddatenbank. Der Vorteil der Verbundkatalogisierung liegt darin, dass jedes Dokument nur einmal zentral beschrieben werden muss, während bei lokaler Katalogisierung durch Fremddatenübernahme nur zum Teil auf vorhandene Kataloge zurückgegriffen werden kann.\nIm Idealfall sollte die Erfassung nach Autopsie, also auf Grundlage des vorliegenden Werkes, durch geschultes Personal und nach etablierten Regelwerken (Katalogisierungsrichtlinien) erfolgen. Um möglichst viele Publikationen zu erfassen, wird jedoch zunehmend auch auf anderweitig erfasste Metadaten von Verlagen, Repositorien und aus anderen Quellen zurückgegriffen. Dazu müssen Daten unterschiedlicher Erschließungstiefe und -qualität im Rahmen von ETL-Prozessen gesammelt, analysiert und mit vorhandenen Daten vereinheitlicht werden. In jedem Fall muss beachtet werden, dass sich Regeln und Umstände, nach denen Daten erfasst werden, mit der Zeit ändern können (beispielsweise der Umstieg der Erfassungsregeln von RAK auf RDA) und dass das Ergebnis auch davon abhängt, wie gut überprüft werden kann, was die Anforderungen an die Daten sind.\nDarüber hinaus gibt es Verfahren zur automatischen Erstellung von Metadaten aus vorhandenen Dokumenten, beispielsweise zur Erkennung und Auswertung von Literaturangaben und zur thematischen Einordnung von Dokumenten. Mit diesen Verfahren lassen sich zwar größere Mengen von Daten erfassen, es muss aber immer mit einer gewissen Fehlerrate gerechnet werden.\nWelche Art und welcher Umfang von Fehlern und Uneinheitlichkeiten bei der Datenerfassung tolerierbar sind, hängt letztlich davon ab, wozu die Daten erfasst werden. So gelten beispielsweise für eine historische Bibliografie andere Maßstäbe als für einen Suchindex.\nNicht zuletzt sollte bedacht werden, dass Geschwindigkeit und Qualität von Datenerfassung auch von der Usability der Werkzeuge abhängen, mit denen Daten erstellt, bearbeitet und analysiert werden können.\n\n\n\n\n\n\nInfo\n\n\n\nMehr zur bibliothekarischen Datenerfassung in den Grundlagen der Informationswissenschaft (2023), Teil B.\n\n\n\n\nETL-Prozess\nDa sich die IT in- und außerhalb von Bibliotheken über verschiedene Organisationen und Systeme erstreckt, müssen an vielen Stellen Daten von einer oder mehreren Quellen in ein anderes Informationssystem übertragen werden. Der grundsätzliche Prozess der Datenintegration, der Quell- und Zielsysteme verbindet, wird als ETL-Prozess bezeichnet. Der Prozess aus drei zentralen Schritten „Extract“, „Transform“ und „Load“ stammt ursprünglich aus dem Bereich des Data Warehousing und findet sich auch in anderen Anwendungsfällen. Im Folgenden wird er am Beispiel der Integration von Metadaten in ein Discovery-System beschrieben. Abbildung 6.2 illustriert den generellen ETL-Prozess.\n\n\n\n\n\n\nAbbildung 6.2: Allgemeiner Ablauf eines ETL-Prozesses\n\n\n\n\nExtraktion\nZiel der Extraktion (Extract) ist die Auswahl und der Abzug relevanter Daten aus verschiedenen Datenquellen. Hierbei handelt es sich primär um einen technischen Vorgang, das sogenannte Harvesting, welcher automatisiert oder manuell gestartet werden kann. Der Aufwand und die Qualität des Harvestings können je nach Datenquelle sehr unterschiedlich ausfallen. Denkbare Datenquellen sind Dateien, Datenbanken bzw. Datenbankabzüge, Schnittstellen als auch unstrukturierte Quellen wie z. B. Websites, die mittels Screenscraping erschlossen werden müssen.\nDer Extraktionsvorgang erfolgt bei Bedarf regelmäßig, um die Daten im Zielsystem aktuell zu halten. Mögliche Aktualisierungsintervalle sind:\n\nperiodisch, das heißt in zeitlich regelmäßigen Abständen unabhängig von der jeweiligen Aktualisierung der Daten in den Quellsystemen\nereignisgesteuert, immer wenn bestimmte Bedingungen wie zum Beispiel die Änderung von Daten in den Quellsystemen eintreten\nmanuell, beispielsweise wenn Daten aus Quellsystemen ad hoc importiert werden sollen. Manuelle Aktualisierungen bieten sich vor allem an, wenn der Inhalt der Datenquellen weitestgehend statisch ist, da die dauerhaft verlässliche Aktualisierung im Zielsystem ohne Automatismus nicht garantiert ist.\n\nDer Extraktionsvorgang ist technisch relativ einfach handhabbar, wenn strukturierte Datenformate und/oder Schnittstellen existieren – die wesentlichen Aufwände finden sich dann im nachfolgenden Transformationsschritt. Anders sieht es aus, wenn beispielsweise Daten manuell eingesammelt werden müssen oder Screenscraping notwendig ist. Beim Screenscraping müssen aufwändig Extraktionsskripte erstellt werden, um Daten aus Webseiten in ein strukturiertes Format zu überführen. Diese Skripte sind zudem sehr fehleranfällig und müssen jedes Mal angepasst werden, wenn die Betreiber*innen der Datenquelle Veränderungen vornehmen.\nDie extrahierten Daten werden in einem Arbeitsbereich abgelegt und dort im nächsten Prozessschritt aufbereitet.\n\n\nTransformation\nDaten aus verschiedenen Quellsystemen liegen zumeist in unterschiedlichen Formaten mit unterschiedlichen Datenmodellen vor. Neben Unterschieden in der Syntax können gleiche Sachverhalte auch auf semantischer Ebene unterschiedlich beschrieben sein, da die Daten mitunter für abweichende Anwendungsfälle erfasst wurden. So müssen beispielsweise in einem Discovery-System Metadaten zur einfachen Beschreibung so aufbereitet werden, dass sie auch erweiterte Suchstrategien unterstützen.\nZiel der Transformation ist es, alle Daten in ein einheitliches Format mit gemeinsamen Datenmodell zu überführen. Dieses Zielformat wird beim ETL-Prozess auch als Schema bezeichnet. Die Vereinheitlichung des Schemas (Mapping) ist ein wesentlicher Schritt jeder Datenkonvertierung. Zur Minimierung des Transformationsaufwands dienen gemeinsame Standards wie MARC21 als Austauschformat oder die einheitliche Verwendung von RDF-Ontologien.\nÜber die einfache Konvertierung hinaus sind im Rahmen der Transformation oft weitere Aufbereitungen zur Vereinheitlichung und Verbesserung der Datenqualität notwendig. Beispiele hierfür sind:\n\nPrüfung und Vereinheitlichung der Zeichencodierung auf normalisierten Unicode\nformale Anpassungen von Daten wie die Vereinheitlichung von Datumsformaten, Ländercodierungen etc.\nErkennung und Eliminierung von Duplikaten\nAbgleich, Vereinheitlichung und Konsistenzprüfung von Aussagen über dieselben Objekte aus verschiedenen Datenquellen\nAnreicherung oder Korrektur von Datensätzen mittels Zusatzinformationen aus Normdaten oder anderer zusätzlicher Datenquellen\n\nDer Aufwand der Transformation sollte nicht unterschätzt werden, da die Datenqualität verschiedener Datenquellen stark variieren kann und Qualitätsprobleme oft erst spät entdeckt werden. Zudem können sich Datenquellen und ihre Qualität zwischen Aktualisierungen ändern. Sofern die Datenübernahme nicht nur einmalig stattfinden soll (Konversion), ist die Betreuung des Transformationsschrittes eine Daueraufgabe.\nIm Transformationsschritt müssen regelmäßig Massendaten analysiert und modifiziert werden, daher ist der Einsatz von IT-gestützten Werkzeugen und Verfahren der Datenanalyse unerlässlich. Die aus der Analyse gewonnenen Erkenntnisse müssen wiederum kontinuierlich in die Anpassung des Schema-Mappings einfließen, damit der Transformationsprozess nicht ins Stocken gerät.\n\n\nLaden\nAuf die Transformation folgt beim Laden (Load) die Überführung der vereinheitlichten Daten in das Zielsystem – beispielsweise in den Suchindex eines Discovery-Systems. Dabei dürfen nur Datensätze in Produktivsysteme übernommen werden, die den Transformationsschritt erfolgreich durchlaufen haben, während für Test- und Entwicklungssysteme andere Regeln möglich sind.\nDas Laden selbst ist ein technisch beherrschbarer Schritt, welcher optimalerweise darauf abzielt, das Zielsystem ohne Ausfallzeiten aktuell zu halten. Bei Änderungen des Schemas muss deshalb besonders darauf geachtet werden, gleichzeitig entsprechende Anpassungen im Zielsystem vorzunehmen. Daneben ist es bei absehbaren Änderungen ratsam, das Schema und die Verarbeitung von Daten im Zielsystem von vornherein flexibel zu gestalten.\n\n\nUmsetzung des ETL-Prozesses\nDie Umsetzung von Datenkonvertierung und ETL-Prozessen erfolgt im bibliothekarischen Umfeld oft über selbst entwickelte Skripte für das Harvesting und die Transformation. Das entsprechende Know-How ist in der Regel auf wenige Köpfe verteilt und kann auch nicht leicht durch das Hinzuziehen externer Expertise verfügbar gemacht werden.\nEs gibt einige kommerzielle ETL-Komplettlösungen mit Data-Warehouse- oder Business-Intelligence-Hintergrund. Angesichts der teils erheblichen Einstiegskosten und zur Vermeidung von Vendor Lock-in sind für Bibliotheken möglichst einfache und allgemeine Werkzeuge zur Datenverarbeitung meist die bessere Wahl. Die Vorteile etablierter ETL-Werkzeuge liegen in Schulungsmöglichkeiten und der Verfügbarkeit externer Expertise. Mit Catmandu, Metafacture und OpenRefine gibt es mehrere Open-Source-ETL-Frameworks, deren eingeschränkter Funktionsumfang und verbesserungswürdige Usability durch Anpassungen für bibliothekarische Datenformate und Schnittstellen möglicherweise aufgewogen werden.\nGrundsätzlich lassen sich die kontinuierlich anfallenden Aufwände der Transformation und Qualitätssicherung durch ein ETL-Werkzeug nicht vermeiden, sondern nur besser handhabbar machen. Dazu sollten die einzelnen Arbeitsschritte zentral verwaltet und dokumentiert werden, beispielsweise durch ein Versionskontrollsystem. Durch den Fokus auf Usability kann mittels ETL-Werkzeugen bibliothekarisches Personal stärker eingebunden werden. Da hier insbesondere im Bereich der Programmierung die Einstiegshürden niedriger sind, ist es Fachpersonal leichter möglich, direkt Änderungen und Optimierungen am ETL-Prozess vorzunehmen. Bei einer reinen Verankerung von ETL im IT-Bereich sind solche Eingriffe dagegen nur in Zusammenarbeit von IT und Metadaten-Expert*innen umsetzbar.\nIn jedem Fall gehen mit der Einführung von ETL-Werkzeugen in die bibliothekarische Arbeit immer auch individuelle Anpassungen im Prozess von Extraktion, Transformation und Laden einher. Dieser Aufwand kann sowohl gegen die Einführung solcher Werkzeuge sprechen als auch dafür, vorhandene „Bastellösungen“ zu evaluieren und zu konsolidieren.\n\n\n\nWerkzeuge\nFür die Verarbeitung von Daten im Rahmen bibliothekarischer IT-Systeme werden grundsätzlich die gleichen Werkzeuge verwendet wie außerhalb des Bibliothekswesens, daher wird an dieser Stelle auf eine allgemeine Einführung in die Datenverarbeitung verzichtet. Ganz allgemein sind hier als Werkzeuge\n\nMittel zur Dateiverwaltung und ein Texteditor unabdingbar,\nallgemeine Kommandozeilenprogramme (curl, sort, grep …) sehr zu empfehlen\nund Programmiersprachen vor allem für komplexere Aufgaben hilfreich.\n\nMaterial und Kurse für die praktischen Grundkenntnisse mit Bibliotheksbezug finden sich insbesondere in den Bereichen Data Librarianship und Data Science.\nWerkzeuge für konkrete Datenformate orientieren sich an den zugrunde liegenden Strukturierungssprachen. So gibt es beispielsweise eigene Editoren oder Editor-Plugins für XML- und JSON-Daten und entsprechende Kommandozeilentools wie XMLStarlet für XML und jq für JSON. Für tabellarische Daten eignet sich etwa eine Tabellenkalkulation oder das tabellenbasierte Werkzeug OpenRefine (openrefine.org).\nFür bibliothekarische Datenformate und Schnittstellen gibt es darüber hinaus einige speziellere Werkzeuge:\n\nProgrammierbibliotheken wie MARC4J und YAZ erleichtern die Datenverarbeitung im Rahmen eigener Programme.\nAnwendungsprogramme wie WinIBW und BibControl erfordern zwar weniger IT-Kenntnisse, sind dafür aber nur eingeschränkt und/oder nur für sehr spezielle Aufgaben verfügbar.\nFreie Werkzeuge zum Metadatenmanagement wie die Frameworks Catmandu (librecat.org/Catmandu) und Metafacture (metafacture.org).\n\nFür einzelne Anwendungen und Formate gibt es einige weitere Werkzeuge und es kann sich lohnen, solche Werkzeuge selbst zu entwickeln und als Open Source zur Verfügung zu stellen. Für das PICA-Format sind derartige Programme in der Einführung in die Verarbeitung von PICA-Daten (Voß 2022) aufgeführt.\n\n\nSchnittstellen\nEine API (Application Programming Interface, auch Programmierschnittstelle) ist eine definierte Methode zur Abfrage und/oder Änderung von Daten in einem Informationssystem. Wie die Daten innerhalb des Systems verwaltet werden, ist dabei nebensächlich. Dieses Prinzip ermöglicht die Kombination unterschiedlicher Softwarekomponenten. Wenn möglich, sollten produktunabhängige, offen dokumentierte APIs verwendet werden. Im Bibliotheksbereich sind insbesondere folgende APIs relevant:\n\nZ39.50 wurde vor Erfindung des Web zur Suche in Bibliotheksdatenbanken entwickelt. Nachfolger ist das XML-basierte Search/Retrieve via URL (SRU) mit der zugehörigen Abfragesprache Contextual Query Language (CQL).\nDas Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) dient dem Abruf von Metadaten aus Repositorien. Die Daten können nach Datum und Teilmengen gefiltert und so in Suchmaschinen und Portalen wie BASE und der Deutschen Digitalen Bibliothek (DDB) zusammengeführt werden.\nDas NISO Circulation Interchange Protocol (NCIP), das Simple Library Network Protocol (SLNP) und das Standard Interchange Protocol (SIP2) sind interne APIs für Ausleihe und Fernleihe. Sie werden zwischen Bibliotheken, Fernleihservern und zur Anbindung von Verbuchungsautomaten eingesetzt (Michaelis 2014).\nDie Patrons Account Information API (PAIA) ist eine offene Schnittstelle zum Zugriff auf Ausleihkonten.\nDie Document Availability Information API (DAIA) ist eine offene Schnittstelle zur Abfrage der Verfügbarkeit von Medien.\nMit unAPI können einzelne Datensätze in verschiedenen Formaten abgerufen werden.\nDie Reconciliation Service API ermöglicht den Abgleich mit Normdaten zur eindeutigen Referenzierung (siehe Abschnitte zu Identifikatoren und zu Normdaten).\nDie APIs des International Image Interoperability Framework (IIIF) ermöglicht die Referenzierung und Nutzung digitalisierter Werke in externen Werkzeugen durch gezielte Verlinkung auf einzelne Bestandteile.\nÜber SPARQL-Schnittstellen können RDF-Daten aus Wissensgraphen wie zum Beispiel Wikidata abgerufen werden.\nSchnittstellen zur Authentifizierung und Autorisierung wie LDAP, Shibboleth und OAuth.\n\nDarüber hinaus bieten die meisten Anwendungen eigene, meist interne Schnittstellen, zum Beispiel die Solr-API der Suchplattform Apache Solr. Besonders im Bereich Forschungsnaher Dienste gibt es weitere, spezialisierte Schnittstellen.\n\n\nDatenanalyse\nIm Gegensatz zu physischen Objekten ist Daten ihre Beschaffenheit nicht direkt anzusehen. Lediglich der Umfang von Daten in Bytes und ggf. die Anzahl von Dateien und Datensätzen kann einen ersten Anhaltspunkt liefern. Weitere Einschätzungen, insbesondere darüber, ob Daten vollständig oder fehlerhaft sind, setzen eine konkrete Analyse der Daten voraus. Dies beinhaltet auch die Visualisierung von Daten zur Exploration, Kommunikation und Diskussion (Jetter 2023).\nDie Auswertung von Daten ist nicht nur für das Qualitätsmanagement relevant (Voß 2021), beispielsweise um im Rahmen des ETL-Prozesses Verteilungen und Ausreißer zu erkennen, sondern auch, um aus Daten weitere Erkenntnisse zu gewinnen. So können beispielsweise Ausleihzahlen nach Medien gruppiert für die Bestandsplanung eingesetzt werden. Voraussetzung dafür ist, dass Daten überhaupt vorliegen, ebenso wie Mittel und Kenntnisse zu ihrer Auswertung. Bei fehlenden oder zu umfangreichen Daten können Stichproben erhoben werden, wobei auf die Zufälligkeit der Stichprobe und das Konfidenzintervall des Ergebnisses geachtet werden muss.\nNeben rudimentären Statistik-Kenntnissen helfen bei der Datenanalyse Werkzeuge wie die im vorigen Abschnitt beschriebenen Mittel zur Datenverarbeitung. Für allgemeine Analysen eignet sich vor allem eine Tabellenkalkulation. Für komplexere Analysen gibt es spezielle Statistik-Programme und Programmiersprachen wie SAS und R. Für explorative Analysen und um bei geänderter Datenlage automatisch aktuelle Ergebnisse zu bekommen, bieten sich interaktive Umgebungen wie Jupyter Notebooks oder Observable an. Für spezielle, wiederkehrende Analysen und Aufbereitungen kann es auch sinnvoll sein, eigene Anwendungen zu entwickeln bzw. entwickeln zu lassen. Beispiele hierfür sind BibControl, das Metadata Quality Assessment Framework (MQA), die Deutsche Bibliotheksstatistik sowie Statistikfunktionen als Teil anderer Programme (zum Beispiel Statistik und Reporting als Teil des BMS).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Daten & Metadaten</span>"
    ]
  },
  {
    "objectID": "metadaten.html#künstliche-intelligenz",
    "href": "metadaten.html#künstliche-intelligenz",
    "title": "Daten & Metadaten",
    "section": "Künstliche Intelligenz",
    "text": "Künstliche Intelligenz\nUnter den Begriffen Künstlichen Intelligenz, Machine Learning und Deep Learning werden verschiedene Verfahren der Datenverarbeitung zusammengefasst, die Aspekte menschlicher Intelligenz imitieren können. Auf Grundlage statistischer Verfahren lassen sich so Aufgaben automatisieren, für die bisher Personal notwendig gewesen wäre. Der Umfang der verarbeitbaren Daten wird lediglich durch die zur Verfügung stehende Rechenleistung begrenzt. Die Qualität der Ergebnisse hängt stark von der jeweiligen Aufgabenstellung und den eingesetzten Verfahren ab. Im besten Fall lassen sich die Ergebnisse praktisch nicht mehr von menschlich erzeugten Daten unterscheiden. Beim Einsatz von KI-Technologien sind allerdings auch ethische Aspekte zu beachten.\nFür die bibliothekarische Datenverarbeitung lassen sich grob zwei Arten von KI-Anwendungen unterscheiden:\n\nVerfahren zur Analyse und Anreicherung von Daten, beispielsweise die automatische Erschließung und Musterkennung im Rahmen der Digitalisierung.\nSysteme, die Antworten, Texte und Medien erzeugen, umschreiben und zusammenfassen, von einfachen Chatbots bis zu umfangreichen Sprachmodellen wie ChatGPT.\n\nDie zunehmende Verfügbarkeit von leistungsfähigen KI-Systemen wird in absehbarer Zeit zu Änderungen in der Rezeption und Produktion von Medien führen und damit auch Auswirkungen auf die Arbeit von Bibliotheken haben.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Daten & Metadaten</span>"
    ]
  },
  {
    "objectID": "metadaten.html#zusammenfassung-ausblick",
    "href": "metadaten.html#zusammenfassung-ausblick",
    "title": "Daten & Metadaten",
    "section": "Zusammenfassung & Ausblick",
    "text": "Zusammenfassung & Ausblick\nStrukturierte Metadaten sind unverzichtbar für die Verwaltung und den Zugriff auf Ressourcen in Bibliotheken. Daher bilden sie und ihre Verarbeitung die Grundlage für praktisch alle von Bibliotheken angebotenen IT-Dienste. Während Aufwand und Bedeutung von Datenverarbeitung auch in Zukunft hoch bleiben werden, ist davon auszugehen, dass der Einsatz semantischer Technologien (RDF) zur Zusammenführung heterogener Daten und Verfahren der künstlichen Intelligenz zunehmen werden.\n\n\n\n\nAssfalg, Rolf. 2023. „Metadaten“. In Grundlagen der Informationswissenschaft, herausgegeben von Rainer Kuhlen, Dirk Lewandowski, Wolfgang Semar, und Christa Womser-Hacker, 7. Ausgabe, 245–56. Berlin, Boston: De Gruyter Saur. https://doi.org/10.1515/9783110769043-021.\n\n\nJetter, Hans-Christian. 2023. „Informationsvisualisierung und Visual Analytics“. In Grundlagen der Informationswissenschaft, herausgegeben von Rainer Kuhlen, Dirk Lewandowski, Wolfgang Semar, und Christa Womser-Hacker, 7. Ausgabe, 295–306. Berlin, Boston: De Gruyter Saur. https://www.degruyter.com/document/doi/10.1515/9783110769043-025/pdf.\n\n\nKuhlen, Rainer, Dirk Lewandowski, Wolfgang Semar, und Christa Womser-Hacker, Hrsg. 2023. Grundlagen der Informationswissenschaft. 7. Ausgabe. Berlin, Boston: De Gruyter Saur. https://doi.org/doi:10.1515/9783110769043.\n\n\nMichaelis, Barbara. 2014. In RFID für Bibliothekare: ein Vademecum, herausgegeben von Frank Seeliger, 3. Auflage, 145–50. Verlag News & Media. https://doi.org/10.15771/RFID_2014_13.\n\n\nRölke, Heiko, und Albert Weichselbraun. 2023. „Ontologien und Linked Open Data“. In Grundlagen der Informationswissenschaft, herausgegeben von Rainer Kuhlen, Dirk Lewandowski, Wolfgang Semar, und Christa Womser-Hacker, 7. Ausgabe, 257–70. Berlin, Boston: De Gruyter Saur. https://www.degruyter.com/document/doi/10.1515/9783110769043-022/pdf.\n\n\nVoß, Jakob. 2021. „Datenqualität als Grundlage qualitativer Inhaltserschließung“. In Qualität in der Inhaltserschließung, 167–76. De Gruyter Saur. https://doi.org/10.1515/9783110691597-010.\n\n\n———. 2022. „Einführung in die Verarbeitung von PICA-Daten“. 2022. https://pro4bib.github.io/pica/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Daten & Metadaten</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html",
    "href": "bibliotheksmanagementsysteme.html",
    "title": "Bibliotheksmanagementsysteme",
    "section": "",
    "text": "Einleitung\nDas BMS spielt eine zentrale Rolle für die meisten klassischen Geschäftsprozesse in Bibliotheken.\nDurch die Ausweitung der Aufgaben in den Bereichen Publikationsdienste, Open Science oder auch Lernort sind in neuerer Zeit jedoch noch weitere Aufgaben hinzugekommen, die durch die klassischen BMS nicht abgebildet werden. Darüber hinaus haben die frühen Systeme nur sehr unzureichende Möglichkeiten, die nötigen Informationen zu elektronischen Ressourcen und ihrer Zugänglichkeit abzubilden. Auch zur Unterstützung von neueren Aufgaben wie der Publikationsunterstützung oder der Verwaltung räumlicher Ressourcen werden separate Systeme genutzt. Daraus ergibt sich der Bedarf, das BMS an diese separaten Systeme anzubinden, was die Bedeutung von Schnittstellen und offenen Architekturen erhöht hat.\nIn diesem Text wird der Begriff Bibliotheksmanagementsystem verwendet. Teilweise wird im Deutschen auch der allgemeinere Begriff (lokales) Bibliothekssystem verwendet. In der angloamerikanischen Literatur finden sich die Begriffe Integrated Library System (ILS) und Library Management System (LMS), zuletzt aber auch Library Services Platform.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#einleitung",
    "href": "bibliotheksmanagementsysteme.html#einleitung",
    "title": "Bibliotheksmanagementsysteme",
    "section": "",
    "text": "Definition\n\n\n\nEin Bibliotheksmanagementsystem (BMS) ist ein Softwareprodukt, mit dem die Arbeitsprozesse rund um die Erwerbung, Bestandsmanagement, Ausleihe, den Zugriff und die Auffindbarmachung von Bibliotheksbeständen über Kataloge abgebildet und automatisiert werden können.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#geschichte-der-bibliotheksmanagementsysteme",
    "href": "bibliotheksmanagementsysteme.html#geschichte-der-bibliotheksmanagementsysteme",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Geschichte der Bibliotheksmanagementsysteme",
    "text": "Geschichte der Bibliotheksmanagementsysteme\nIhren Ursprung haben Bibliotheksmanagementsysteme in den 1960er Jahren, als Bibliotheken damit begannen, Katalogdaten untereinander auszutauschen und auf diese Weise Prozesse zu optimieren. In dieser Zeit entwickelten sich auch die heute noch gebräuchlichen Austauschformate für Katalogdaten, z.B. MARC.\nIn den 1970er Jahren erlaubte die fortschreitende technische Entwicklung die Automatisierung weiterer Prozesse über den Datenaustausch hinaus. Zunächst war dies vor allem die Ausleihe mit der Verbuchung von Medien und der Erzeugung von Mahnschreiben. Auch die Verwaltung von Bestellungen im Rahmen der Erwerbung wurde möglich, so dass man in der Folge von Integrated Library Systems zu sprechen begann. Davon, dass Katalog-, Erwerbungs- und Nutzer*innen-Daten an einem Ort gehalten und bearbeitet wurden, versprach man sich eine größere Effizienz der Arbeitsprozesse. Diese erste Generation von BMS beinhaltete teilweise auch schon digitale Funktionen für Bibliotheksnutzer*innen wie über Telnet erreichbare Kataloge, die von Anfang an als integraler Bestandteil der BMS gesehen wurden (Borgman 1997).\nDie Entstehung des World Wide Web in den 1990er Jahren hatte zunächst vor allem Einfluss auf die Benutzbarkeit der Kataloge, die Web-Oberflächen erhielten. Aber auch die anderen Komponenten der BMS wurden überarbeitet, und zwar zunehmend auch von kommerziellen Anbietern, während die ersten Systeme als Eigenentwicklungen von Bibliotheken entstanden. Die Landschaft an Systemen der 2. Generation war von den späten 1990er bis in die Nullerjahre sehr divers, ist zuletzt aber von vielen Übernahmen geprägt worden, sodass man von einem konsolidierten Markt sprechen kann (Breeding o. J.).\n\n\n\n\n\n\nInfo\n\n\n\nDie erste Generation der Bibliotheksmanagementsysteme umfasste Grundfunktionen für die Ausleihe wie Verbuchung und Mahnung, für die Erwerbung die Verwaltung von Bestellungen und teilweise auch über Telnet erreichbare Kataloge für die Bibliotheksnutzer*innen. Die zweite BMS-Generation verfügte über erweiterte Funktionalitäten zur Unterstützung der Kernprozesse und zeichnete sich durch die Bereitstellung der (Nutzer)-kataloge über eine Weboberfläche aus. Die dritte Generation zeichnete sich durch stärkere Modularisierung und mehr Schnittstellen zur Anbindung weiterer Systeme aus.\n\n\nSeit den 2010er-Jahren entstand eine neue Generation von BMS, die Next-Generation Library Management Systems (LMS), die auch Library Services Platforms (LSP) genannt werden. Diese zeichnen sich durch verschiedene technische und funktionale Neuerungen aus. Die Datenhaltung erfolgt in der Regel cloudbasiert (auch wenn dies bei bei älteren Systemen durch Hosting auch schon möglich war), außerdem werden in der Regel mehr Schnittstellen zur Integration des Systems mit anderen Lösungen angeboten. Funktional wurden die Systeme vor allem um die Möglichkeit der Verwaltung von elektronischen Ressourcen erweitert sowie Statistik- und Reporting-Funktionalitäten verbessert.\nSeit dem Ende der 1990er Jahre spielen auch wieder Lösungen eine Rolle, die nicht kommerziell sind. Diese Open Source-Lösungen haben in der Regel eine große Anwender-Community und lassen einen vielfältigen Markt für Support- und Wartungsdienstleistungen zu.\n\n\n\n\n\n\nAbbildung 7.1: Evolution der Bibliotheksmanagementsysteme (nach Matthews und Block (2020), S. 7)\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nNach Matthews und Block (2020) lässt sich die Geschichte der BMS in sechs überlappende Epochen einteilen (siehe Abbildung 7.1):\n\nSystem-Epoche: Erste Schritte in den 1950er bis in die 1970er-Jahre hin zur Entwicklung von Software, z. T sehr experimentell, die die klassischen Geschäftsgänge von Bibliotheken in einem digitalen System abbilden sollen – dadurch prägt sich der Begriff „Bibliothekssystem“. Das Augenmerk bei der Entwicklung liegt besonders auf Nachbildungen des Leihverkehrs unter besonderer Beachtung der Identifikation überfälliger Medien.\nEpoche der Funktionalität: Kommerzielle Bibliothekssoftware-Anbieter beginnen sich zu formieren, die erstmals eine integrierte Lösung der verschiedenen Automationsbereiche (Erwerbung, Katalogisierung, Zeitschriftenakzession, Verbuchung, Leihverkehr usw.) anbieten. Hierdurch entsteht die Bezeichnung „Integriertes Bibliothekssystem“ (IBS), der auf den aus dem US-amerikanischen Raum übernommenen Begriff „Integrated Library System“ (ILS) zurückgeht. In den 1980ern entstehen die ersten Online-Kataloge (OPAC), die die in Bibliotheken traditionellen Zettelkataloge nachbilden.\nNutzer*innen-Fokus-Epoche: Durch die Erkenntnis, dass sich die Gewohnheiten von Bibliotheksnutzer*innen im Zugang zu und Umgang mit Medien u.a. mit dem Aufkommen des WWW in ihrem Alltag zunehmend ändern (z.B. durch die Nutzung von Online-Shopping und Suchmaschinen), rücken die Bedürfnisse der Nutzer*innen immer mehr in den Fokus bei der Entwicklung von Bibliothekssystemen.\nEpoche der Verbreiterung der Informationsressourcen: Der Übergang in eine Phase, bei der Medien nicht mehr erworben, sondern digital lizenziert werden. Entsprechend entwickelt sich das Bedürfnis nach einem Electronic Resource Management (ERM) und neuartige BMS unterfüttern zum Ende der Epoche diesen Wandel mit einer von Medientypen unabhängigen Ressourcenverwaltung.\nDiscovery-System-Epoche: Systeme, die über den lokalen Medienbestand hinaus auch extern lizenzierte Inhalte über eine alleinige Suchplattform zugänglich machen, erfreuen sich zunehmender Beliebtheit bei den BMS-Betreiber*innen. Sie sollen den Nutzer*innen einen deutlichen Mehrwert bieten. Seit den 2010er Jahren sind es überwiegend kommerzielle Verlage, die umfangreiche e-Medien-Pakete oder Indizes von Volltext- und Bibliografie-Datenbank als lizenzierbare Resource Discovery Services anbieten.\nWissensinnovation: Bibliotheken realisieren überwiegend, dass Discovery-Systeme nicht ihre gewünschte Wirkung entfalten und sie sich deutlicher von Plattformen großer Tech-Unternehmen abgrenzen müssen. Wissen soll neu erschlossen werden mit innovativen Technologien wie 3D-Druck, Virtual Reality (VR), Open-Access-Repositorien etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganisation\nMarktstatus\nOpen Source\nIndividuelle Entwicklung\n\n\n\n\naDIS/BMS\naStec\nÖBs und WBs vor allem im BSZ\nnein\ndurch aStec\n\n\nAlma\nExLibris\nWBs in Berlin, NRW, Bayerische Staatsbibliothek, Schweiz\nnein\ndurch ExLibris, integrierte Apps in Eigenregie\n\n\nFOLIO\nOpen Library Foundation\nEinführung in WBs im GBV und hebis\nja\nFOLIO Community und durch Dienstleister\n\n\nKoha\nKoha Community\nÖBs und Spezialbibliotheken, in Planung im KOBV\nja\nin Eigenregie oder durch Dienstleister\n\n\nLBS\nOCLC\nWBs im GBV, Hebis, DNB, Spezialbibliotheken und in den Niederlanden\nnein\ndurch VZG\n\n\nLIBERO\nLIBERO/Knosys\nÖBs und WBs\nnein\ndurch LIBERO\n\n\n\n\n\nTabelle 7.1: relevante Software-Produkte (Stand Anfang 2024)\n\n\n\n\nIn Tabelle 7.1 sind die aktuellen BMS mit der derzeit größten Marktreife und -durchdringung im deutschsprachigen Raum (Stand Anfang 2024, Sortierung nach Namen) angegeben. Weitere BMS wie ExLibris Aleph, SISIS Sunrise und allegro werden zwar auch noch an vielen Bibliotheken eingesetzt, aber nicht mehr wesentlich weiterentwickelt. Das Cloud-basierte System WMS von OCLC ist in Deutschland bislang nur vereinzelt im Einsatz. Für BibliothecaPlus ist von OCLC ein Nachfolger angekündigt.\nDarüber hinaus gibt es mehrere kommerzielle Systeme, deren Funktionsumfang auf bestimmte Arten von Bibliotheken zugeschnitten ist, beispielsweise:\n\nPerpustakaan ist in Schulbibliotheken verbreitet und wendet sich auch an nicht-bibliothekarisch vorgebildetes Personal,\nNOS ist in internen Forschungs- und Behörden-Bibliotheken verbreitet,\nQuria von Axiell ist in skandinavischen ÖBs verbreitet und löst im deutschsprachigen Raum das BMS BIBDIA ab.\n\nEine umfangreiche internationale Übersicht von BMS enthält der von Marshall Breeding gepflegte Library Technology Guide. Für den deutschsprachigen Raum gibt es Übersichten von Verbundzentralen oder Büchereifachstellen, z.B. Kluge (2022) für öffentliche Bibliotheken. Darüber hinaus sind Daten zu BMS systematisch in Wikidata erfasst und können beispielsweise unter https://w.wiki/574K abgefragt werden.\n\n\n\n\n\n\nInfo\n\n\n\nDer IT-Lebenszyklus von BMS ist im Vergleich zu anderen IT-Systemen eher lang. So blicken z. B. Systeme wie Aleph, aDIS, LBS, SISIS Sunrise und Koha auf eine 20-30 jährige Entwicklungsgeschichte zurück und befinden sich aktuell immer noch im Einsatz. Auch\nSo wurde beispielsweise FOLIO im Rahmen des Open Library Environment Project bereits 2009 initiiert und wird wahrscheinlich erst im nächsten Jahrzehnt in die Wartungsphase übergehen.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#funktionalitäten-von-bibliotheksmanagementsystemen",
    "href": "bibliotheksmanagementsysteme.html#funktionalitäten-von-bibliotheksmanagementsystemen",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Funktionalitäten von Bibliotheksmanagementsystemen",
    "text": "Funktionalitäten von Bibliotheksmanagementsystemen\nBMS sind in der Regel modular aufgebaut und verfügen mindestens über Module für folgende Funktionen:\n\nErwerbung\nKatalogisierung / Erschließung\nAusleihe\nein Recherche-Modul, das sich vorwiegend an die Bibliotheksnutzer*innen richtet\n\n\nGrundlegende Komponenten\nDie Systeme der 1. und 2. Generation können als sehr ausgereift bezeichnet werden und lassen vielfältige Möglichkeiten zu, bibliothekarische Geschäftsgänge in einem hohen Detaillierungsgrad abzubilden. Nachfolgend werden diese entsprechenden Aufgabenbereiche skizziert.\nErwerbung meint die Beschaffung benötigter Bestände im Buchhandel und bei Verlagen. Darunter fallen z.B. folgende Aufgabengebiete:\n\nBestellungen\nBudgetverwaltung\nRechnungsverwaltung und Mahnwesen\nLieferantenverwaltung\nZeitschriftenmanagement und Fortsetzungsverwaltung\nUnterstützung EDIFACT-Standards\nBuchbinder\n\n\\(\\Rightarrow\\) Siehe auch Prozessabbildung: Erwerbung\nKatalogisierung meint die Erschließung der verwalteten Medien und digitalen Quellen, z.B. anhand\n\nÜbernahme von Fremddaten\nAnbindung an Verbünde\nIntegration digitalisierter Medien\nBestandsnachweis / Exemplarmanagement\n\n\\(\\Rightarrow\\) Siehe auch Prozessabbildung: Katalogisierung\nAusleihe meint vorwiegend die Verwaltung physischer Medien bzw. Objekte und regelt die Interaktionen mit Nutzer*innen wie z.B.:\n\nAbbildung komplexer Reglements nach Benutzer- und Medientypen, Standorten usw (siehe auch Benutzungsbedingungen)\nAusleihfristen\nVerwaltung von Standorten\nVersand von Benachrichtigungen\nAnbindung an Selbstverbuchungslösungen\nMahngebühren\n\n\\(\\Rightarrow\\) Siehe auch Prozessabbildung: Ausleihe\nDas Recherchemodul stellt die Sicht für die Nutzer*innen auf Bestände der Einrichtung zur Recherche und Kontofunktionen dar:\n\nKatalog (auch OPAC genannt)\nBenutzerkonto\n\n\\(\\Rightarrow\\) Siehe auch Prozessabbildung: Katalog\nDie Next Generation-Systeme zeichnen sich gegenüber den Systemen der 1. und 2. Generation in der Regel durch andere Systemarchitekturen aus. Das heißt, sie verfügen über aktuellere technische Einzelkomponenten und Schnittstellen, auf deren Grundlage auch zahlreiche zusätzliche Funktionalitäten angeboten werden können. Im Einzelnen gibt es folgende Merkmale, die ein Next Generation-System kennzeichnen (Schweitzer 2016):\n\nPlatformbasiertes Angebot als Software as a Service (SaaS)\nMandantenfähigkeit\nInteroperabilität durch offene, standardisierte und dokumentierte Schnittstellen\nVerfügbarkeit von Datenbanken bzw. Knowledge Bases für bibliografische Daten und Lizenzinformationen\nVerwaltung elektronischer Ressourcen\nkein fest integrierter Katalog, sondern Schnittstellen zu Discovery-Systemen\nzusätzliche Statistik-Werkzeuge\n\n\n\nVergleich mit anderen Managementsystemen\nAufgrund der hohen Kosten für die Einführung oder die Migration eines BMS dürfte sich für viele Entscheider*innen die Frage stellen, ob sich die Investition lohnt bzw. ob sich die Aufgaben auch mit anderen Lösungen erledigen lassen (siehe hierzu Beschaffung und Marktanalyse).\nSysteme zur Automatisierung von Geschäftsprozessen gibt es in verschiedenen Branchen. Eine genauere Betrachtung der Aufgaben, die durch Automatisierung unterstützt werden sollen, kann aufzeigen, ob dafür ein Bibliotheksmanagementsystem oder eine andere Lösung besser geeignet ist.\nDie folgenden Alternativen sind möglicherweise für kleine Einrichtungen relevant, die über sehr überschaubare Bestände verfügen und kaum oder wenig ausleihen:\nErfassung von Medien:\n\nListen in einer Tabellenkalkulation (Excel, LibreOffice …)\n\nErfassung und Web-Präsentation von Medien:\n\nLibrary Thing for Libraries\nZotero Groups\nStand-Alone-Lösungen für Electronic Resource Management wie Coral\n\nAusleihe\n\nPlugins für Wordpress wie WebLibrarian\n\nErwerbung\n\nFinanzbuchhaltungssysteme wie SAP, HIS-Hochschul-ERP\n\nNutzerdatenverwaltung\n\nIDM-Systeme\n\nBibliotheken mit einem jährlichen Zuwachs von über 500 Medien und verschiedenen Benutzertypen und Ausleihbedingungen ist die Nutzung eines BMS zu empfehlen, da hier eine gewisse Prozesseffizienz einerseits und eine Erschließungs- und Dienstleistungsqualität andererseits erreicht werden kann.\n\n\n\n\n\n\nInfo\n\n\n\nAls gedankliches Experiment ist die Überlegung, auf ein BMS zu verzichten, jedoch gut geeignet, um sich über die Anforderungen der eigenen Institution klar zu werden. Insbesondere die Rolle des Bibliothekskataloges als Schnittstelle zu den Bibliotheksnutzer*innen kann und sollte kritisch hinterfragt werden. Beispielsweise gab es Überlegungen der Universitätsbibliothek in Utrecht, auf dieses klassische Instrument gänzlich zu verzichten.\n\n\n\n\nIntegration des BMS mit anderen IT-Systemen\nInnerhalb der Bibliothek werden BMS meist zusammen mit anderen Softwaresystemen eingesetzt. Insbesondere sind dies:\n\nSelbstbedienungsautomaten (Ausleihe, Rücknahme, Sortierung von Medien, Bezahlung von Gebühren)\nDokumentenserver, Content Management Systeme und andere Repositorien\nWorkflowsysteme (Digitalisierung von Altbestand; Publikationsunterstützung …)\n\nWeitere Systeme müssen für eine effektive Arbeit sinnvoll mit dem BMS verbunden werden:\n\nHaushaltssysteme wie SAP, HIS Haushalt-ERP\nIdentitätsmanagementsysteme (Account-Verwaltung)\nLieferantensysteme (bibliografische Daten, Bestell- und Rechnungsdaten)\n\nIm bibliothekarischen Umfeld sind folgende Systeme relevant:\n\nder Verbundkatalog\ndie Zeitschriftendatenbank\ndie elektronische Zeitschriftendatenbank\n\nFür die regionale und überregionale Literaturversorgung (physische, Print-Medien, E-Medien) spielt die Anbindung an folgende Systeme eine wesentliche Rolle\n\nFernleihe\nDokumentenlieferdienste (wie Subito und Fachinformations-Lieferdienste)\n\nDie Anbindung an die entsprechenden Dienste (Zentraler Fernleih-Server, Fernleihdienst, Subito-Server etc.) ist für viele, aber durchaus nicht alle Bibliotheken relevant.\nIm Zusammenhang mit dem Aufbau der Fachinformationsdienste für die spezialisierte Informationsversorgung in Deutschland werden in zunehmendem Maße Fachportale entwickelt. Relevante Katalog-Informationen werden aus möglichst vielen Bibliotheken regelmäßig abgerufen (Harvesting), in ein einheitliches Datenformat übertragen und anschließend als gemeinsamer Index für die übergreifende Recherche in Discovery-Systemen angeboten. Die BMS müssen entsprechend über Standardschnittstellen die relevanten Katalogdaten in einem vereinbarten Datenformat bereitstellen.\n\n\nVerbundkataloge\nIn Deutschland haben sich Katalogverbünde in den 1970er und 1980er Jahren entwickelt. Zunächst haben sich die wissenschaftlichen Bibliotheken meistens auf Bundesland-Ebene für die Rationalisierung der Katalogisierung zu Verbünden zusammengeschlossen. Inzwischen sind in diesen Verbünden auch öffentliche Bibliotheken vertreten. Darüber hinaus gibt es mit WorldCat einen internationalen Verbundkatalog. Die Anbindung an WorldCat geschieht in Deutschland in der Regel über die Bibliotheksverbünde, international auch direkt über die lokalen BMS.\nTabelle 7.2 gibt eine Übersicht über die deutschsprachigen Bibliotheksverbünde.\n\n\n\n\n\n\n\n\n\n\n\nVerbund\nVerbundkatalog\nSystem\n\n\n\n\nBVB\nB3Kat\nALEPH (Ex Libris)\n\n\nBSZ\nK10plus\nCBS (OCLC)\n\n\nGBV\nK10plus\nCBS (OCLC)\n\n\nhebis\nhebis\nCBS (OCLC)\n\n\nhbz\nhbz\nAleph (Ex Libris)\nAlma-Netzwerkzone (Ex Libris)\n\n\nKOBV\nB3Kat\nAleph (Ex Libris)\n\n\nVÖBB (öffentliche Bibliotheken)\nVÖBB\naDIS/BMS (aStec)\n\n\nÖsterreichischer Bibliothekenverbund\nOBV\nAlma-Netzwerkzone (Ex Libris)\n\n\nSwiss Library Service Platform (SLSP)\nswisscovery\nAlma\n\n\n\n\n\nTabelle 7.2: Bibliotheksverbünde und -kataloge\n\n\n\n\n\nAnbindung an Verbundkataloge/Verbundkatalogisierung\nDie Übernahme von bibliografischen Daten und – bei elektronischen Medien – Paket- bzw. Lizenzinformationen aus anderen Systemen ist für eine Bibliothek unabhängig davon, ob sie in einem Verbund organisiert ist, von Interesse. Eine Anbindung von bibliografischen Datenquellen, z.B. per Z39.50, für die Übernahme der entsprechenden Daten gilt daher als Mindeststandard. In Verbünden organisierte Bibliotheken katalogisieren in der Regel bereits in Verbunddatenbanken und übernehmen die Katalogisate dann vielfach verzögerungsfrei in die BMS.\nFür Informationen zu elektronischen Medien gibt es neben den Verbunddatenbanken weitere Datenbanken bzw. Knowledge Bases, aus denen Paket- und Lizenzinformationen hervorgehen. Darunter fallen zum Beispiel\n\ndie Zeitschriftendatenbank (ZDB) und die Elektronischen Zeitschriftenbibliothek (EZB) als zentrale Nachweissysteme für Zeitschriften und Fortsetzungen in deutschen und österreichischen Bibliotheken\ndie GOKb als kooperativ gepflegte Knowledge Base für elektronische Ressourcen\nkommerzielle Knowledge Bases wie die EBSCO KB, Alma NZ und den OCLC Collection Manager\n\n\n\nStatistik und Reporting\nMitunter verfügen BMS über eigene Module für die Erstellung von Statistiken. Folgende Statistiken sind typischerweise erforderlich:\n\nArbeitsstatistiken für die tägliche Arbeitsorganisation und die Erfüllung der jeweiligen Berichtspflicht innerhalb der Einrichtung und gegenüber vorgesetzten Institutionen\nReporte für Mahnwesen in Ausleihe und Erwerbung\nBibliotheken können Daten für die Deutsche Bibliotheksstatistik zu erfassen. Die notwendigen Daten sollten über das BMS ermittelt werden können. Durch die einheitliche Definition der statistischen Kennzahlen ist eine umfassende, vergleichende Auswertung aller Bibliothekssparten (wissenschaftliche, öffentliche, Spezialbibliotheken) möglich.\nSonderstatistiken wie Statistiken der Fachinformationsdienste (FID)\n\nBei den Systemen der 1. und 2. Generation ist es bisweilen nötig, zusätzliche Werkzeuge zum Einsatz zu bringen, um alle gewünschten Berichte zu erstellen (z.B. BibControl oder Crystal Reports). Während die integrierten Module vor allem auf die Daten des eigenen Systems fokussiert sind, können externe Werkzeuge auch Fremddaten aufnehmen, zum Beispiel Daten aus Besucherzählern.\n\n\nBibliotheksorganisation\nBei der Implementierung oder Anpassung eines BMS ist die Organisation der Bibliothek, die Gestaltung der Prozesse sowie die räumliche Situation zu berücksichtigen. Handelt es sich zum Beispiel um einen öffentliche oder wissenschaftliche Bibliothek? Ist die Organisation als ein einschichtiges oder zweischichtiges System angelegt? Ist es eine einzelne Bibliothek oder eine Zentralbibliothek mit Zweigstellen?\nAuch die Aufstellung der Medien innerhalb der Gebäude nimmt Einfluss auf die Ablauforganisation und damit die Konfiguration des Systems. Dies lässt sich anhand der folgenden Beispiele darstellen:\n\nBücher können in einer anderen Zweigstelle ausgeliehen und zurückgegeben werden.\nDie Bibliothek verfügt über einen Magazinbestand, also physische Medien, die für die Nutzer*innen nicht unmittelbar zur Verfügung stehen.\n\nIn beiden Fällen muss auch der Bestellprozess über das System abgebildet werden. Im Magazin bzw. der Zweigstelle sind der Anschluss und die Aufstellung von Druckern für die Erzeugung von Bestellzetteln zu berücksichtigen. Sind die Medien für die Bibliotheksnutzer*innen direkt zugänglich, entfällt der Bestellschritt und der abzubildende Prozess beginnt mit der Ausleihverbuchung.\nAuch das Rechtemanagement eines BMS ist abhängig von der Größe der Organisation. So sind ggf. verschiedene Berechtigungsstufen für die Bearbeitung von Daten im BMS für die Bibliotheksbeschäftigen einzuführen. Die Berechtigungen bilden die Arbeitsorganisation ab und berechtigen z.B. zum Lesen, Anlegen, Editieren oder Löschen von Ausleihdaten, Nutzer*innen- oder Katalogdaten, Erwerbungsdatensätzen, Gebühreninformationen u.ä.\n\n\nBenutzungsbedingungen\nDie Benutzungsbedingungen werden durch die Ausleihpolitik der Bibliothek bestimmt. Die Gestaltung der Bedingungen erfolgt bezogen auf die Medien und die Bibliotheksnutzer*innen. Dabei geht es um die Frage, was von wem ausgeliehen werden darf und, wenn eine Ausleihe möglich ist, wie und für welchen Zeitraum diese erfolgen kann.\nEine Grundlage zur Abbildung der Benutzungsbedingungen ist die Definition von Benutzungsgruppen. Die Benutzungsgruppen werden durch verschiedene Kriterien charakterisiert. Zur Illustration zwei Beispiele:\n\nGruppenbildung Universitätsbibliothek:\n\nintern: Studierende, Lehrende, weitere Universitätsangehörige\nextern: externe Wissenschaftler*innen, interessierte Öffentlichkeit\n\nGruppenbildung öffentlichen Bibliothek: Kinder, Jugendliche, Erwachsene\n\nDie Einteilung von Bibliotheksnutzer*innen in Gruppen dient der gezielten Zuweisung von Rechten und Ausleihbedingungen und wird für statistische Zwecke genutzt. Die Ausleihpolitik bestimmt, welche Rechte den verschiedenen Benutzungsgruppen zugewiesen werden. So erfolgt z.B. die Gruppeneinteilung in den meisten öffentlichen Bibliotheken nach dem Alter. Einerseits wird damit die Zugänglichkeit der Medien für Kinder und Jugendliche gesteuert. Andererseits dient diese Gruppierung der Einstufung der Gebühren (Kinder und Jugendliche zahlen oft weniger oder keine Gebühren).\nNeben den Gruppen werden Ausleihbedingungen auch auf die Medien bezogen. Zur Illustration:\n\nPräsenzbestände vs. ausleihbare Medien,\nbesonders wertvolle Medien oder\nelektronische Publikationen, die nur unter bestimmten Bedingungen und von bestimmten Benutzergruppen genutzt werden können.\n\nBenutzungsbedingungen werden also sowohl durch die Zugehörigkeit zu einer Benutzungsgruppe als auch durch das Medium selbst bestimmt. Die Beschreibung der Benutzungsbedingungen ist somit eine wesentliche Voraussetzung für die Einrichtung des Ausleihmoduls eines BMS.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#datenverwaltung-in-bms",
    "href": "bibliotheksmanagementsysteme.html#datenverwaltung-in-bms",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Datenverwaltung in BMS",
    "text": "Datenverwaltung in BMS\nEin BMS verwaltet zum einen Daten über die von der Bibliothek bereitgestellten oder vermittelten Ressourcen (vor allem physische und digitale Medien) und zum anderen Daten über wesentliche Arbeitsprozesse (beispielsweise Erwerbung und Ausleihe). Dabei lassen sich grob zwei Arten von Daten unterscheiden:\n\nBibliografische Metadaten zur Beschreibung von Ressourcen\nVerwaltungsdaten zur Unterstützung von Workflows\n\nDarüber hinaus gibt es Digitale Inhalte, die allerdings nicht im BMS verwaltet sondern von dort nur verwiesen werden. Die Datenhaltung erfolgt in der Regel in relationalen Datenbanken (MySQL, PostgeSQL, Oracle/Sybase …). Zur sinnvollen Verarbeitung von Daten im BMS und in Integration mit anderen System müssen Daten bestimmten Datenformaten entsprechen, über Schnittstellen abruf- und ggf. änderbar sein und Mindestanforderungen an die Datenqualität genügen.\n\nBibliografische Metadaten\nBibliografische Metadaten in Form von Titel-, Exemplar- und Normdaten bilden den Kern des klassischen Katalogs. Sie werden u.a. im internen Arbeitskatalog, im Verbundkatalog, im klassischen Nutzerkatalog „OPAC“ und zum Aufbau von Suchindizes für Discovery-Systeme verwendet. Das BMS verwaltet diese Daten um Medien zu beschaffen, auffindbar und zugreifbar zu machen. Diese Daten können von verschiedenen Bibliotheken gemeinsam erstellt, gepflegt und genutzt werden, z. B. über Verbundkataloge. Oft werden bibliografische Metadaten auch als Open Data zur Verfügung gestellt.\n\n\nVerwaltungsdaten\nVerwaltungsdaten dienen der Unterstützung von Arbeitsabläufen innerhalb der Bibliothek (siehe Prozessabbildung). Diese Daten sind zum größten Teil nicht öffentlich und müssen insbesondere im Falle von Daten von Nutzer*innen im Rahmen des Datenschutzes vertraulich behandelt werden.\nZur Interoperabilität mit anderen Informationssystemen innerhalb der eigenen oder übergeordneten Einrichtung gibt es in der Regel nur wenig übergreifend etablierte Standards und Schnittstellen, sodass hier oft zusätzliche Anpassungen an das BMS notwendig sind.\n\n\nDigitale Inhalte\nDies sind letztendlich die Daten die für die Nutzer*innen der Bibliothek vor allem von Interesse sind. Im Falle von Open Access Publikationen bietet das BMS nur einen möglichen Weg zum Zugriff, für erworbene oder lizenzierte Inhalte muss das BMS dagegen unterschiedliche Zugriffsrechte unterstützen.\nDigitale Inhalte werden in der Regel nicht direkt im BMS, sondern in eigenen Content Management Systemen (CMS) und Repositorien verwaltet. Ein BMS muss mit diesen Systemen durch Verwendung gemeinsamer Datenformate sowie von Importen, Exporten und Verlinkungen zusammenarbeiten können. Der Unterschied zwischen Metadaten und Inhalten ist dabei mitunter fließend und hängt vom Anwendungsfall ab. Reicht es oft, Publikationen mit Metadaten zu beschreiben, so umfasst in anderen Fällen die Erschließung von Publikationen auch Dokumentstrukturen und inhaltliche Bestandteile wie z.B. einzelne Abbildungen.\n\\(\\Rightarrow\\) Das Kapitel Digitalisierung geht ausführlicher auf digitale Inhalte ein.\n\n\nDatenformate und Schnittstellen\nDa Computer nicht selbständig mitdenken und interpretieren können, müssen Daten nach klar definierten Regeln aufgebaut sein. Diese Regeln sollten möglichst genau dokumentiert sein. Damit verschiedene Systeme Daten austauschen können, sollten möglichst etablierte Standardformate verwendet werden.\nTrotz gemeinsamer Standards ist ein genaues Hinschauen immer erforderlich, da sich die Handhabung gleicher Formate in der Praxis zwischen verschiedenen Systemen und Einrichtungen unterscheidet.\nNeben Standardformaten gibt es speziellere Anwendungsformate. Diese basieren allerdings in der Regel auf allgemeinen Strukturierungssprachen (CSV, XML, JSON oder RDF) die je nach BMS besser oder schlechter unterstützt werden.\n\nBeispiele für bibliografische Standardformate sind MARC21, BIBFRAME und als kleinster gemeinsamer Nenner Dublin Core. Das PICA-Format bzw. darauf aufbauende Formate werden auf Basis der technischen Infrastruktur von OCLC CBS in Bibliotheksverbünden wie GBV und SWB und an der DNB eingesetzt.\nVerbreitete Metadaten-Schnittstellen sind Z39.50, SRU und OAI-PMH.\nBeispiele für relevante Formate und Schnittstellen für digitale Inhalte sind PDF, METS/MODS und IIIF.\nRelevante Schnittstellen für BMS-Verwaltungsdaten sind unter Anderem LDAP, NCIP, SIP2, PAIA sowie Schnittstellen an Haushaltssysteme wie SAP und HIS.\n\nEine umfassende Übersicht von Datenformaten mit Schwerpunkt auf Formate, die für Bibliotheken relevant sind, bietet die Seite https://format.gbv.de.\n\n\nDatenqualität\nDie Beurteilung von Daten auf Vollständigkeit, Aktualität und Korrektheit kann im täglichen Umgang schwierig sein, vor allem wenn keine geeigneten Werkzeuge zur Verfügung stehen. Ohne kontrolliertes Qualitätsmanagement muss davon ausgegangen werden, dass die Qualität von Daten kontinuierlich abnimmt. Zur Ermittlung und Verbesserung der Datenqualität tragen bei:\n\nRichtlinien legen einheitliche Regeln für Daten fest, beispielsweise durch Katalogisierungsregeln wie RDA (Soll-Stand)\nValidierung ermittelt die Übereinstimmung von Daten mit formal definierten Vorgaben (Ist-Stand)\nStatistiken geben quantitative Auskunft, zum Beispiel über die Anzahl erfolgreich importierter oder exportierter Datensätze\n\nNicht zuletzt beeinflussen auch die Möglichkeiten der Ein- und Ausgabe von Daten ihre Qualität, beispielsweise über die Usability der Katalogisierung.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#marktanalyse-und-beschaffung",
    "href": "bibliotheksmanagementsysteme.html#marktanalyse-und-beschaffung",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Marktanalyse und Beschaffung",
    "text": "Marktanalyse und Beschaffung\n\n\n\n\n\n\nInfo\n\n\n\nDer deutschsprachige BMS-Markt 2024 ist überschaubar. Für den Entscheidungsprozess sind daher vor allem auch der Umfang der gewünschten und gewichteten Funktionalitäten, Varianten des Betriebs- und Geschäftsmodells oder auch die Mitgliedschaft in einem Verbund als Kriterien heranzuziehen.\n\n\nDie Beschaffung eines BMS ist für eine Bibliothek eine große Herausforderung, nicht nur wegen der zu kalkulierenden Kosten, sondern auch wegen des erheblichen Einflusses auf alle bibliothekarischen Arbeitsschritte. Der Aufwand für die Migration von Altdaten, die Revision von Geschäftsgängen und die Schulung von Personal muss bei der Beschaffung berücksichtigt werden. Nicht zuletzt ist die Wahl eines BMS auch eine strategische Entscheidung, da die Möglichkeiten auf zukünftige Anforderungen einzugehen je nach System und eigenen Ressourcen unterschiedlich ausfallen.\nEs kann auch eine ethisch-moralische Entscheidung oder ein Commitment zu einer ökologisch-nachhaltigen Betriebsführung (öffentliche Einrichtungen als Vorzeigecharakter für einen ökologischen Wandel) sein, die Aspekte der nachhaltigen Beschaffung zu berücksichtigen, wie sie sich bei BMS als auch anderen IT-Anwendungen stellen, etwa die Konsequenzen des ökologischen Fußabdrucks der genutzten Infrastruktur (z. B. CO2-Ausstoß des Rechenzentrums).\nAuch aus datenschutzrechtlicher Perspektive gibt es Voraussetzungen zu berücksichtigen, die gegen die Anschaffung bestimmter BMS-Lösungen sprechen (siehe Abschnitt Datenschutz).\nVor diesem Hintergrund ist die Auswahlentscheidung für einzelne Bibliotheken oft ein langwieriger Prozess. Bei Teilnahme an einem Verbund können sich Bibliotheken durch diesen über die BMS, die vom Verbund unterstützt werden, informieren und beraten lassen (siehe Übersicht deutscher Verbundsysteme). Die Beschaffung und Einführung von BMS liegt immer in der Verantwortung der jeweiligen Bibliothek oder der Einrichtung, zu der die Bibliothek gehört.\nDie Gründe für einen Systemwechsel sind primär technischer oder finanzieller Natur. Beispielhaft werden folgend einige Gründe aufgezählt:\n\nDas Altsysteme ist technisch überholt, wird nicht mehr gewartet oder vom Serviceanbieter abgekündigt.\nEs fehlen Schnittstellen für die Integration des BMS in die lokale Informationsinfrastruktur.\nDie Kosten für den laufenden Betrieb sind zu hoch und sollen mit einem anderen System gesenkt werden.\nEine Funktionserweiterung, z.B. für die Verwaltung von elektronischen Ressourcen, kann an dem bestehenden System nicht mehr vorgenommen werden.\n\n\nEntscheidungsprozess\nEin Entscheidungsprozess umfasst typischerweise folgende Schritte:\n\nWorkflowanalyse: Dokumentation bestehender und zukünftig gewünschter Prozesse, die mit dem BMS abgebildet werden sollen\nAnforderungsanalyse: Zusammenstellung und Priorisierung der gewünschten Funktionalitäten und strategischen Zielen unter Einbeziehung aller Stakeholder\nMarktanalyse: Auswahl der in Frage kommenden Lösungen und Betriebsmodelle\nEvaluation: Vertiefte Beschäftigung mit einer Auswahl von Lösungen durch Ausprobieren von Test-Installationen und Kontakt mit Anwendungsbibliotheken\nAufwandsabschätzung von Migration, Einrichtung und Schulung\nAusschreibung, falls erforderlich\nAuswahlentscheidung\n\n\n\nAuswahlkriterien eines BMS\nEs kann davon ausgegangen werden, dass die aktuell am Markt verfügbaren Systeme die klassischen Geschäftsgänge (siehe Kapitel Prozessabbildung) einer Bibliothek gut abbilden können. Die Anforderungen aus dem Kapitel Nutzer*innenzentrierte Gestaltung gelten grundsätzlich natürlich auch hier.\nDie Betrachtung einzelner Systeme einschließlich der Nutzungsszenarien und Use Cases kann sehr aufwändig werden. Daher empfiehlt es sich, die gewünschten Funktionalitäten zu bestimmen und durch die Stakeholder bewerten zu lassen. Die Bewertung kann beispielsweise in Form einer Matrix geschehen, in der die Funktionalitäten nach ihrer Bedeutung/Wichtigkeit einerseits und den zu erwarteten Aufwänden andererseits eingeordnet werden.\n\n\n\n\n\n\nAbbildung 7.2: Beispiel für eine Matrix zur Einordnung von Funktionalitäten\n\n\n\nZur Evaluierung der BMS können bestehende Anforderungskataloge für die Evaluierung von BMS herangezogen werden, zum Beispiel der gemeinsam von HBZ und VZG entwickelte Kriterienkatalog. Dieses umfangreiche Dokument zeigt die Anforderungen an alle Komponenten auf Grundlage der Analyse von sehr ausgereiften Prozessen in Altsystemen auf. Es empfiehlt sich, insbesondere diejenigen Funktionalitäten genau zu überprüfen, die strategisch von besonderer Bedeutung sind.\n\n\nMarktanalyse\nDa es sich bei BMS um relativ spezialisierte Software handelt und in den letzten Jahren einige Produkte aufgekauft oder eingestellt wurden, ist der Markt sehr überschaubar (siehe Kapitel aktuell relevanten BMS).\nNeben der Wahl konkreter Produkte gibt es grundsätzlich drei Möglichkeiten:\n\nBeitritt zu einem Bibliotheksverbund und Nutzung eines BMS, das von diesem Verbund unterstützt wird, zu den jeweils gültigen Konditionen\nLizenzierung eines kommerziellen BMS und Einkauf einschlägiger Dienstleistungen für Hosting, Wartung und Support sowie Migration und individuelle Konfiguration\nImplementierung und individuelle Konfiguration eines Open Source-BMS, entweder in Eigenregie oder durch vollständige oder punktuelle Unterstützung von einschlägigen Dienstleistern für Hosting, Wartung und Support sowie Migration und individuelle Konfiguration\n\nDie Vor- und Nachteile im Überblick:\n\n\n\n\n\n\n\n\n\n\nVerbund\nkommerzielles BMS\nOpen Source BMS\n\n\n\n\nVorteile\nregelmäßige Produktentwicklung\ngewisser State-of-the-Art garantiert\nklare Kosten- und Leistungsstruktur\ngroße Anwendungscommunity\nregelmäßige Produktentwicklung\neinheitlicher Leistungsumfang\nklare Verantwortlichkeiten\nniedrige Anschaffungskosten\ngroße Anwendungscommunities\noftmals regelmäßige Produktenwicklung\nviele Dienstleister, die Services rund um Migration, Betrieb und individuelle Anpassung anbieten\noffene Schnittstellen und Formate\n\n\nNachteile\nbegrenzte individuelle Anpassung\nWartezeiten bei individueller Anpassung\neher geringe individuelle Anpassbarkeit\nrelativ hohe und intransparente Preise\nAbhängigkeit bei der Weiterentwicklung\nevtl. Verlust der Datenhoheit\nz.T. proprietäre (herstellerspezifische) Systeme und Schnittstellen\nerfordert eigene IT-Kapazitäten oder Outsourcing\nRisiko der Sicherung von Nachhaltigkeit und Kompatibilität\n\n\n\nVerbünde bieten in der Regel ein oder zwei Lösungen an, die entweder kommerziell oder Open Source sind. Die Mitgliedschaft in Verbünden kann ein kostengünstiger Weg sein, um mit einem BMS und dazugehörigen Dienstleistungen versorgt zu werden. Allerdings steht möglicherweise nicht allen Bibliotheken die Mitgliedschaft in einem Verbund offen oder bedingt andere Nachteile (z.B. den Zwang, an der Fernleihe teilzunehmen und begrenzte Möglichkeiten zur individuellen Anpassung der Software).\nBei kommerzieller Software fallen typischerweise Lizenzkosten an, die sich nach der Größe der Bibliothek oder der übergeordneten Einrichtung richten (z.B. an der Anzahl von Mitarbeitenden, Studierenden oder Einwohner). Dabei werden einmalige Beschaffungs- und jährlichen Wartungskosten unterschieden. Es muss klar vereinbart werden, welche Dienste mit den Wartungskosten (Support, Update auf neue Versionen …) abgegolten sind.\nDer Betrieb der Lösungen kann von den Anbietern oder anderen Dienstleistern (Verbund, andere kommerzielle Anbieter) übernommen werden (Cloud/Software as a Service), d.h. die Bibliotheken brauchen keine eigenen Server zur Verfügung stellen und administrieren. Eine Installation auf eigenen Servern (On-Premise-Lösung) erfordert hingegen eigenes, ausgebildetes Personal.\nBei Open Source-Lösungen gibt es keine initialen Anschaffungskosten. Bei Verfügbarkeit entsprechender Server-Infrastruktur und erfahrenem Personal kann eine Bibliothek die Software selbst installieren und in Betrieb nehmen oder diese Leistungen von Dienstleistern einkaufen.\nDie initiale Konfiguration sowie die Migration von Daten aus einem Alt-System können ebenso von den Bibliotheken selbst durchgeführt werden oder sind Teil des Kauf-/Wartungsvertrages.\nDie laufende Betreuung des Betriebs von BMS erfordert speziell geschultes und berechtigtes Personal - sogenannte System-Bibliothekar*innen. In wenigen Fällen wird die Systembetreuung an Dienstleister (beim Hoster) übergeben.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#prozessabbildung",
    "href": "bibliotheksmanagementsysteme.html#prozessabbildung",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Prozessabbildung",
    "text": "Prozessabbildung\nFür den Einsatz eines BMS bilden Prozessbeschreibungen bzw. Workflows eine wesentliche Grundlage. Auf der Basis der Abbildung der Kernprozesse wie Erwerbung, Katalogisierung, Ausleihe sowie der Rolle des Systems und anderer Akteure können Anpassungen (leichter) vorgenommen werden.\nZur Modellierung, Dokumentation und Visualisierung von Workflows bietet sich klassischerweise eine Modellierungssprache wie BPMN (Business Process Model and Notation) an. Für diese und verwandte Sprachen existieren umfangreiche Werkzeuge und Toolchains, mit denen einerseits Prozesse erstellt werden können, gleichzeitig aber auch – sollte das nötig sein – die modellierten Prozesse automatisiert werden können. Im Endeffekt bedeutet dies, dass aus dem Prozessmodell Programmcode erzeugt wird.\n\n\n\n\n\n\nAbbildung 7.3: Beispiel einer BPMN-Prozessabbildung\n\n\n\nLässt man die Aspekte der Prozessautomatisierung oder Codegenerierung außer Acht, so lässt sich auch eine abgespeckte BPMN-ähnliche Semantik nutzen, um Prozesse zu dokumentieren und zu visualisieren. Andere Alternativen zur Modellierung finden sich in den verschiedenen Diagrammformen der UML (Unified Modelling Language).\nAus heutiger Sicht sollten für die im Folgenden genannten Bereich Prozessbeschreibungen erstellt werden, damit potentielle BMS an Hand dieser geprüft werden können. Hierbei könnten sich Notwendigkeiten für Änderungen in den Prozessabläufen der Bibliothek ergeben, die auf Basis der Beschreibungen genauer adressiert werden können.\n\nNutzer*innen\nAls Nutzer*innen werden in diesem Kapitel diejenigen Menschen bezeichnet, die mit einem BMS interagieren. Man unterscheidet zwischen den Bibliotheksbeschäftigten, die mit dem Modulen Ausleihe, Erwerbung, Katalogisierung, ERM etc. interagieren, und den Bibliotheksnutzer*innen (oft auch als Leser*innen bezeichnet), die mit dem BMS über das Modul OPAC oder nur indirekt über ein Discovery-System oder ein anderes Drittsystem mit dem BMS in Kontakt kommen.\n\n\nUser-Interfaces für verschiedene BMS-Anwender*innen\nDie Bibliotheksbeschäftigten und die Bibliotheksnuter*innen haben verschiedene Sichtweisen auf ein BMS. Bibliotheksbeschäftigte müssen über das User-Interface bei ihrer Arbeit spezifisch durch die Workflows geführt werden. Dabei ist auf eine einheitliche Benutzungsführung und Gestaltung der Oberfläche zu achten.\nFür die Bibliotheksnutzer*innen steht die Information über die Dienste der Bibliothek, deren Bestand und die Nutzung des Bestandes im Vordergrund. Bibliotheksnutzer*innen kommen dabei häufig mit mehreren IT-Systemen in Kontakt (BMS, OPAC, Web-Server, Discovery-System …). Daher sollte auch hier auf eine einheitliche Oberfläche der eingesetzten IT-Systeme geachtet werden, auch bezüglich Accounts und Login, zumindest aber auf ein einheitliches Design und eine einheitliche Benutzerführung.\nEs ergeben sich daraus die folgenden Anforderungen\n\nIntuitive Benutzbarkeit\nBarrierearme Gestaltung\nResponsivität\n\nDiese Themen werden auch in den Abschnitten zu rechtlichen Rahmenbedingungen und im Kapitel zu den Anforderungen an Bibliotheks-IT angesprochen.\n\n\nErwerbung\nEin BMS sollte das Bibliothekspersonal bei den folgenden Aufgaben unterstützen:\n\nÜberprüfen von vorhandenen Beständen (Vorakzession)\nImport von Erwerbungsdaten, z. B. von Patron-Driven- Acquisition (PDA) und Approval-Plänen\nAufgabe von Bestellungen bei definierten Lieferanten auf verschiedenen Wegen\nVerwaltung von Lieferantendaten\nAnlegen und Verwalten von Bestellungen von Zeitschriften, Fortsetzungswerken mit Abonnementsmanagement\nÜberwachung von Bestellungen\nAnlegen und Verwalten von Budgets\nAkzessionierung von Medien\nRechnungsverwaltung inkl. Schnittstellen für haushalterische Systeme (E-Rechnungen)\nMahnwesen und Reklamation\nVerwaltung von Bindeaufträgen\nVerwaltung von Nicht-Kauf-Beschaffungen\nStatistik und Reporting\n\nDiese Aufgaben lassen sich mit den am Markt befindlichen Systemen in der Regel gut abbilden. Allerdings werden die meisten Bibliotheken für die Verwaltung von notwendigen Bestellungen von Materialien jenseits des Bibliotheksbestandes (Büromaterial, IT-Ausstattung etc.) zusätzliche haushalterische Systeme einsetzen. Das Erwerbungsmodul ist insofern meist nur eine Komponente im Haushaltswesen.\n\n\nVerwaltung von elektronischen Ressourcen\nFür die Verwaltung elektronischer Ressourcen (ERM) sollten folgende Aufgaben unterstützt werden:\n\nVerknüpfungen zur Erwerbung, Haushalt/Budget, Rechnungswesen und Lieferanten bzw. Plattformbetreibenden\nErfassung von Lizenzinformationen nach unterschiedlichen Erwerbungsmodellen wie Pakete, Allianz- oder Nationallizenzen sowohl in maschinenlesbarer Form als auch in Textform und als gesicherte Dokumentenablage für Verträge\nZuordnung von Titeln und digitalen Inhalten zu Paketen\nVerwaltung von Paketen mit Vergleich, Zugangs- und Abgangskontrolle\nVerwaltung von Lizenzen\nBezug von bibliografischen Daten von Aggregatoren, Verlagen und Knowledge Bases\nUnterstützung der direkten Verlinkung auf Volltexte aus Katalogen und Discovery-Systemen\nAuslieferung von aussagekräftigen Zugangsinformationen in Kataloge und Discovery-Systeme\nUnterstützung bei der Bereitstellung von digitalen Inhalten jenseits von proprietären Apps\n\nDie BMS der 1. und 2. Generation haben erhebliche Defizite bei der Verwaltung von elektronischen Ressourcen. Die Bereitstellung von entsprechenden Funktionalitäten ist daher ein Alleinstellungsmerkmal von BMS der neuen Generation.\nZur Bereitstellung von Daten für die Verwaltung von elektronischen Ressourcen bieten sich außerdem sogenannte Electronic Resource Management-Tools (z. B. FOLIO-ERM, Coral, LAS:eR) und Datenbanken („Knowledge Base“) wie GOKb, we:kb oder kommerzielle Knowledge Bases an.\n\n\nKatalogisierung\nBei der Katalogisierung müssen folgende Tätigkeiten unterstützt werden:\nErfassung von unterschiedlichen Medientypen gemäß aktueller Metadaten-Standards\n\nÜbernahme von Katalogdaten aus Bibliotheksverbünden\nMöglichkeit der Integration von Normdaten\nErfassung von lokalen Daten\nKonfigurierbarkeit von Erfassungsmasken\n\n\n\nKatalog\nDer Katalog ist die Sicht für die Bibliotheksnutzer*innen auf die Bestände der Bibliothek. An das Katalogmodul werden folgende Anforderungen gestellt:\n\nWeb-Interface nach aktuellen Standards bezüglich Barrierefreiheit, Responsivität etc.\nAngebot von Möglichkeiten der Suche nach bekannten Titeln\nAngebot von Möglichkeiten der Suche nach Themen\nFilterung von Trefferlisten nach formalen oder inhaltlichen Kriterien bzw. Standorten\nAnzeige von Verfügbarkeitsinformationen\nAnzeige von Neuerwerbungslisten\nKontobezogene Funktionalitäten (Einsicht, Verlängerung, Vormerkung, Bestellungen)\nAnzeige von Neuigkeiten und wichtigen Links auf der Startseite des Katalogs\nAnpassbarkeit der Katalogoberfläche an das Corporate Design (wenigstens Logo und Farbschema)\n\nDas Katalogmodul als Teil des BMS war zunächst nur ein primär intern genutzes Suchinstrument für alle erfassten Bestände. Mit fortschreitender Technik entstanden verschiedene Konstrukte, die Daten der Bibliothek auch direkt den Nutzer*innen zur Verfügung zu stellen:\n\nKlassischer Katalog (OPAC) als Bestandteil des BMS\nKatalog als separates Modul (nicht Bestandteil des BMS), selbst entwickelt, zugekauft oder als Open Source\nDiscovery-Systems als Bestandteil des BMS: Daten aus dem eigenen Bestand sowie Fremddaten, die als Metadaten zur Verfügung stehen. Eine Herausforderung besteht hierbei darin dass sich Anforderungen an die Suchoberfläche relativ schnell ändern, so dass die BMS-eigene Suchoberfläche nicht leicht auf dem neuesten Stand erscheint.\nDiscoverysystem als zugekauftes Modul eines anderen Herstellers oder als Eigenbau mit zugekauften Metadaten oder als Open Source mit offenen Daten oder zugekauften Metadaten. Eine Herausforderung hierbei besteht darin, die im BMS gehosteten Informationen, zum Beispiel über Ausleihstatus und Verfügbarkeit, auch in der Oberfläche des Discovery-System aktuell darzustellen.\n\n\n\nAusleihe\nEin BMS sollte die folgenden Aufgaben der Ausleihe unterstützen:\n\nAnlegen von Benutzergruppen, Standorten, Medienarten\nImport von Benutzerdaten aus Hochschulverwaltungssystemen (IDM) oder Online-Anmeldung\nAbbildung der in den Benutzungsordnungen festgelegten Ausleihbedingungen, z.B. Leihfristen nach Benutzergruppen, Standorten, Medienarten\nVerbuchung von Medien (Ausleihe, Rücknahme)\nKonfiguration von Ausdrucken für Bestellzettel, Vormerkungen und Mahnungen\nErmöglichen von Bestellungen und Vormerkungen\nMahnwesen (Fristen, Mahnstufen)\nBenachrichtigungen für Bestellungen, Vormerkungen, Mahnungen, Leihfristerinnerungen\nGebührenverwaltung\nStatistik und Reporting, u. a. Erzeugung von Listen (überfällige Medien, nicht abgeholte Vormerkungen)\nAnbindung an Bezahlsysteme (Kassenautomaten, Online-Bezahlsysteme)\nAnbindung von Verfügbarkeits- und Kontoinformationen an Discovery-Systeme\nAnbindung an Automatisierungslösungen und externe Verbuchungssysteme (über Schnittstellen wie SIP2 und NCIP und mittels RFID)\n\nDie Parametrisierung der Ausleihe ist ein besonders komplexer Bereich der BMS-Installation aufgrund der Vielzahl von zu beachtenden Benutzungsregeln, der Sensibilität der Daten und der besonderen Relevanz eines reibungslosen Betriebs beim Versand von Benachrichtigungen. Ein Beispiel für eine solche Komplexität ist die das Verhalten bei Feiertagen: Hier muss ein Schließtagekalender regelmäßig gepflegt werden, um zu vermeiden, dass Leihfristenden auf Feiertage oder Wochenenden fallen.\n\n\nAutomatisierung und Selbstbedienung\nAls Automatisierung wird die Möglichkeit bezeichnet, die Geschäftsgänge einer Bibliothek mit digitalen Werkzeugen abzubilden und durchführen zu können. Dazu sind Maschinen notwendig, die die entsprechenden Funktionen anbieten. Das schon recht betagte Standardprotokoll für die Kommunikation zwischen BMS und Automat ist SIP2. Dieses Protokoll hat den Nachteil, dass es ohne Verschlüsselung entwickelt wurde und daher - sofern es sich um ein BMS in der Cloud handelt zumindest - über stunnel verschlüsselt getunnelt wird. Moderne BMS unterstützen mittlerweile zusätzlich auch allgemeine Kommunikationsprotokolle, etwa über REST, sodass das Tunneln von Verbindungen nicht mehr nötig ist. Außerdem ist man nicht mehr daran gebunden, dass anzubindende Geräte SIP2 unterstützen, was deutlich mehr Marktalternativen öffnet.\nNachfolgend werden Automaten für die Selbstbedienung im Bereich der Ausleihe dargestellt.\n\nSelbstverbucher / Ausleihautomaten\nSelbstverbucher / Ausleihautomaten bestehen meist aus einer Auflagefläche für die auszuleihenden Medien, einer Schnittstelle für Bibliotheksausweise sowie einem PC, der die Endgeräte verwaltet und mit dem BMS kommuniziert. Bei einer funkgestützten Medienerkennung (RFID) gibt es die Möglichkeit der Stapelverbuchung, es werden also vom Automaten mehrere gestapelte Bücher erkannt und zur Verbuchung angeboten. Bei einer barcodegestützten Medienerkennung wird jedes Medium einzeln verbucht.\nBibliotheksausweise gibt es in verschiedenen Ausprägungen: Barcode (1D-Code), Funkchip (u. U. proprietär, Bsp.: Intercard), QR-Code (2D-Code). Die 1D- oder 2D-Codes können entweder auf Papier oder in einer App auf dem Smartphone beigebracht werden. Die Schnittstelle im Automaten muss auf die vorhandenen Ausweistypen vorbereitet sein.\nBei Nichtvorhandensein einer separaten Rückgabeanlage kann der Selbstverbucher / Ausleihautomat auch eine Rückgabefunktion anbieten. Zumeist werden die zurückgegebenen Medien unsortiert gesammelt; im Anschluss erfolgt die Sortierung durch das Bibliothekspersonal.\nNach der Rückgabe- oder Ausleihverbuchung muss der Selbstverbucher / Ausleihautomat auch die Buchsicherung (sofern vorhanden) bedienen. Bei der in vielen Bibliotheken auslaufenden EM-Sicherung (elektromagnetisch über einen im Medium eingeklebten magnetisierbaren Metallstreifen) geschieht dies über die Ansteuerung eines Elektromagneten mit hörbarem Feedback an die Nutzer*innen („klack“). Bei RFID-Sicherung wird bei erfolgter Verbuchung ein Sicherungsbit auf dem RFID-Chip verändert. Aufgrund der größeren Geschwindigkeit dieses Vorganges geschieht dies ohne Feedback an die Nutzer*innen.\n\n\nRückgabeautomat / -sortierung\nEin separater Rückgabeautomat hat zum einen den Vorteil, dass die Prozesse Ausleihe und Rückgabe bei starker Nutzung entzerrt werden und zum anderen, dass eine Sortierung der zurückgegebenen Medien möglich ist. Die Medien werden von den Nutzer*innen auf ein Förderband gelegt und eingezogen (außer Reichweite des Nutzer*innen. In dieser Position wird der Barcode auf dem Medium oder der RFID-Chip gelesen. Wird keines der beiden erkannt, wird das Medium wieder zurückgegeben. Bei erfolgreicher Erkennung und Verbuchung im BMS (und anschließender Aktivierung der Buchsicherung) wird im BMS mithilfe der Signatur oder Mediennummer erfragt, wie das Medium sortiert werden soll. In den meisten BMS gibt es dazu Tabellen, die z.B. über die Anfänge von Signaturen oder anderen Kriterien (Bsp: „SN …“ in Wagen 3, „ist vorgemerkt“ in den Wagen x) arbeiten. Steht das Sortierziel fest, wird das Medium über Förderbänder zu dieser Stelle transportiert und abgeworfen. Das Ziel kann ein sog. Tray sein, ein oben offener Korb oder Wagen, oft mit einem gewichtgesteuerten Boden, damit die Medien nicht allzu tief fallen. Alternativ bieten immer mehr Hersteller sog. Ergocarts an, auf die die Medien so geschichtet werden, dass sie am Regal Rückenschonend aus einem Stapel entnommen und einsortiert werden können.\nÜblicherweise gibt es am Rückgabeautomaten keine Authentifizierung.\nEs gibt auch Rückgabeautomaten, die eine erneute Ausleihe des gerade zurückgegebenen Werkes an den/ie gleiche Bibliotheksnutzer*in ermöglichen. Dies ist in den Fällen sinnvoll, wenn die maximale Leihfrist / maximal mögliche Verlängerungen der Leihfrist erreicht ist und der/die Bibliotheksnutzer*in das Buch weiter nutzen möchte und das Medium nicht anderweitig bestellt ist.\n\n\nKassenautomat\nEin Kassenautomat ermöglicht die personalfreie Bezahlung der offenen Gebühren. Auch hier wird erst der Nutzungsausweis eingelesen und nach einer optionalen Passworteingabe die offenen Gebühren angezeigt. Die Gebühren können dann mit Bargeld oder bargeldlos gezahlt werden. Auf eine Bargeldzahlung wird zunehmend verzichtet, da das Handling von Bargeld aufwändig und teuer ist.\n\n\nFernleihautomat\nAus einem Fernleihautomat können Fernleihen personalfrei an Nutzer*innen ausgegeben werden. Da diese Bücher weder mit dem eigenen System der Bibliothek gesichert noch verbuchbar sind, muss eine separate Verbuchung durchgeführt werden. Die Nutzer*innen bekommen eine Nachricht, dass ihr bestelltes Medium in einem Fach mit der Nummer xy bereit liegt sowie eine PIN zur Öffnung dieses Faches. Sobald das Fach geöffnet wird, wird das Medium auf das Konto des Nutzers/der Nutzerin verbucht. Auch eine Öffnung des Faches mit einem funkgesteuerten Nutzungsausweis statt der PIN ist möglich.\n\n\nSicherungsgates\nSicherungsgates erkennen unverbuchte Medien, die die Bibliothek verlassen. Die dafür übliche Technik war in den letzten Jahrzehnten die EM-Sicherung, also die Erkennung der Magnetisierung von metallischen Streifen, die in die Medien geklebt waren. Mit der Umstellung auf RFID geschieht die Buchsicherung über Funk, ein Sicherungsbit im Speicher der RFID-Chips wird untersucht. Bei EM-Sicherung ist der maximale Abstand zwischen zwei Gates zur halbwegs zuverlässigen Erkennung ca. 90 cm und stellt somit eine Einschränkung des Zugangs, z.B. bei der Nutzung mit Rollstühlen, dar. Etwa der gleiche Abstand ist notwendig bei RFID-HF, bei RFID-UHF (Reichweite bis zu 10m) ist ein sehr großer Abstand möglich und somit der Verzicht auf eine Einengung des Ausgangs.\nBei Erkennung eines gesicherten (und nicht entliehenen) Mediums ertönt ein Warnton. Bei manchen Systemen wird das entsprechende Medium mit Titel und Cover auf einem Monitor angezeigt.\nSicherungsgates verhindern keine Diebstähle, Diebe wählen andere Wege. Sicherungsgates verhindern das versehentliche Verlassen der Bibliothek mit unverbuchten Medien.\n\n\n\nAnbindung von Systemen über Schnittstellen\nEin BMS muss in der Lage sein, mit anderen Systemen automatisiert Daten auszutauschen. Diese Austauschprozesse betreffen folgende Szenarien\n\nBereitstellung von Konto- und Verfügbarkeitsinformationen, z.B. über PAIA und DAIA\nAnbindung an Buchhaltungssysteme wie SAP oder HIS Haushalt-ERP\nAnbindung an Tools für statistische Auswertungen (s.a. Kapitel Statistik)\nBereitstellung von bibliografischen Daten\nRecherche in Fremddatenbeständen, z.B. über Z39.50\nSchnittstellen zu Kataloganreicherungsdiensten (Buchcover)\nSchnittstellen zu IDM-Systemen (s.a. Kapitel IDM)\nSchnittstellen zu einschlägigen Plattformen der jeweiligen Zielgruppen, zum Beispiel Lernmanagementsysteme\n\nDie Systeme der neuen Generation verfügen in der Regel über Schnittstellen, über die sie in die bestehenden Informationsinfrastrukturen, d.h. die umgebenden Systeme, eingebunden werden können.\nEine Schnittstelle (engl. Interface oder manchmal auch API - application programming interface) bildet einen definierten Kommunikationsweg zwischen verschiedenen Systemen als „Gesprächspartner“. Im bibliothekarischen Universum gibt es für diese Fälle auch schon viele etablierte Austauschformate, etwa SIP2. Ein BMS „von der Stange“ kann im Regelfall die üblichen Austauschformate unterstützen, sodass ein Austausch zwischen den gängigen Systemen einfach möglich ist. Hierzu zählen insbesondere der jeweilige Bibliotheksverbund, etwa zum Austausch von Metadaten oder für das verteilte Lizenzmanagement, aber auch nutzer*innen-nahe Dienstleistungen, wie die Fernleihe.\n\n\nNicht-bibliothekarische Schnittstellen\nEin BMS existiert im Regelfall nicht nur für sich oder nur im Kosmos der eigenen und anderer Bibliotheken, sondern ist auch in die lokalen IT-Strukturen eingebunden.\nEin gutes Beispiel ist der Einsatz eines BMS an einer Hochschule: Im Regelfall sind alle Mitglieder einer Hochschule auch gleichzeitig (potenzielle) Nutzer*innen der Bibliothek. Die Daten der Mitglieder dieser Einrichtung werden an einer zentralen Stelle verwaltet und sollen durch andere Systeme, z.B. im Bibliothekssystem, durch Verknüpfung nachgenutzt werden. Dies ist die Rolle des Identity Managements (IDM).\n\nIdentity Management\nEin IDM (Identity Management System) ist ein System, mit dem die Basisdaten von Personen und Gruppen an zentraler Stelle verwaltet werden können. Dies sind etwa persönliche Daten, Kontaktdaten und Angaben zur Organisation. Ziel ist es, alle relevanten Informationen nur an einer zentralen Stelle vorzuhalten. In verknüpften Systemen sollen Aufwände einer ggf. nötigen Synchronisation und die Speicherung von dubletten Informationen vermieden werden. Damit Personen in einem System eindeutig identifiziert werden können, existieren zumeist eine oder mehrere eindeutige IDs, etwa die Matrikelnummer eines Studierenden.\nDas IDM hält im Regelfall mehr Daten über eine/n Nutzer/in bereit, als von den jeweiligen verbundenen Systemen benötigt werden. Beispielsweise könnte in einem IDM vorgehalten werden, dass eine Person Mitarbeiterin einer Hochschule ist, dass sie zu einer gewissen Fakultät der Hochschule gehört und dass sie zu einer bestimmten Arbeitsgruppe gehört. In der Kommunikation des BMS mit dem IDM ist jedoch nur die erste der Informationen relevant, etwa um die Ausleihkonditionen der Person festlegen zu können. Daher werden in der Kommunikation mit einem IDM nach dem Prinzip der sogenannten „Datensparsamkeit“ nur relevante Informationen übertragen.\nEin IDM kann als Identity Provider zu einem Authentifizierungsdienst werden. Über diesen Dienst kann man dann unter Umständen ein Single Sign On realisieren, bei dem die Daten der Nutzer*innen nicht mehr an den Service oder Content Provider weitergegeben werden, sondern nur noch eine Art Ticket, das eine Erlaubnis regelt. Im Idealfall gilt dieser Dienst dann für verschiedene Service- bzw. Contentprovider, so dass für deren Nutzung nur eine einmalige Anmeldung erforderlich ist.\nAuthentifizierungsprotokolle sind bspw.: Shibboleth / SAML2, OpenID\nSoftwareprodukte für IDM sind: SAP (mit Plugins), Microsoft Active Directory, uvm.\n\nSpeicherung von Nutzer*innendenaccounts\nEin Account besteht aus den Kontaktdaten des Nutzenden sowie Authentifizierungsinformationen. Hier ist Datensparsamkeit nach DSGVO geboten. Für die Speicherung aller personenbezogenen Daten müssen die Notwendigkeiten oder rechtlichen Gründe nachgewiesen werden. Als Beispiel kann die Speicherung des Geburtsdatums angesehen werden. Wird für die Begründung der Speicherung des Geburtsdatums die Prüfung der Volljährigkeit oder die Befähigung eines Seniorentarifes herangezogen, ist davon auzugehen, dass die Speicherung des Geburtsdatums nicht notwendig ist. Wird zur Begründung eine als notwendig erachtete Adressermittlung bei Behörden angegeben, ist die Speicherung des Geburtsdatum notwendig, um eine Adressermittlung (zur Wiederbeschaffung vermisster Exemplare) zu ermöglichen. Die Speicherung nutzungsbezogener Daten wie Verweise auf die ausgeliehenen Medien, angefallene Gebühren, offene Bestellungen und bestellte Digitalisate muss in der Regel nicht explizit begründet werden.\nSofern die übergreifende Institution über eine Datenbank zur Speicherung der Accounts verfügt (IDM, Identity Management) ist eine Anbindung an diese sinnvoll. Dieses IDM enthält dann allerdings nicht notwendigerweise die externen Nutzer*innen der Serviceeinrichtung.\nDie technisch einfachste Lösung für Accounts der externen Bibliotheksnutzer*innen ist die Speicherung im IDM der übergeordneten Einrichtung, sofern vorhanden. Komplexer ist die Speicherung in einem separaten System, da dann bei Autorisierung u. U. mehrere Systeme abgefragt werden müssen.\nDatenschutzbezogene Vorgehensweisen z. B. in Bezug auf personenbezogene und personenbeziehbare Daten von Nutzer*innen finden sich in Abschnitt Datenschutz im Kapitel zum technischen Betrieb eines BMS.\n\n\n\nBezahlsysteme\nOnline-Payment, Kassensysteme/-automaten (siehe auch Kapitel Kassenautomat)\n\n\nE-Rechnung\nE-Rechnungen müssen seit 2020 von Einrichtungen des Bundes, der Länder und Kommunen verarbeitet werden können. Der Umgang mit E-Rechnungen ist sehr unterschiedlich geregelt. Zum Teil nehmen Einrichtungen nur noch an einer zentralen Stelle E-Rechnungen entgegen. In anderen Einrichtungen werden E-Rechnungen dort entgegengenommen, wo die Bestellungen ausgelöst wurden. Es gibt verschiedene Formate, in der eine E-Rechnung übermittelt werden kann (PDF, XML oder direkt per EDIFACT).\nElektronische Rechnungen kommen immer dann ins Spiel, wenn kostenpflichtige Bestellvorgänge über das BMS abgewickelt werden. In diesem Zusammenhang entstehen Rechnungen von Lieferanten, die von der Bibliothek oder ihrer Organisation zu begleichen sind.\nOhne einen „E-Rechnungs-Workflow“ würde dies bedeuten, dass Rechnungen der Lieferanten bei der Bibliothek eingehen, einem Bestellvorgang zugeordnet werden müssen, von der jeweiligen Rechnungsstelle beglichen und schließlich wieder im BMS „abgehakt“ werden müssen. Diese repetitiven Workflows lassen sich mittlerweile weitgehend automatisieren. Das BMS ist in der Lage, elektronisch übermittelte Rechnungsdaten automatisiert den jeweiligen Bestellprozessen zuzuordnen. Bei einer gleichzeitigen Anbindung eines elektronischen Rechnungswesens z.B. über SAP können auch die Zahlungsinformationen automatisiert zugeordnet werden und somit ein Bestellvorgang komplett automatisiert abgeschlossen werden.\n\n\nStatistik\nMit dem Begriff „Statistik“ können verschiedene Dinge im Rahmen eines BMS gemeint sein, etwa Betriebsstatistiken, wie die Rechnerauslastung eines Servers, auf dem das BMS betrieben wird. In diesem Fall ist jedoch mit „Statistik“ gemeint, dass Daten des BMS in eine Form gebracht werden können, aus der Mitarbeiter*innen der Bibliothek Informationen ziehen können, die zur Dokumentation, zum Reporting oder zur weiteren Arbeit benutzt werden können.\nBeispiele für Statistiken sind z. B. die Ausleihzahlen einer Bibliothek, ggf. aufgeteilt nach verschiedenen Themen oder Fächern, die Rückschlüsse auf die Nutzung bestimmter Medien ermöglichen. Dies ist für das Bestands- und Budgetmanagement der Einrichtung relevant.\nEinige BMS bieten eine integrierte Statistikfunktion an. Andere halten ihre Daten in einer Datenbank und diese müssen aktiv exportiert werden. Wieder andere bieten entsprechende Schnittstellen, über die statistische Daten exportiert und in geeignete Drittsysteme (im einfachsten Fall eine Tabellenkalkulation) übernommen werden können.\nJe nach Anforderung an den Umfang und an die Arbeit, die mit statistischen Auswertungen erfolgen soll, kann die Entscheidung fallen, die aus dem BMS kommenden Daten nur in eine Tabellenkalkulation zu exportieren, oder eine speziell auf die statistische Datenanalyse zugeschnittene Statistik-Software zu nutzen. Hier kommen Softwarelösungen wie Excel, BibControl oder gar komplexe Statistik-Plattformen wie SPSS in Frage. BMS der neuesten Generation bringen schon eigene Statistik-Module mit, die eine externe Lösung optional machen. Diese Systeme können automatisiert oder manuell COUNTER-Reports für statistische Daten zur Nutzung digitaler Medien importieren.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#technischer-betrieb",
    "href": "bibliotheksmanagementsysteme.html#technischer-betrieb",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Technischer Betrieb",
    "text": "Technischer Betrieb\nDer technische Betrieb eines BMS variiert je nach Betriebsmodell (lokale Installation, gehostete Variante oder Cloud-Dienst). Kosten entstehen dabei für Lizenz- und Wartungsverträge sowie für Betriebsressourcen. Für den Betrieb sind weiter das Monitoring sowie die Aspekte der IT-Sicherheit, Backup und Datenschutz zu berücksichtigen.\n\nKosten\nDie Anschaffungskosten eines BMS machen nur einen kleinen Teil aus. Wichtiger ist, sich über folgende Kosten klar zu werden:\n\nPersonalkosten für den laufenden Betrieb\nLizenzkosten und Wartungsverträge der Software\nBetriebsressourcen, wie z.B. Serverraum, Energieverbrauch, Wartung, Backuplösungen\n\nPersonalkosten und Ressourcen richten sich hauptsächlich nach Art der Installation (Lokal, Hosting oder Cloud). Lizenzkosten sind teilweise nach Größe der Einrichtung gestaffelt, d. h. sie richten sich nach Anzahl des Personals, der verwalteten Medien und/oder Endnutzer*innen.\nInsbesondere der Punkt Personalkosten kann zu einem Engpass bzw. Risiko werden, denn in vielen Fällen zeigt sich, dass einige wenige Personen durch ein BMS gebunden werden und gleichzeitig auch die einzigen sind, die das System in der Tiefe bedienen können. Wirklich kritisch wird es, wenn nur eine einzige Person diese Rolle erfüllt. Je mehr Verantwortung beim Betrieb auf das Personal vor Ort fällt (lokaler Betrieb), desto wichtiger wird dieser Aspekt. Selbst bei der Nutzung eines Cloud-BMS ist davon auszugehen, dass für die fachliche Administration der Software Personal dauerhaft gebunden ist. Bei dieser Betriebsmethode gibt der Anbieter meistens den Updatezeitpunkt vor, insofern müssen unter Umständen Workflows in der Bibliothek aufgrund von Änderungen in der Software durchgeführt werden, ohne dass man die zeitliche Planung dafür in der Hand hat.\nUm Personalengpässe zu vermeiden, ist es sinnvoll, Einführungsprozesse nur in einer Expertengruppe durchzuführen und Verantwortlichkeiten auf mehrere Schultern zu verteilen (Ausfallsicherheit, Urlaubsvertretung usw.). Auch die gute Dokumentation teils komplexer Zusammenhänge sollte bedacht werden, damit Fachwissen nicht nur in den Köpfen einiger weniger Mitarbeiter*innen schlummert.\n\n\nInstallation & Updates\nZur Einrichtung eines BMS gehört:\n\nInstallation auf einem Server: Erfordert Kenntnisse in Systemtechnik (Hardware, Server, Kommandozeile …). Wenn Hosting durch Drittanbieter geleistet wird (Cloud, Dienstleister wie Verbundzentrale o.A.), verändert sich diese Aufgabe. Sie entfällt, wenn der Hoster spezialisiert auf das Hosting von BMS ist (bspw. Verbundzentrale), sie wird geringer, wenn der Hoster eher allgemein aufgestellt ist.\nKonfiguration/Parametrisierung: Teilweise über Administrator-Oberfläche möglich, teilweise nur über Konfigurationsdateien. Erfordert vor allem Kenntnisse der eigenen IT-Infrastruktur und der verwendeten Schnittstellen und Formate. Die Grenzen zwischen Konfiguration und Programmierung eigener Erweiterungen sind fließend. Zu beachten ist auch die Migration bestehender Daten in das neue System.\n\nNach Einrichtung werden BMS laufend erweitert. Fehler werden behoben und neue Funktionen kommen hinzu. Die Aktualisierung kann je nach Produkt agil in kleinen, häufigen Schritten oder in längeren Zeitabschnitten erfolgen.\n\n\nOpen Source\nWird ein System auf Open-Source-Basis eingesetzt, sollte eine Verständigung darüber erfolgen, ob und unter welchen Bedingungen lokale Anpassungen am System auch der Community zur Verfügung gestellt werden. Hierzu müssen die Lizenzbedingungen des Systems geprüft werden.\n\n\nLaufender Betrieb\nWährend des laufenden Betriebs ist es wichtig, sich über den aktuellen Betriebszustand des Systems ein klares Bild machen zu können. Dieser „Statusbericht“ kann sich über alle Ebenen des Systems ziehen: Wie viel Speicherplatz ist noch frei? Ist das System für alle Nutzer*innen erreichbar? Sind verbundene Systeme verfügbar und betriebsbereit? Je nach Betriebsmodell werden diese Fragestellungen durch klassisches IT-Monitoring abgedeckt, benötigen teilweise aber auch bibliotheksspezifische Lösungen.\n\nMonitoring\nMonitoring-Lösungen für den Betrieb von IT-Infrastrukturen sind beispielsweise Check_MK oder Prometheus. Diese Anwendungen bieten eine kontinuierliche Überwachung von Systemen anhand definierter Metriken und warnen die Administratoren aktiv, wenn definierte Werte bestimmte Grenzen überschreiben.\nDie Nutzung einer Monitoring-Lösung wird umso relevanter, je mehr Betriebsverantwortung für das BMS bei der Einrichtung liegt. Beim Cloud-BMS liegt diese beim Anbieter der Software, trotzdem sollte zumindest die reine Verfügbarkeit des Systems auch von der nutzenden Einrichtung überwacht werden können.\n\n\nNotfallbetrieb\nBzgl. der Themen Support, Wartung & IT-Sicherheit, als auch Fehlersuche und -vorbeugung, unterscheiden sich die Aufwände für die Einrichtung je nach gewähltem Betriebsmodell erheblich. Die zu nutzenden Prinzipien gelten hier jedoch genauso wie bei anderen zu wartenden Systemen in der IT-Welt.\nDazu gehören Maßnahmen zur Aufrechterhaltung des Bibliotheksbetriebs im Notfall. Dies kann ein temporärer Offlinebetrieb des Systems sein. In diesem Fall werden die Prozesse mit den Daten abgewickelt, die zum Zeitpunkt des Offline-Gangs im System vorhanden waren. Wenn das System wieder online geht, muss gewährleistet werden, dass Änderungen an den Daten aus der Offlinezeit nachvollzogen werden (Beispiele: Ausleihen, Erwerbungen, Rechnungsbearbeitung, Benutzerdatenänderungen). Im Idealfall erledigt das die genutzte Komponente oder das BMS selbst.\nBei lokalen Installationen sollte man je nach Größe der Einrichtung ebenfalls über ein Spiegelsystem des BMS nachdenken. Dieses wird parallel auf dem aktuellen Stand gehalten und kann einspringen, wenn das laufende BMS ausfällt.\nEs empfiehlt sich in jedem Fall, neben dem Einsatz eines Produktivsystems mindestens eine Test-Instanz und ggf. eine oder mehrere Entwicklungs-Instanzen des BMS zu betreiben. So können neue Funktionen schneller umgesetzt werden, ohne den laufenden Betrieb durch unerwartete Fehler zu gefährden.\nZu beachten ist weiterhin, die Nutzer*innen des BMS (intern als auch extern) bei Problemen zu informieren. Dabei sind vor allem von Bedeutung, welche Interaktionen nicht mehr möglich sind, ob es alternative Möglichkeiten für die Nutzer*innen gibt und wann das System voraussichtlich wieder zur Verfügung steht.\n\n\n\nIT-Sicherheit\nUm ein BMS vor den zunehmenden Angriffen durch böswillige Akteure (Hacking, Malware, Ransomware) abzusichern, können die folgenden Empfehlungen als Grundlage dienen (Breeding, Marshall 2022):\n\nDie Infrastruktur um das BMS herum sollte durch starke Sicherheitsvorkehrungen getragen werden.\nDie Gefahr kurzfristig entstehender Sicherheitslücken sollte nicht unterschätzt werden.\nCloud-basierte Systeme sollten aktiv überwacht und der Überblick behalten werden.\nAnbieter sollten aufgefordert werden, die Konzepte ihrer Sicherheitsvorkehrungen offenzulegen.\nGerade Administrator*innen sollten ihre Zugänge gesondert absichern.\nEs sollte sichergestellt werden, dass jede Software stets auf dem aktuellen Stand ist, sowohl auf den Arbeitsplatz-PCs als auch den Servern.\n\nAllgemein gelten die im Kapitel Sicherheit & Datenschutz beschriebenen Richtlinien, Maßnahmen und Empfehlungen sowie der Grundsatz: „Bleiben Sie wachsam, in Bezug auf ungewöhnliche Ereignisse auf Ihren IT-Systemen“.\n\n\nBackup und Rollback\nFür den Fall, dass der Betrieb eines BMS lokal erfolgt, ist es wichtig, dass sich die Einrichtung über Backup und Rollback der Software Gedanken macht. Da dies ein generelles Thema des Betriebs von IT-Systemen ist, wird im Folgenden auf die Spezifika für BMS eingegangen, und Themen wie das Backup von Servern lediglich angerissen.\nFolgende Aspekte sollten im Rahmen von BMS besondere Beachtung finden:\n\nDefinition von Backup-Zyklen: wie oft werden welche Daten in welchem Umfang auf welche Art gesichert? Es können hier durchaus verschiedene „Sicherungsaspekte“ mit unterschiedlichen Zyklen definiert werden.\nDefinition des Umfangs der Sicherung. Sollen die Daten komplett gesichert werden, sollen nur Veränderungen gesichert werden? Wichtig für diese Entscheidung ist die Frage, wie schnell ein System wiederhergestellt werden kann/soll.\nEin off-site-Backup sollte in die Überlegungen einbezogen werden, also ein kompletter Satz einer Sicherung, der außerhalb der Institution gelagert wird. Dabei ist der Datenschutz zu berücksichtigen, u. U. müssen die Sicherungsdaten daher verschlüsselt werden.\nSicherung von Daten, die bei rechtlichen Fragen von Relevanz sein können: Bsp. Rechnungen, Ausleihen, Mahnungen.\n\nGanz allgemein ist die Frage zu klären, wer Verantwortung für die Einrichtung, Durchführung und die regelmäßige Kontrolle der Sicherungen hat. Der letzte Punkt meint hierbei einerseits das Monitoring der erfolgreichen regelmäßigen Ausführung von Sicherungen, aber auch der Test der erstellten Sicherungen, etwa durch periodisches Einspielen auf einer Testinstanz des BMS.\n\n\nZusammenspiel Hard- und Software\nDas BMS steht in der IT-Landschaft einer Bibliothek im Regelfall nicht alleine, sondern kommuniziert mit anderen Hard- und Softwaresystemen (s. Kapitel Automatisierung). Hierzu gehören beispielhaft:\n\nLesegeräte: (Barcode-)Scanner und Chip-Lesegeräte für Benutzungsausweise und/oder Medien\nSelbstverbucher für die Ausleihe und/oder Rückgabe\nRückgabeautomaten, die ggf. auch eine automatische Vorsortierung von Medien übernehmen\nSicherungsgates zur Detektion nicht entliehener Medien an Ein- und Ausgängen\nDrucker zur Erstellung von Quittungen, Ausweisen, Labeln, usw.\n\nFür den Fall, dass die externen Systeme nicht lokal an einem Computer angeschlossen sind, sondern über das Netzwerk der Einrichtung angebunden sind, gibt es vielfach etablierte bibliothekarische Schnittstellen (APIs), etwa SIP, oder man setzt auf moderne, allgemeine API-Standards wie REST.\n\n\nDatenschutz, User-Tracking, Analytics\nInnerhalb der EU gilt seit 2018 die Datenschutz-Grundverordnung (DSGVO), nach der personenbezogene Daten grundsätzlich zu schützen sind.\nIm Kontext eines BMS sind die anfallenden personenbezogenen Daten etwa:\n\ndurch Nutzer*innen bei der Anmeldung angegebenen Daten für den Bibliothekszugang\nmit dem Benutzerkonto verbundene Ausleihvorgänge und Mahnhistorien\ndie Protokolle (Logs) über Online-Zugriffe auf das BMS (z.B. IP-Adresse, Seitenaufrufe)\n\nUm den Schutz personenbezogener Daten gewährleisten zu können, gibt es verschiedene Ansätze:\n\nVerschlüsselung: Daten werden auf verschlüsselten Servern gespeichert, ebenso ist die Übertragung Ende-zu-Ende verschlüsselt\nSeparierung: Personendaten werden getrennt von nicht-sensiblen Daten gehalten (siehe IDM)\nPseudonymisierung: Nutzer*innen-Daten werden mit Pseudonymen präpariert, sodass sie nicht mehr oder nur unter großem Aufwand den einzelnen Personen zuzuordnen sind\nAnonymisierung: Daten werden derart verändert, dass sie nicht rückverfolgbar sind (z.B. Maskierung IP-Adressen)\n\nDie Entscheidung zur Verschlüsselung und Separierung von Daten sollte bereits im Vorfeld des Betriebs eines BMS getroffen werden.\nDie Pseudonymisierung und Anonymisierung kann auch im Laufe der Erhebung der personenbezogenen Daten zur Anwendung kommen, sofern bestimmte Daten nicht mehr für einen konkreten Zweck erforderlich sind.\nLeider sind personenbezogene Daten für Bibliotheksstatistiken oft notwendig (siehe Kosten). In diesem Fall sollten ebenfalls pseudonymisierte oder anonymisierte Datensätze zur Grundlage genommen werden.\nWenn ein BMS durch einen externen Anbieter gehostet wird (siehe Betriebsmodelle für serverbasierte Software), muss Folgendes sichergestellt sein:\n\nDie Verschlüsselung der Datenübertragung (Ende-zu-Ende-Verschlüsselung)\nBetrieb und Steuerung der Server innerhalb der EU (DSGVO)\nDer Ausschluss von User-Tracking durch Ad-Tech (Werbe-Netzwerke)\nDer Abschluss eines Datenverarbeitungsvertrags im Auftrag\n\nIn der Kombination eines IDM mit einem cloudbasierten BMS außerhalb der EU wäre denkbar, die personenbezogene Daten dort in pseudonymisierter Form speichern zu lassen oder Personendaten von nicht sensiblen Daten zu trennen.\nFür alle personenbezogenen und personenbeziehbaren Daten sind Lösch- oder Anonymisierungsfristen festzulegen. Die Anonymisierungsfristen ergeben sich aus den Vorgaben der DSGVO und müssen betrieblichen und rechtlichen Aspekten genügen. So ergeben sich Fristen für die Speicherung von Daten über Gebühren (Entstehung, Bezahlung, …) aus den Landeshaushaltsordnungen oder anderen für die Einrichtung maßgeblichen Regelungen. Betriebliche Gründe für die Länge von Speicherfristen von personenbezogenen und personenbeziehbaren Daten können sich aus Fristen für Einsprüche ergeben.\nDie über die vergangenen Jahrzehnte geschehenen sukzessiven Aufkäufe kleinerer BMS-Service-Provider durch einige wenige große kommerziellen Bibliotheksdienstleister hat ganze Firmenkonglomerate entstehen lassen, die inzwischen den Bibliotheksmarkt dominieren. Einige von ihnen, die Dienste für wissenschaftliche Bibliotheken anbieten, wandeln sich in den letzten Jahren zu Data-Analytics-Konzernen. In diesem Zuge präparieren sie ihre cloud-basierten BMS-Lösungen mit Trackern, die Verhaltensprofile über die Nutzer*innen erstellen. Durch die ebenfalls seitens der Anbieter gestellten Zugangsauthentifizierungssysteme wird versucht, zusätzlich eine möglichst hohe Personalisierung bei der Erstellung einzelner Profile zu erreichen. Die dabei entstehenden Datenflüsse werden für gewöhnlich nicht transparent gemacht (Siems 2022). Der Einsatz solcher Analytics-Technologien unterminiert die Integrität konventioneller IDM-Systeme und tangiert somit nicht nur datenschutzrechtliche Belange, sondern auch die IT-Sicherheit. Idealerweise sollte bereits vor der Anschaffung einer BMS-Lösung abgeklärt werden, ob solche Analytics-Technologien eingesetzt werden. Im Zweifelsfall sollte immer der*die lokale Datenschutzbeauftragte oder IT-Sicherheitsbeauftragte hinzugezogen werden.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "bibliotheksmanagementsysteme.html#zusammenfassung-und-ausblick",
    "href": "bibliotheksmanagementsysteme.html#zusammenfassung-und-ausblick",
    "title": "Bibliotheksmanagementsysteme",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nEin BMS ist im Normalfall kein statisches System - vielmehr muss es aufgrund der sich verändernden Bedürfnisse einer Bibliothek und deren Nutzer*innen sowie sich wandelnder Technologien und technischer Anforderungen stetig angepasst werden.\nDer Import, Export oder auch die Zusammenführung von Daten erfordert klar definierte Metadaten und Schnittstellen für den freien Austausch aus gut nachnutzbaren Quellsystemen. Dies ist vor allem erforderlich bei der aktuell stärkeren Entwicklung hin zu Open Data und öffentlicher Datennutzung. Die Integration und Interaktion mit anderen Informationssystemen nimmt also zu. Vor allem herkömmliche BMS der zweiten Generation kommen hier schnell an ihre Grenzen.\n\n\n\n\nBorgman, Christine L. 1997. „From Acting Locally to Thinking Globally: A Brief History of Library Automation“. The Library Quarterly: Information, Community, Policy 67 (3): 215–49. https://www.jstor.org/stable/40039721.\n\n\nBreeding, Marshall. 2022. „How to Secure Library Systems From Malware, Ransomware, and Other Cyberthreats“. 2022. https://www.infotoday.com/cilmag/jan22/Breeding--How-to-Secure-Library-Systems-From-Malware-Ransomware-and-Other-Cyberthreats.shtml.\n\n\nBreeding, Marshall. o. J. „Library Technology Industry Mergers and Acquisitions. Library Technology Guides“. Zugegriffen 28. April 2022. http://librarytechnology.org/mergers/.\n\n\nKluge, Matthias. 2022. Anbieter von Bibliothekssoftware. Landesfachstelle für das öffentliche Bibliothekswesen. https://www.oebib.de/bau-einrichtung-it/it-und-internet/bibliothekssoftware.\n\n\nMatthews, Joseph R., und Carson Block. 2020. Library information systems. Second edition. Library and information science text series. Santa Barbara, California: Libraries Unlimited.\n\n\nSchweitzer, Roswitha. 2016. „Anforderungen an ein Bibliothekssystem der neuen Generation - der Kriterienkatalog von hbz und VZG“. Köln. https://docplayer.org/61296444-Anforderungen-an-ein-bibliothekssystem-der-neuen-generation.html.\n\n\nSiems, Renke. 2022. „Das Lesen der Anderen: Die Auswirkungen von User Tracking auf Bibliotheken“. o-bib. Das offene Bibliotheksjournal / Herausgeber VDB 9 (1): 1–25. https://doi.org/10.5282/o-bib/5797.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bibliotheksmanagementsysteme</span>"
    ]
  },
  {
    "objectID": "discovery.html",
    "href": "discovery.html",
    "title": "Discovery-Systeme",
    "section": "",
    "text": "Einleitung\nAls Discovery-Systeme werden Rechercheplattformen bezeichnet, die möglichst verfügbaren Ressourcen über einen einheitlichen Zugang nutzbar machen. Insbesondere beschränken sich die recherchierbaren Medien nicht nur auf den lokalen Bibliotheksbestand. Die Benutzung und der Funktionsumfang von Discovery-Systemen orientieren sich dabei an gängigen Suchmaschinen und Verzeichnissen im Web.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "discovery.html#ursprung-und-motivation-von-discovery-systemen",
    "href": "discovery.html#ursprung-und-motivation-von-discovery-systemen",
    "title": "Discovery-Systeme",
    "section": "Ursprung und Motivation von Discovery-Systemen",
    "text": "Ursprung und Motivation von Discovery-Systemen\nDie Entstehung von Discovery-Systemen zu Beginn der 2000er Jahre hatte mehrere Gründe: Bibliothekarische Recherchesysteme spielten im Informationsverhalten insbesondere von studentischen Nutzer*innen nur noch eine untergeordnete Rolle. Parallel zeichnete sich ab, dass die dritte Generation der Bibliotheksmanagementsysteme bezüglich ihrer OPAC-Module stagnierte, vornehmlich in Bezug auf das Design, aber auch hinsichtlich ihrer Funktionalitäten. Außerdem wurde Suchmaschinen-Technologie als Open Source-Software verfügbar, sodass technisch aufgeschlossene Einrichtungen eigene Experimente mit der Indexierung bibliografischer Daten begannen.\nZum gegenwärtigen Zeitpunkt sind Discovery-Systeme in wissenschaftlichen und zunehmend auch in öffentlichen Bibliotheken verbreitet. Es gibt eine Reihe von Produkten kommerzieller Anbieter*innen und einige Open Source-Projekte. Discovery-Systeme können von Bibliotheken selbst oder durch Hosting-Anbieter wie Verbundzentralen, Hersteller*innen und kommerzielle Dienstleister*innen betrieben werden. Die Hersteller*innen kommerzieller Bibliotheksmanagementsysteme der neueren Generation bieten Discovery-Systeme an, die besonders gut mit dem BMS der gleichen Hersteller*innen zusammenarbeiten.\nWenn Bibliotheken neben dem Bestandskatalog andere Repositorien betreiben (Dokumenten-Server, Digitalisate-Server, Forschungsdaten-Server etc.), ist die Einführung eines Discovery-Systems eine Möglichkeit, diese Datenbestände gemeinsam zugänglich zu machen.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "discovery.html#bestandteile-von-discovery-systemen",
    "href": "discovery.html#bestandteile-von-discovery-systemen",
    "title": "Discovery-Systeme",
    "section": "Bestandteile von Discovery-Systemen",
    "text": "Bestandteile von Discovery-Systemen\n\nKomponenten\nEin Discovery-System umfasst verschiedene Komponenten. Dazu gehören\n\neine Benutzungs- oder Rechercheoberfläche (Frontend bzw. User Interface),\nder Suchindex (ein oder mehrere Quell-Indizes)\nETL-Prozesse und\ndie Konfiguration der Rechercheoberfläche.\n\nAngebunden ist häufig auch eine Komponente zur Authentifizierung und Autorisierung.\n\nFrontend\nDie Rechercheoberfläche (Frontend bzw. User Interface) umfasst typischerweise eine Startseite, eine einfache und eine erweiterte Suche, eine Trefferliste mit Facetten sowie eine Detailseite. Mitunter sind auf der Startseite auch thematische Sucheinstiege verfügbar, z.B. ein Browsing über eine Klassifikation oder Sammlungen. Außerdem gibt es meistens einen persönlichen Bereich, in dem auf das eigene Bibliothekskonto im BMS zugegriffen und gespeicherte Suchanfragen und Literaturlisten verwaltet werden können.\nDie Gestaltungsmöglichkeiten für Design und Layout des User Interface reichen von einer einfachen optischen Anpassung bei Schriften, Farben und Logos bis hin zu größeren Veränderungen im Seitenaufbau, je nachdem, welcher Art das eingesetzte System ist (Eigenentwicklung auf Open Source-Basis, gehostetes kommerzielles System o.Ä.).\n\n\nSuchindex\nZentraler Bestandteil eines Discovery-Systems sind auf Grundlage etablierter Suchmaschinentechnologie wie Apache Solr und Elasticsearch entwickelte Suchindizes.\nDer Index eines Discovery-Systems enthält Metadaten und ggf. damit verknüpfte Daten wie Volltexte, Inhaltsverzeichnisse und Übersetzungen. Im Suchindex kommen also Daten unterschiedlicher Art und Herkunft zusammen. Die Daten können in einzelnen Kollektionen aufbereitet sein, z.B. nach Bestandsdaten von (Teil-)Bibliotheken oder Verbünden, Daten einzelner Verlage, Metadaten aus Repositorien etc. Es besteht die Möglichkeit, den Suchraum des Discovery-Systems individuell zu konfigurieren. Der Aufbau eines Index in Eigenregie ist bei entsprechenden Prozesskenntnissen und personellen Kapazitäten möglich und schafft Freiheiten zur Berücksichtigung eigener Datenkollektionen.\nEinige Discovery-Systeme können Suchanfragen gleichzeitig an mehrere Suchindizes senden und die Treffer aus den unterschiedlichen Suchindizes in einer Gesamtliste zusammenführen. Dies setzt allerdings eine Koordination der genutzten Suchindizes voraus. Dieser Aufbau ermöglicht es Bibliotheken verschiedene Datenquellen in ihrem Discovery-System gemeinsam zugänglich zu machen. Teils werden die Quellen selbst ausgewertet (z.B. Harvesting der Daten des eigenen Katalogs, relevanter Repositorien etc.), teils werden dafür andere freie oder kommerzielle Suchindizes (K10plus-Zentral, Gemeinsamer Verbünde Index, EBSCO-Discovery-Index, ExLibris Central Discovery Index etc.) genutzt.\nDie Daten, die in Suchindizes aufgenommen werden sollen, werden im Rahmen eines ETL-Prozess aus verschiedenen Datenquellen gesammelt, konvertiert und dann in den Suchindex geladen. Für jede Quelle muss dieser Prozess entsprechend eingerichtet und für Aktualisierungen regelmäßig ausgeführt werden.\nBei den ETL-Prozessen werden die Daten aus den verschiedenen Datenquellen transformiert. Dabei werden z.B. die MARC-Struktur mit Feldern, Indikatoren und Unterfeldern in eine einfachere Feldstruktur überführt. Die Daten werden entsprechend den verschiedenen Such- und Navigationsbedürfnissen in unterschiedliche Indexfelder überführt. Ein Datum (z.B. Name des*der Autor*in) kann für verschiedene Suchtypen unterschiedlich aufbereitet und mehrfach im Index gespeichert werden. Bei der Aufbereitung werden die Daten analysiert und etablierte Verfahren zur Relevanzberechnung für die Sortierung innerhalb der Trefferliste eingesetzt.\nFür die Bildung von Facetten aus den Einträgen der Trefferliste werden spezielle Daten ermittelt. Diese Facetten-Daten sind technisch gesehen Suchbegriffe und dienen der nachträglichen Verfeinerung der Trefferliste. Beispiele für Facetten, die zur Einschränkung genutzt werden, sind Namen von Autor*innen, Schlagwörter, Medienarten, Standorte physischer Medien, die Kennzeichnung von Open Access-Material und vieles mehr).\n\n\n\nFunktionen\nEin Discovery-System ist mehr als ein reines Nachweissystem. Der Funktionsumfang umfasst daher auch mehr als die reine Recherche. Der Anspruch an ein Discovery-System, alle Informationen zu Medien an einer Stelle zu bündeln, sollte prinzipiell auch alle Dienstleistungen zu diesen Medien umfassen. Daher sollten auch Informationen zur Bereitstellung von Literatur enthalten, weitere Dienste integriert und eine Personalisierung möglich sein.\n\nRecherche\nHauptfunktionen des Discovery-Systems sind die Recherche, die Anzeige von Metadaten und die Hinführung zur Nutzung der Medien. Im Einzelnen geht es um folgende Funktionalitäten:\n\neinfache Suche ohne Spezifizierung eines Suchfeldes\nSuche in Feldern der Metadaten (Titel, ISBN, Schlagworte)\nerweiterte Suche mit Möglichkeiten der Verknüpfung von Suchen in verschiedenen Feldern\nNavigation in Trefferlisten über Facetten und Sortierung\nDetailanzeige einzelner Treffer\nExport von Literaturangaben\n\nDie Suche in Discovery-Systemen nutzt in der Regel verschiedene Funktionen der Suchmaschinentechnologie, um einen eingegebenen Suchbegriff gegen den Index abzuprüfen. Daher liefern Discovery-Systeme mit dem Suchparadigma „beste Treffer“ statt „exakte Treffer“ mehr Suchergebnisse als Bibliothekskataloge (Steilen 2012). Sie nutzen außerdem Algorithmen für die Relevanzsortierung (Ranking), um die Trefferlisten möglichst nutzungsorientiert aufzubereiten. Die Sortierungsalgorithmen sorgen bei Übereinstimmungen von Suchbegriff und Indexeintrag in definierten Feldern (z. B. Titel, Schlagwort) für eine Bevorzugung. Anders als bei Web-Suchmaschinen gehen Popularitätsdaten wie die Anzahl von Ausleihen, Aufrufen und Zitationen in der Regel nicht in das Ranking ein.\nZu den Funktionalitäten für die Suchunterstützung gehören auch die Autovervollständigung sowie die Vorschlagsfunktion von Suchbegriffen. In beiden Fällen wird der Suchindex in Echtzeit geprüft. Es gehört zu den zentralen Zielen von Discovery-Systemen, Null-Treffer-Meldungen zu vermeiden.\nFacetten sind ebenfalls eine für Suchmaschinen typische Funktion und dienen der Eingrenzung von Treffermengen. Hierfür werden einzelne Metadatenfelder wie z. B. Schlagwörter, Namen von Verfasser*innen oder Dokumenttypen in Bezug auf eine Suchanfrage ausgewertet und nach Vorkommenshäufigkeit sortiert. Den Facetten wird eine wichtige Rolle beim entdeckenden Suchen zugesprochen. Zur Präsentation der Facetten in der Rechercheoberfläche gibt es verschiedene Möglichkeiten (siehe Abbildung 8.1). Die Auswahl der angebotenen Facetten muss jedoch gut vorbereitet werden. Fehlen die entsprechenden Metadaten bei bestimmten Titeln, können durch Facettierung auch Treffer verloren gehen.\nDie Weiterverwendung der Suchergebnisse wird durch verschiedene Exportmöglichkeiten unterstützt. In der Regel lassen sich Angaben per Mail verschicken, ausdrucken oder in unterschiedlichen Formaten und Zitierstilen herunterladen.\n\n\n\n\n\n\nAbbildung 8.1: Beispiel eines Rechercheergebnisses in einem Discovery-Interface (Quelle)\n\n\n\n\n\nBereitstellungsdienste\nDie Evaluationen früher Discovery-Systeme haben bereits gezeigt, dass Informationen darüber, ob und wie ein gefundenes Medium zugänglich ist, von zentraler Bedeutung sind. Diese Bereitstellungsdienste, auch Delivery-Funktionen genannt, umfassen für physische und digitale Medien jeweils unterschiedliche Aspekte.\nBereitstellungsdienste für physische Medien umfassen z. B.:\n\nNachweise von Standorten, Ausleihbarkeit und aktuellem Ausleihstatus\nVerlinkung zu Verbundkatalogen mit Fernleihmöglichkeiten\nVerlinkung zu Fernleihe und Dokumentlieferdiensten\nMöglichkeit zur Anfrage nach einer Digitalisierung oder Bereitstellung in einem Semesterapparat\nMöglichkeit zur Abgabe eines Anschaffungsvorschlags\n\nBereitstellungsdienste für digitale Medien umfassen z. B.:\n\nidealerweise eine auf das jeweilige Nutzungsszenario angepasste Zugangs-URL\nweitere Zugangs-URLs\nHinweise zur Nutzung elektronischer Medien, z.B. zur Zugänglichkeit über VPN, notwendigen Readern, Digital Rights Management (DRM) etc.\n\nDie Verfügbarkeit und Entleihbarkeit von physischen Medien, die der Bibliothek gehören, werden über eine sogenannte Verfügbarkeitsrecherche, die das Discovery-System im Hintergrund ausführt, ermittelt und angezeigt. Diese Abfragen werden mittels Schnittstellen zu den Ausleihmodulen der BMS durchgeführt. Diese Schnittstellen können proprietär oder offen sein. Beispiele für herstellerunabhängige Schnittstellen sind die Patrons Account Information API (PAIA) als offene Schnittstelle und das Session Initiation Protocol (SIP2) als intern genutzter Standard oder das NISO Circulation Interchange Protocol (NCIP). Verschiedene Discovery-Systeme unterstützen diese oder andere Schnittstellen zum Ausleihsystem in Form von sogenannten Treibern – beispielsweise unterstützt VuFind die Anbindung an FOLIO durch einen eigenen FOLIO-Treiber.\nBei den digitalen Medien ist die größte Herausforderung, den jeweils besten von in der Regel mehreren Zugangslinks für ein Medium zu identifizieren und zur Anzeige zu bringen. Zur Ermittlung des besten Zugangslinks sind in der Regel mehrere Prüfschritte erforderlich. Idealerweise sind solche Prüfschritte konfigurierbar, allerdings ist diese Funktion oftmals kein integraler Bestandteil von Discovery-Systemen, sondern ein eigener Dienst. Ein Beispiel für einen solchen separaten Dienst ist der Webdienst DAIA+ (Keßler 2018). Eine andere Möglichkeit ist der Einsatz sogenannter Link Resolver. Beim Link Resolving wird über die Metadaten ein Hyperlink zu Diensten der Bibliothek ermittelt. Es wird vorrangig bei solchen Medien genutzt, die nicht aus dem BMS der Bibliothek und E-Ressourcen stammen. Ein Verfahren für das Link-Resolving ist Open-URL (NISO-Standard Z39.88).\n\n\n\nAnreicherungsdienste\nDie Ergänzung bibliotheksseitig erstellter Metadaten mit weiteren Informationen erfolgte bereits in klassischen OPACs. Beispiele sind gescannte Inhaltsverzeichnisse, Links auf Wikipedia-Artikel oder die Integration von Buchcovern.\nZu den am häufigsten genutzten Anreicherungsdiensten gehören:\n\nCover-Anzeigen\nkontextabhängige Infoboxen mit Informationen aus Nachschlagewerken,\n\nB. Autor*innenenportraits via Wikidata oder GND, Informationen aus Nachschlagewerken wie Munzinger\n\nEmpfehlungsdienste mit Hinweisen auf Literatur zum selben Thema (z.B. BibTip, bX)\nVisualisierungen von Buchstandorten über Gebäudeinformationssysteme (z.B. Mapongo, V:Scout)\nIntegration mit weiteren Diensten, z.B. der Leseförderungs-App Antolin\n\nGrundsätzlich erlaubt die Systemarchitektur von Discovery-Systemen die Integration von diesen und anderen Diensten über einschlägige Schnittstellen, sodass sich über die gelisteten Dienste noch zahlreiche weitere Möglichkeiten ergeben.\n\nPersonalisierung\nDiscovery-Systeme erlauben in der Regel eine Anmeldung in einem persönlichen Bereich, der folgende Funktionalitäten umfassen kann:\n\nEinsicht in das Bibliothekskonto einschließlich der Möglichkeit zum Vormerken und Verlängern\nSpeicherung von Suchanfragen\nSpeicherung von Literaturlisten\nAlerting-Dienste\n\nLiteraturlisten können alternativ dazu auch sitzungsbasiert gespeichert werden. Dauerhaft gespeicherte Listen lassen sich auch veröffentlichen und damit allgemein zugänglich machen, was auch die Präsentation von Auswahllisten oder Semesterapparaten erlaubt.\nHäufig können auch Suchanfragen gespeichert werden. Die Einrichtung von Alerting-Diensten hilft den Nutzer*innen, sich mit wenig Aufwand über neue Titel informieren zu lassen. Alerting-Dienste beinhalten das regelmäßige (automatisierte) Absetzen einer Suchanfrage und das Versenden von Informationen, wenn die Suchanfrage veränderte Trefferlisten (in der Regel: neue Titel) liefert.\n\n\nThematische Sucheinstiege\nWie beschrieben bieten Trefferlisten mit Facetten und Empfehlungen zwar durchaus die Möglichkeit, sich eine Treffermenge zu erschließen. Allerdings fehlt Discovery-Systemen genau wie OPACs häufig die Möglichkeit, eine systematische Suche durchzuführen. Teilweise wird ein Browsing durch die klassifikatorische Inhaltserschließung angeboten, jedoch fehlen vielen Datensätzen entsprechende Daten und das Browsing bezieht sich dadurch jeweils nur auf Teilmenge des Suchraums.\nAus diesem Grund werden derzeit verschiedene Ansätze erprobt, um eine thematische Suche zu ermöglichen. Hierzu zählen u.a. folgende Projekte und Dienste:\n\nein Nachbau der Browsing-Funktion an physischen Bücherregalen, z. B. bei dem kommerziellen Dienst Blended Shelf\ndie Nutzung von Normdaten zur Erstellung von Übersichtsseiten, z. B. im Katalog des Deutschen Literaturarchivs Marbach\ndie Visualisierung von Treffermengen und den darin enthaltenen Zusammenhängen, wie zum Beispiel bei Open Knowledge Maps, in einer prototypischen Installation der SLUB Dresden oder mit dem kommerziellen Dienst Yewno.\n\nDiese Projekte und Dienste sind jedoch entweder noch relativ neu oder wenig verbreitet und nicht oder nur mit Aufwänden nachnutzbar. Im Rahmen einer strategischen Planung für den Einsatz eines Discovery-Systems muss daher abgewogen werden, ob und wie ein thematischer Sucheinstieg umgesetzt werden soll, zumal für eine Darstellung im Sinne einer optimalen User Experience jeweils auch erhebliche Design-Aufwände entstehen.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "discovery.html#aufbau-und-betrieb-eines-discovery-systems",
    "href": "discovery.html#aufbau-und-betrieb-eines-discovery-systems",
    "title": "Discovery-Systeme",
    "section": "Aufbau und Betrieb eines Discovery-Systems",
    "text": "Aufbau und Betrieb eines Discovery-Systems\n\nBetriebsmodelle\nDer Betrieb eines Discovery-Systems stellt vergleichbare Anforderungen und unterliegt ähnlichen Rahmenbedingungen wie beim Betrieb eines BMS.\nIm Inhouse-Betrieb werden alle Komponenten selbst durch die Bibliothek betrieben. Damit sind in diesem Szenario die weitestgehenden Anpassungen möglich. Ein vollständiger Inhouse-Betrieb erfolgt aufgrund der benötigten Ressourcen allerdings meist nur bei kleinen oder sehr speziellen Datenbeständen (z. B. durch die Fachinformationsdienste) oder in sehr großen Einrichtungen. Oft trifft man stattdessen auf hybride Lösungen, in denen neben einem vergleichsweise kleinen eigenen Index ein bestehender kommerzieller oder nicht-kommerzieller Index genutzt wird.\nIn einem Hosting-Betrieb wird die gesamte Infrastruktur durch eine*n Dienstleister*in bereitgestellt. Dabei erfolgt die Indexierung in der Regel durch einheitliche Indexierungsverfahren, die von allen teilnehmenden Bibliotheken gemeinsam genutzt werden. Bei diesen Lösungen werden alle Daten in einen einheitlich aufgebauten, in einer Cloud gehosteten Index eingespielt. Die Frontends sind nur eingeschränkt individualisierbar und lassen sich ausschließlich durch Konfigurationen parametrisieren. Zusatzfunktionen lassen sich über Schnittstellen anbinden. Wesentlicher Vorteil dieser Systeme ist ein vergleichsweise geringer Wartungsaufwand, ihre gute Skalierbarkeit und durch standardisierte Workflows ihre hohe Betriebssicherheit. Als Hoster*innen von Discovery-Systemen treten Bibliotheken, Verbünde und kommerzielle Anbieter auf.\nEin Spezialfall des Hostings ist die Nutzung von Cloud-Services externer Anbieter*innen für den Betrieb von BMS und Discovery-Systemen. Mehrere Hersteller*innen von BMS und Discovery-Systemen sind gleichzeitig Betreiber*innen solcher Cloud-Lösungen. In diesen Fällen wird die Software nicht mehr lizenziert, sondern über eine jährliche Pauschale werden Nutzung, Update und Betrieb des jeweiligen Software-Systems abgegolten.\nBeim Hosting oder bei der Nutzung von Software, die in der Cloud betrieben wird, erfolgt aus datenschutzrechtlicher Sicht eine „Datenverarbeitung im Auftrag“. Die Verantwortung für Datenschutz und Datensicherheit verbleibt damit bei der Bibliothek als Auftraggeberin.\n\n\nMarktsituation\nDie ersten Discovery-Systeme haben Bibliotheken selbst entwickelt, im deutschsprachigen Raum z.B. E-LIB an der Staats- und Universitätsbibliothek Bremen oder beluga an der Staats- und Universitätsbibliothek Hamburg. Seit Ende der 2000er Jahre gibt es auch kommerzielle Systeme am Markt, entweder als Teil von BMS der neuesten Generation oder auch als individuell lizenzierbare Systeme. Die Open Source-Lösung VuFind ermöglicht es, verschiedene Suchindizes unter einer Oberfläche nutzbar zu machen, sodass es eine relativ große Vielfalt von Nutzungsszenarien gibt.\n\nKommerzielle Komplettsysteme\nIm Wesentlichen gibt es zwei Anbieter*innen von Komplettsystemen für Discovery-Systeme\n\nExLibris mit Primo und Summon\nEBSCO mit Ebsco Discovery-Service\n\nDiese Systeme bieten eine fertige Lösung, in die lokale Bestandsdaten und weitere lokale Metadaten integriert werden können. Für die Nutzung fallen jährliche Lizenzgebühren sowie einmalige Implementierungskosten an. Beide Systeme sind weit verbreitet und ausschließlich über die Cloud der jeweiligen Hersteller*innen nutzbar. Diese sorgen für eine hohe Verfügbarkeit und regelmäßige Softwarepflege. Individuell zu prüfen sind vor einem möglichen Einsatz insbesondere folgende Fragen:\n\nEinbindung von Verfügbarkeitsinformationen\nDatenschutzrechtliche Fragen (Ort des Hostings, Verfahrensbeschreibungen)\nDatenhoheit (siehe Digitale Souveränität)\n\nDie Indizes dieser Systeme können separat lizenziert und beispielsweise an VuFind-Systeme angebunden werden.\nEin weiteres kommerzielles Discovery-System ist WorldCat Discovery, das allerdings die Nutzung von WorldCat als Suchindex voraussetzt.\n\n\nOpen Source-Systeme\nUnter den von Bibliotheken selbst entwickelten Discovery-Systemen sind international VuFind und Blacklight am weitesten verbreitet.\nVuFind basiert auf PHP und lässt sich an verschiedene kommerzielle und frei verfügbare Komponenten wie Indizes und Bibliotheksmanagementsysteme anbinden. In den deutschsprachigen Ländern besteht eine lebendige Anwender*innengemeinschaft, die sich regelmäßig trifft. Mit Qcovery und finc gibt es zwei Sub-Communities für wissenschaftliche Bibliotheken, die sich die Aufgaben der Pflege und Weiterentwicklung der Software unter sich aufteilen.\nBlacklight ist hauptsächlich im angloamerikanischen Raum verbreitet, aber auch bei Europeana Einsatz. Die Software basiert auf Ruby on Rails.\nDas von der VZG entwickelte System Lukida spielt vor allem im Rahmen des Index K10plus-Zentral eine Rolle und wird primär als SaaS angeboten.\n\n\nIndizes\nNeben den kommerziellen Anbieter*innen bieten im Bereich wissenschaftlicher Bibliotheken einige Verbundzentralen auf Suchmaschinen-Technologie basierende Indizes an, teilweise für die teilnehmenden Bibliotheken, teilweise auch darüber hinaus für die nicht-kommerzielle Nutzung. Diese frei verfügbaren Indizes sind für Bibliotheken, die ihre Bestandsdaten an einen Verbund liefern, eine hervorragende Möglichkeit, um relativ kostengünstig an ein Discovery-System zu kommen, da die Erstellung eines eigenen Index mit hohen Investitionen verbunden ist. Metadaten-Kollektionen enthalten z. B. der ALBERT-Index des Kooperativen Bibliotheksverbundes Berlin-Brandenburg sowie der Gemeinsame Verbündeindex für Bestandsdaten aus allen wissenschaftlichen sowie vielen Spezial- und öffentlichen Bibliotheken.\n\n\n\nAuswahl- und Entscheidungsprozesse\nSofern ein Discovery-System nicht Teil des BMS ist, ist die Einführung immer mit beträchtlichen Aufwänden verbunden, die aus initialen Kosten für die Implementierung und laufenden Kosten für die Pflege bestehen. Diese Kosten fallen unabhängig davon an, ob es sich um ein kommerzielles oder ein Open Source-System handelt. Sie richten sich nach unterschiedlichen Kriterien und dürften im Bereich der initialen Kosten im höheren vierstelligen Bereich liegen. Grundsätzlich sind die Entscheidungsprozesse bei Auswahlentscheidungen mit denen für ein BMS vergleichbar (vgl. Abschnitt Marktanalyse und Beschaffung).\nAllerdings müssen die strategischen Vorteile eines Discovery-Systems sehr deutlich und auf den lokalen Bedarf hin herausgearbeitet werden. Es hat sich als hilfreich erwiesen, dass Bibliotheken klar definieren, an welche Zielgruppen sich ihr Discovery-System richtet und welche Aufgaben es erfüllen soll. Es sollte auch geklärt werden, ob der klassische OPAC nach Einführung eines Discovery-Systems überhaupt weiter angeboten werden soll.\nAuch der Zuschnitt der Suchräume sollte genau bedacht werden, vor allem wenn über lokale Bestandsdaten hinaus eigene Metadatenkollektionen (z.B. aus institutionellen Repositorien) integriert und durch eigene Suchfilter angesprochen werden sollen. Generell kann davon ausgegangen werden, dass auf die initiale Implementierung eines Discovery-Systems eine längere, oft mehrjährige Phase der Optimierung folgt, die idealerweise konsequent auf die Usability und User Experience der Hauptzielgruppen ausgerichtet ist (vgl. Kapitel Anforderungen an die IT-Entwicklung).\nDie grundsätzliche Entscheidung für ein Discovery-System beinhaltet auch einen Wechsel der Suchparadigmen. Die Einführung eines Discovery-Systems kann nur dann sinnvoll erfolgen, wenn die Abkehr der Dualität von Bestandsverzeichnis und Bibliografie sowie den traditionellen Suchparadigmen strategisch erwünscht ist und von entsprechenden Schulungen für das Bibliothekspersonal begleitet wird.\nWenn ein Discovery-System im Hosting genutzt werden soll, relativieren sich obige Aussagen zur Flexibilität, da die Hosts in diesem Fall die Möglichkeiten festlegen, die durch die Bibliotheken genutzt werden können. In umgekehrter Weise verschieben sich die obigen Aussagen zur Verantwortung für Betriebssicherheit und Verfügbarkeit.\n\n\nMonitoring und Weiterentwicklung\nWie jedes IT-System brauchen Discovery-Systeme kontinuierliches technisches Monitoring (vgl. Kapitel Management von IT-Diensten), ebenso wie konzeptionelle Betreuung. Anders als der klassische Bibliothekskatalog sind Discovery-Systeme angetreten, um sich konsequent nach dem Informationsverhalten der Nutzer*innen auszurichten. Daraus ergibt sich, dass sowohl die Implementierung als auch die weitere Entwicklung möglichst kleinschrittig und unter Einbeziehung von Analysen der Nutzung erfolgen sollten. Neben Methoden der Usability-Forschung (siehe Kapitel Wie beziehen wir unsere Nutzer*innen ein?) bietet sich als niedrigschwellige Methode vor allem die Analyse von Logfiles an. So kann z. B. mit der Software Matomo, auch unter Berücksichtigung von datenschutzrechtlichen Vorschriften, ermittelt werden, welche Anfragen an ein System gestellt werden.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "discovery.html#vergleich-mit-klassischen-bibliothekskatalogen",
    "href": "discovery.html#vergleich-mit-klassischen-bibliothekskatalogen",
    "title": "Discovery-Systeme",
    "section": "Vergleich mit klassischen Bibliothekskatalogen",
    "text": "Vergleich mit klassischen Bibliothekskatalogen\nDa Discovery-Systeme die Metadaten und Volltexte anders als die klassischen OPACs aufbereiten, sind Suchstrategien und -ergebnisse in beiden Systemen unterschiedlich.\nDiscovery-Systeme richten sich in der Regel an Benutzer*innen, die den Umgang mit bibliografischen Recherchesystemen wie Katalogen und Fachbibliografien nicht gewohnt sind und die mit den Nutzungsmustern bedient werden sollen, die sie auch aus dem Web gewohnt sind.\nNeben der Recherche nach bibliografischen Informationen sollen Discovery-Systeme auch den Zugriff bzw. die Bereitstellung von Medien unterstützen. Dieser auch als Delivery bezeichnete Prozess hat sich bereits in der frühen Phase der Discovery-Systeme als zentrales Element aus Sicht der Nutzer*innen herausgestellt. Die Anbindung an Ausleihsysteme und Link Resolver ist daher ein wichtiges Qualitätskriterium.\nNeuere BMS wie FOLIO und Alma enthalten zum Teil gar keinen klassischen OPAC mehr. Mit diesen Systemen muss daher immer ein zusätzliches Discovery-System eingesetzt werden.\nEin Vorteil von Discovery-System gegenüber OPACs liegt in der Auffindbarkeit von E-Ressourcen. Viele Bibliotheken erschließen insbesondere im Open Access zugängliche E-Ressourcen nicht in vollem Umfang in ihrem BMS. Daher sind im OPAC die E-Ressourcen nicht oder nur eingeschränkt auffindbar. Wenn die Bibliothek ein Discovery-System betreibt, können Metadaten zu E-Ressourcen über einen ETL-Prozess in den Index des Discovery-Systems geladen werden. Voraussetzung dafür ist, dass den Metadaten mittels Electronic Resource Management (ERM) entsprechende Nutzungslizenzen zugeordnet sind.\n\n\n\n\n\n\n\nOPAC/Katalog\nDiscovery-System\n\n\n\n\nSuchraum\nnur lokaler Bestand, nur selbständige Werke\nlokaler Bestand, aber auch Verbunddaten, bibliografische Daten, Volltexte etc.\n\n\nSuchprinzip\nexakte Suche, feldbasierte Suche mit Boolescher Logik\nbest match/natürlichsprachliche Suche\n\n\nSuchunterstützung\neher wenig\nAutovervollständigung, Suchvorschläge, Facetten\n\n\nSortierung\nstandardmäßig nach Aktualität\nstandardmäßig nach Relevanz\n\n\nMehrwertdienste\nBuchcover, Listen, Exportformate\nBuchcover, Listen, Exportformate, Stöbern/Entdecken\n\n\nMetadatenmodell\nbibliothekarisches Schema mit Hierarchien und Verweisen\n„flache Version“ eines bibliothekarischen Schemas\n\n\n\n\n\nTabelle 8.1: Vergleich typischer Eigenschaften von OPAC/Katalog und Discovery-System",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "discovery.html#grenzen-und-alternativen",
    "href": "discovery.html#grenzen-und-alternativen",
    "title": "Discovery-Systeme",
    "section": "Grenzen und Alternativen",
    "text": "Grenzen und Alternativen\nDiscovery-Systeme sind in der Regel nur einer von vielen Bausteinen in der Prozesskette der Recherche, Bewertung und Beschaffung von Literatur und spielen an unterschiedlichen Stellen eine Rolle. Sie helfen dabei, Literatur zu entdecken und Zugangswege zu ermitteln und brechen die traditionelle Grenze zwischen Katalog und Bibliografien durch einen zentralen Sucheinstieg auf. Trotz dieser Stärken können die Systeme nachgewiesene Medien nur eingeschränkt kontextualisieren und bewerten und bleiben in der Praxis oft hinter den Erwartungen zurück (Christensen 2022). Je nach Anwendung spielen daher alternative Systeme weiterhin eine Rolle:\n\nKomplexe bibliografische Angaben, zum Beispiel zum Erscheinungsverlauf von Zeitschriften oder mehrbändigen Werken, oder die Suche nach Signaturen lassen sich möglicherweise schneller über herkömmliche bibliothekarische Instrumente beziehungsweise Spezialdatenbanken wie die des BMS ermitteln.\nZum Entdecken von Literatur eignen sich auch allgemeine Suchmaschinen oder spezielle Academic Search Engines wie Google Scholar sowie gänzlich andere Wege wie bestehende Literaturverzeichnisse, Empfehlungslisten auf Lernplattformen und Webshops.\n\nInsbesondere Webshops haben im Vergleich zu Discovery-Systemen sehr personalisierte Such- und Empfehlungsdienste, die jedoch auf einer intensiven Auswertung des jeweiligen Nutzungsverhaltens basieren. Die Verwendung dieser Daten zur Personalisierung ist auch in Discovery-Systemen denkbar, wird aber aus Datenschutz- und Neutralitätsgründen grundsätzlich eher abgelehnt.\nEine vergleichsweise neue Herangehensweise insbesondere an das entdeckende Suchen bieten Wissensgraphen (knowledge graphs), die die vielfältigen Beziehungen zwischen Dokumenten und damit verknüpften Elementen darstellen und visualisieren. Die Anforderungen an die Qualität der so aufbereiteten Daten sind jedoch ungleich höher. Entsprechende Systeme existieren bereits in ausgewählten Bereichen, z. B. die Plattform SoNAR zur historischen Netzwerkanalyse. Ein ernstzunehmendes Beispiel für einen allgemeinen Wissensgraphen ist die Datenbank Wikidata mit ihren bibliografischen Inhalten WikiCite und dem dazu gehörigen Browsing-Interface Scholia (siehe Abbildung Abbildung 8.2).\n\n\n\n\n\n\nAbbildung 8.2: Thematisches Netzwerk von Publikationen in und über Scholia\n\n\n\nGrundsätzlich gilt, dass die Grenzen zwischen Discovery-Systemen und Alternativen in der Praxis fließend sind und dass Discovery-Systeme perspektivisch um Funktionen anderer Systeme erweitert werden können und sollten.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "discovery.html#zusammenfassung-ausblick",
    "href": "discovery.html#zusammenfassung-ausblick",
    "title": "Discovery-Systeme",
    "section": "Zusammenfassung & Ausblick",
    "text": "Zusammenfassung & Ausblick\nDiscovery-Systeme kommen den Erwartungen von Benutzer*innen an das Suchen und die Erreichbarkeit von Medien entgegen. Insbesondere Bibliotheken, die kein Bibliotheksmanagement-System verwenden, das bereits eine Discovery-Oberfläche enthält, müssen sich daher oft und mit hoher Priorität damit auseinandersetzen, wie Informationen über den eigenen Bestand durch ein Discovery-System besser zugänglich gemacht werden. Dies eröffnet Fragen nach möglichen Betriebsmodellen, gegebenenfalls des Change Management, bis hin zum kontinuierlichen Monitoring und der Weiterentwicklung solcher Dienste.\n\n\n\n\nChristensen, Anne. 2022. „Wissenschaftliche Literatur entdecken: Was bibliothekarische Discovery-Systeme von der Konkurrenz lernen und was sie ihr zeigen können“. LIBREAS, Nr. 41. https://doi.org/10.18452/24798.\n\n\nSteilen, Gerald. 2012. „Discovery-Systeme - die OPACs der Zukunft?“ Hamburg. https://www.slideshare.net/steilen/discoverysysteme-die-opacs-der-zukunft.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discovery-Systeme</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html",
    "href": "digitalisierung.html",
    "title": "Digitalisierung",
    "section": "",
    "text": "Grundlagen\nDigitalisierung ist zu einer zentralen Aufgabe an vielen Bibliotheken geworden. Über die Jahren haben sich für diese Aufgabe technische und organisatorische Standards und Workflows entwickelt und konsolidiert. So existieren inzwischen zahlreiche Softwaresysteme wie z.B. Workflow-Managementsysteme.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html#grundlagen",
    "href": "digitalisierung.html#grundlagen",
    "title": "Digitalisierung",
    "section": "",
    "text": "Ziele der Digitalisierung\nZu den Zielen der Digitalisierung zählen vor allem die Bestandserhaltung und Bestandsentwicklung. Durch die digitale Verfügbarmachung soll der Kundennutzen erhöht werden, indem Teilhabe und Zugang zu den Objekten erleichtert werden. Deshalb gibt es einen engen Bezug zu Themen wie Open Access oder Open Science. Durch eine erhöhte Sichtbarkeit der Institutionen bzw. einzelner Objekte und Sammlungszusammenhängen wird die Steigerung der institutionellen Relevanz angestrebt.\n\n\nBegriffsbestimmung\nDigitalisierung steht in diesem Kapitel für die Überführung von materiellen Kulturgütern in digitale Formate. Bibliotheken und andere kulturbewahrende Institutionen nehmen sich für die Digitalisierung sowohl Schriftgut (Bücher, Handschriften, Nachlässe) als auch audiovisuelle Medien und andere Objekttypen vor. Dabei wird oft auch von Retrodigitalisierung oder Bestandsdigitalisierung gesprochen, wenn die Objekte nicht originär digital sind („born-digital“), sondern bereits physische vorlagen und erst später digitalisiert werden.\n\n\nGeschichte\nAls Vorläufer der Digitalisierung kann die Mikroverfilmung betrachtet werden, bei der bereits seit dem 19. Jahrhundert analoge Dokumente mit dem Ziel der Langzeiterhaltung, der Vervielfältigung oder als platzsparender Ersatz für Originale in ein anderes analoges Format transformiert wurden. Die Überführung in digitale Formate begann in den 1990er Jahren. Das Ziel war hier zunächst vor allem die weltweite Zugänglichmachung von besonderen Werken oder Beständen für die wissenschaftliche Nutzung. Waren es zunächst vor allem bedeutende Druckwerke des 16., 17. und 18. Jahrhunderts, die digitalisiert wurden, hat sich der Fokus der Digitalisierung auch zeitlich, thematisch und formal sehr erweitert.\nIn Deutschland war und ist die Deutsche Forschungsgemeinschaft (DFG) ein wesentlicher Akteur, der durch Projektförderung erhebliche Impulse für die großflächige Retrodigitalisierung gegeben hat. Gleichzeitig sorgten die in geförderten Projekten verbindlichen DFG-Praxisregeln, die auch in anderen Projekten oft angewendet werden, für ein hohes Maß an Einheitlichkeit und Interoperabilität zwischen verschiedenen Beständen. Neben technischen Parametern für den Scan-Prozess (Auflösung, Farbtiefe u.a.) und Speicherformaten (TIFF unkomprimierte Master-Images) werden dort Standards zur Speicherung von Metadaten festgelegt, die von der Library of Congress entwickelt wurden und weiter gepflegt werden. Die DFG hat darüber hinaus mit dem DFG-Viewer einen Minimalstandard in der Objektpräsentation entwickeln lassen. Zu den Standards der DFG-Förderung gehört auch, dass jedes Werk nur einmal digitalisiert werden soll.\nEine weitere große Digitalisierungs-Initiative ging und geht von Google aus, die weltweit und in Deutschland insbesondere bei der Bayerischen Staatsbibliothek in großem Umfang alte Druckwerke digitalisieren (Google Books).\nErgänzend digitalisieren, veröffentlichen, erschließen und editieren private Akteure digitale Kulturgüter – Akteure in Wikimedia-, Open Data- und Open Science-Gemeinschaften, oder sie arbeiten ehrenamtlich in einschlägigen bibliothekarischen Projekten mit.\n\n\nStatus quo\nZahlreiche Bibliotheken mit umfangreichen unikalen Beständen haben sich auf den Weg gemacht, Retrodigitalisierung als festes Arbeitsfeld in die eigene Organisation aufzunehmen. Für die Unterstützung der Prozesse und für die Präsentation der Ergebnisse im Web haben sich unterschiedliche Softwarelösungen etabliert, zu denen aktive Anwendergemeinschaften gehören. Zusammen mit den geklärten Standards ist ein Rahmen geschaffen, in dem diese Bibliotheken kontinuierlich neue Digitalisierungsprojekte starten. Größere Häuser öffnen zum Teil ihre Infrastruktur anderen kleineren Einrichtungen in Landesprogrammen wie z.B. in Sachsen oder Hamburg. In Berlin unterstützt die Arbeitsstelle digiS mit einem jährlichen Förderprogramm seit 2012 Digitalisierungsprojekte in unterschiedlichen technischen Umgebungen - um nur einige Beispiele aus der heterogenen föderalen Struktur herauszugreifen.\n\n\nAusblick\nNeben der reinen bildhaften Digitalisierung wird von Forschenden zunehmend auch eine Erstellung und Bereitstellung von Volltexten erwartet. Diese kann entweder über einfache Text-Bild-Zuordnung oder auch mit aufwändiger semantischer Auszeichnung erfolgen. Die von der DFG geförderte Initiative OCR-D erstellt Werkzeuge und Infrastrukturen, um für die Werke des 16., 17. und 18. Jahrhunderts, aber auch für viele weitere Digitalisate eine effiziente und alltagstaugliche Volltext-Erstellung zu ermöglichen.\nAußer Druckwerken und Handschriften werden zunehmend weitere Medien und Objekte digitalisiert. Das sind nicht nur einschlägige Textdokumente aus Bibliotheken und Archiven, sondern auch Objekte aus Sammlungen und Museen, die teilweise auch in 3D digitalisiert werden, und audiovisuelle Medien. Außerdem wird versucht, zusätzliche Attribute der materiellen Objekte zu erfassen (z.B. Strukturerkennung, Kontextinformationen).\nZudem werden sich GLAM-Einrichtungen auch immer stärker des Bedarfes bewusst, Digitalisate auch jenseits eigener Portale auffindbar und nutzbar zu machen (siehe Digitalisierung als Beitrag zur Open GLAM-Bewegung).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html#prozesse",
    "href": "digitalisierung.html#prozesse",
    "title": "Digitalisierung",
    "section": "Prozesse",
    "text": "Prozesse\nDie Digitalisierung von Kulturgütern gliedert sich in mehrere Prozessschritte. Es haben sich dabei einrichtungsübergreifend ähnliche Prozessketten etabliert, zu deren Unterstützung Workflowmanagementsysteme (WMS) entwickelt wurden und die Projektsteuerung erleichtern.\n\n\n\nProzessschritte im Digtalisierungsprozess\n\n\nAls Ausgangspunkt jedes Digitalisierungs-Vorhabens ist eine gründliche Planung der Arbeitsschritte unverzichtbar. Dabei sind die lokalen Rahmenbedingungen und die Infrastruktur der digitalisierenden Einrichtung ebenso zu berücksichtigen wie die konkreten Ziele, die zu erzeugenden Formate und Ergebnisse sowie die Verknüpfungen zu externen datenverarbeitenden Systemen. Die DFG-Praxisregeln geben wichtige Hinweise, was bei den Prozessen beachtet werden sollte. WMS können diese Schritte steuern und teilweise auch automatisieren.\nFür die Planung von Zeitungsdigitalisierung sollte im Vorwege besonders genau auf die Einzelschritte im Prozess geschaut werden oder kollegiale Beratung aus anderen Häusern eingeholt werden, um angesichts der Massen an Material den effizientesten Weg bei der Erschließung zu finden, evtl. Dienstleister optimal einzubinden und alle Möglichkeiten von Automatisierung auszuschöpfen.\n\nVorbereiten: Auswahl von Objekten\nEine sinnvolle Auswahl zu digitalisierender Objekte hängt von Faktoren ab, deren Wichtigkeit gegeneinander abzuwägen ist. Dazu zählen beispielsweise\n\ndie bestehenden Kapazitäten: verfügbare Scanverfahren, Arbeitsplätze, personelle Verfügbarkeiten),\ndie technischen Ausstattung: Scanner, Speicherkapazität, Anbindung an eine Langzeitarchivierung\nzeitliche Faktoren: Projektlaufzeit, Förderbedingungen, Anlässe,\nDigitalisierungswürdigkeit: Einzigartigkeit, Nachfrage\nErhaltungszustand: Dringlichkeit, Machbarkeit\nexistierende Vorarbeiten: vorhandene Metadaten, Vervollständigung bereits angefangener Vorhaben\n\nDie Prioritäten für die eigene Bestandsdigitalisierung sollten immer wieder überprüft werden, wenn sich die genannten Faktoren ändern.\nNach der Auswahl von zu digitalisierenden Objekten folgen weitere vorbereitende Schritte. Dazu gehören z.B.\n\ndas Ausheben der Werke aus dem Bestand\ndas Überprüfen und ggf. das Herstellen der Digitalisierungs-Tauglichkeit\ndas Anlegen von Vorgängen im Workflowmanagementsystem\ndas Erzeugen von Laufzetteln\ndie Vorbereitung von Übergaben physischer Objekte an etwaige Scan-Dienstleister\n\n\n\nDigitalisieren\nDie Entscheidung für das oder die am besten geeigneten Digitalisierungsverfahren ist individuell am Objekt zu treffen und hängt von verschiedenen Faktoren ab, wie der Art des Objekts, der gewünschten Genauigkeit und den verfügbaren Ressourcen.\n\n2D-Objekte\nFür das Scannen von 2D-Objekten („Flachware“) hat sich als Zielformat der Bilddigitalisierung unkomprimiertes TIFF etabliert. Es werden mit Aufsichtscannern oder fotografischen Reproständen Einzelseiten aufgenommen und vom Bildausschnitt her wird mit schwarzem Rand digitalisiert. Verschiedene Auflösungen („Derivate“) für die Webpräsentation werden nachgelagert erzeugt oder dynamisch von einem Bildserver zur Verfügung gestellt.\nFür die verschiedenen Digitalisierungsvorhaben bilden die DFG-Praxisregeln eine gute Orientierungshilfe - für die Auflösung, den Farbraum oder andere technische Parameter. Für die Beurteilung und Sicherstellung der objektiven Bildqualität haben sich zudem Digitalisierungsstandards etabliert, die mittlerweile auch von Scannerherstellern und Scandienstleistern berücksichtigt werden. Erwähnenswert sind hier vor allem Metamorfoze und der daraus hervorgegangene ISO-19263 Standard .\nBei der Zeitungsdigitalisierung ist im Interesse der Forschung die Erzeugung von Volltexten nach der Digitalisierung ein wichtiges Ziel. Diese Perspektive bedingt im Hinblick auf die Qualität der Texterkennung (OCR) eine Präferenz für eine Digitalisierung vom Original gegenüber einer Digitalisierung vom Mikrofilm.\n\n\n3D-Objekte\nMittlerweile haben einige Bibliotheken angefangen, über flache, zweidimensionale Informationen hinaus auch die Tiefe und Mehrdimensionalität des kulturellen Erbes und Wissens innerhalb ihrer Bestände zu erfassen. Bibliotheksbestände sind häufig heterogen und ergeben sich aus einer fortgesetzten Sammlungsgeschichte, sodass auch spezielle dreidimensionale Objekte, wie Globen, Spiel- oder PopUp-Bücher oder historische Objekte des Buchdrucks wie Lettern, Teil eines Bestandes sein können. In diesen Fällen hat sich eine Digitalisierung in allen drei Dimensionen als ein entscheidendes Werkzeug erwiesen, um die Mehrdimensionalität in digitale Formate zu übertragen und um über Inhalte hinaus den Zustand und Details als Kulturerbe-Objekte wiederzugeben. Dies betrifft ebenso seltene Bücher und Manuskripte mit einzigartigen oder kunstvollen Bindungen. Um die drei Dimensionen angemessen abzubilden, sind spezielle Erfassungsmethoden notwendig.\n\n\nAV-Medien\nBibliotheken und andere GLAM-Einrichtungen archivieren häufig auch analoge audiovisuelle Medien. Für die Digitalisierung der unterschiedlichen Quellen (z.B. Magnettonbänder, Film von 8mm bis 35mm, Selbstschnittplatten, Videobänder) werden im Falle einer Massendigitalisierung zumeist externe Dienstleister beauftragt, sodass die Herausforderungen hier in erster Linie in der inhaltlichen Erschließung und der Erhaltung des originalen Materials liegen.\nFür alle Materialarten gilt: Eigene Scanner sind nur erforderlich, wenn der Scanprozess nicht an einen Dienstleister ausgelagert werden soll oder kann. Nach der Auswahl geeigneter Geräte (am besten nach Beratung durch erfahrene Einrichtungen) empfiehlt sich eine Teststellung, bei der diese unter Realbedingungen erprobt werden können.\n\n\n\nErschließen\n\nErschließen von Metadaten\nErschließen bedeutet das Zusammentragen von allen verfügbaren Informationen zu einem Objekt und die Codierung in Form von Metadaten. Im Kontext der Digitalisierung sind drei Arten von Metadaten besonders relevant:\n\nAdministrative Metadaten mit Informationen zu Herkunft, Erhaltungszustand, technische Merkmale, Rechteinformationen.\nDeskriptive Metadaten zur bibliografischen Beschreibung des Objektes.\nStrukturelle Metadaten für Gliederungselemente wie z.B. Kapitel in Texten, Segmente in 3D-Objekten, Frames in Filmen.\n\nBibliografische Basisdaten wie auch strukturelle Metadaten können in einem WMS entweder importiert oder manuell erfasst werden. Viele der Systeme sind dafür individuell konfigurierbar und können je nach Art des Projektes angepasst werden. Ein wichtiges Metadatum sind Identifikatoren (URN, DOI, PURL …) und deren Generierung. Für die Strukturierung von Textdokumenten ist das Stukturdatenset für den DFG-Viewer ein praxistauglicher Referenzrahmen.\nUm die Nachnutzung der digitalisierten Objekte zweifelsfrei zu kennzeichnen, gehören Informationen zur Lizenzierung in die administrativen Metadaten. Dafür hat eine Expertengruppe innerhalb von DINI detaillierte Empfehlungen für Rechteinformationen in Metadaten aufgestellt.\nStrategien für das Crowdsourcing von Erschließungsarbeiten in offenen Informationsinfrastrukturen wie dem Wikiversum ermöglichen potenziell zusätzliche Mehrwerte, auch im Nachhinein und durch Dritte.\n\n\nZusätzliche inhaltliche Erschließung durch Volltexte und Annotationen\nErschließung endet nicht bei der Zuordnung von Metadaten. Ziel der Erschließung ist auch eine Abbildung des Werkes in nachnutzbarer Form. Bei Textdokumenten ist das die Generierung von Volltexten. Für die Auswahl eines Verfahrens und deren Parametrisierung sind Eigenschaften der Vorlage (Schriftart, Layout, Sprache, Papier- und Druckqualität u.a.) sowie die benötigte Qualität der Volltexte entscheidend.\nWaren anfangs vor allem kommerzielle OCR-Werkzeuge dominierend (z.B. ABBYY), haben sich mittlerweile einige Open Source-Projekte (z.B. Tesseract) entwickelt, die qualitativ vergleichbare oder bessere Ergebnisse erzeugen können. Dank neuer technologischer Ansätze sind auch Lösungen für handschriftliche Dokumente verfügbar (z.B. OCR4all). Mit Förderung durch die DFG arbeitet die Initiative OCR-D an einem frei nutzbaren Workflow, der neben der eigentlichen Volltexterkennung weitere Schritte (z.B. automatische Layouterkennung) einbindet, die die Qualität der Erschließung verbessern.\nDie Generierung von Volltexten kann als (automatischer) Prozessschritt im Workflowmanagementsystem eingebunden werden. Die Ergebnisse werden in der Regel in standardisierten Datenformaten geliefert und können dann in Präsentationssystemen z.B. für die Volltextsuche genutzt oder zum Download angeboten werden.\nNeben Volltexten können je nach Kontext auch Annotationen als weiteres Element der Erschließung eine Rolle spielen, die aber in der Regel nicht im Digitalisierungsprozess, sondern erst im Nachhinein entstehen.\n\n\n\nPräsentieren\nDie Präsentation von digitalisierten Objekten und den dazugehörigen Metadaten im Web ist ein wesentliches Ziel der Digitalisierungs-Aktivitäten.\nDie Präsentation der digitalisierten Objekte hängt von der Art von Objekten und beabsichtigten Nutzungsszenarien ab und erfordert eigene Werkzeuge, die unabhängig von dem genutzten Workflowmanagementsystem ausgewählt und konfiguriert werden. Basis für Präsentationslösungen sind die im Workflow erzeugten standardisierten XML-Files (METS/MODS).\nFunktional wird von einer Präsentationslösung eine konfigurierbare Recherche-Umgebung für Metadaten und Volltexte erwartet. Bei der Präsentation des einzelnen Objektes werden die Strukturdaten zusammen mit den dazu erfassten Metadaten als Inhaltsverzeichnis angeboten und es werden zur Arbeit mit den Scans Bildwerkzeuge erwartet (z.B. Zoomen, Rotieren, Thumbnail-Übersicht). Zunehmend wichtige Funktionen sind Download-Angebote oder die Bereitstellung von iiif-Manifesten zur Weiternutzung in anderen Viewern.\nMit dem DFG-Viewer existiert eine Minimallösung für die Anzeige von Digitalisaten, die aus dem Katalog verlinkt werden kann. Für eine umfassendere Präsentation von Digitalisaten entscheiden sich Einrichtungen für eine Integration der Digitalisate in ein existierendes Repositorium oder für den Aufbau von Web-Angeboten mit speziellen Werkzeugen für Digitalisate wie z. B. Kitodo.Presentation. Ein wichtiges Element für mehr Sichtbarkeit der eigenen Digitalisierungs-Aktivitäten ist eine Datenlieferung an die Deutsche Digitale Bibliothek.\nZunehmend relevanter wird die Suche nach Lösungen für eine Präsentation von rechtebehafteten Materialien für eingeschränkte Personenkreise. Repositorien haben hier in der Regel schon anpassbare Lösungen. Bei der Präsentation von Digitalisaten existieren prototypische oder an Institutionen individuell angepasste technische Lösungen. Standards für die Beschreibung von Zugriffsrechten in den Metadaten oder generische Präsentationslösungen auf Basis dieser Metadaten befinden sich noch im Abstimmungsprozess oder der Entwicklung.\nWikimedia Commons kann darüber hinaus als Medienspeicher fungieren. Von dort werden digitalisierte Objekte und ‚born digital‘-Dokumente in Wikipedia und andere Webseiten eingebettet. Die Erschließung mit strukturierten Daten erfolgt mittels Wikidata (siehe Digitalisierung als Beitrag zur Open GLAM-Bewegung).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html#werkzeuge",
    "href": "digitalisierung.html#werkzeuge",
    "title": "Digitalisierung",
    "section": "Werkzeuge",
    "text": "Werkzeuge\n\nScanner und Scanverfahren\n\n2D\nScanner und ihre Verfahren lassen sich grundlegend nach der Eigenschaft ihrer zu scannenden Objekte und der Zielsetzung des Scan-Ergebnisses unterscheiden.\nIm Bereich des 2D-Scans unterstützen z.B. spezielle Buchscanner den Scanprozess im Rahmen der Digitalisierung effizient und für das zu digitalisierende Material auf schonende Weise. Das sind in der Regel Auflichtscanner oder fotografische Systeme, bei denen das Material von oben abgelichtet wird. Je nach Art und Zustand des Materials können dabei Buchwippen bzw. -wiegen mit verschiedenen Winkeln oder Glasplatten eingesetzt werden. Roboter können das automatische Umblättern der Seiten erledigen, sind dabei aber für empfindliche Bestände weniger schonend. Berührungslose Scanner sind vor allem für Handschriften vorgesehen.\n\n\n\nBeispiel eines Auflichtscanners\n\n\nDie 2D-Scanner können sich in verschiedenen Eigenschaften wie Größe der Formate, Farbtiefe, Auflösung, Art und Qualität der Optik und Mechanik sowie in der Verarbeitungsgeschwindigkeit unterscheiden. Um eine hohe und gleichbleibende Bildqualität zu gewährleisten, ist eine regelmäßige Kalibrierung der Systeme und Auswertung der Aufnahmen anhand von Test-Targets (z.B. UTT-Target oder GoldenThread) erforderlich. Neben der Hardware unterscheiden sich Scanner in der Software, durch die Rohdaten des Scanners für die weitere Nutzung bearbeitet (z.B. beschneidet oder glättet), optimiert und in gewünschten Zielformaten speichert.\n\n\n3D\nBei der 3D-Erfassung eines Objektes kann zwischen photo- und scanbasierten Verfahren unterschieden werden.\nSo werden bei der Photogrammetrie Fotos aus verschiedenen Blickwinkeln aufgenommen. Dabei wird die Anzahl der Fotos derart erhöht, dass sie mit Hilfe von Software zu einem 3D-Modell verarbeitet werden können. Nach diesem Prinzip funktionieren auch 3D-Scanning-Apps für Smartphones, die 3D-Scans von Objekten mithilfe der Kamera eines Smartphones ermöglichen.\nDemgegenüber stehen verschiedene Scanverfahren. Beim Laser-Scanning werden z.B. Laserstrahlen auf das Objekt gerichtet, und die reflektierten Daten werden verwendet, um ein präzises 3D-Modell zu erstellen. Neben dem Laser lassen sich solche 3D-Modelle auch mit strukturiertem Licht (Streifenlichtscans) oder Computertomographie erzeugen.\nBei der 3D-Modellierung wird ein 3D-Modell nach Vorlagen manuell und mit Hilfe von spezifischer Software erstellt. Dies kommt insbesondere für die digitale Rekonstruktion von physisch nicht erhaltenen (Teilen von) Objekten zur Anwendung.\nAlle Verfahren können miteinander kombiniert werden.\nWird beispielsweise ein Pop-up Buch oder ein Globus 3D-digitalisiert, sollen zusätzlich die kinetischen Aspekte (ein Globus dreht sich um seine Achse, bei einem Pop-up Buch lässt sich eine Lasche herausziehen usw.) unter Umständen auch im Digitalisat nachvollziehbar bleiben. Besteht das Originalobjekt aus Einzelteilen und werden diese entsprechend in Segmenten digitalisiert, können diese Segmente interaktiv nutzbar gemacht werden oder/und Animationen eingesetzt werden. Ein einfacher Scan reicht in diesem Fall nicht aus, da hier lediglich \"die Hülle im Ganzen\" erfasst wird. Eine manuelle Nachbearbeitung ist nötig.\n\n\n\nWorkflowmanagementsysteme\nBei diesen Systemen handelt es sich im Grunde um klassische Workflowmanagementsysteme (WMS), die auf die Abbildung und Modellierung der oben beschriebenen Prozesse spezialisiert sind. Das Ziel der Systeme ist es, die Prozesse technisch abzubilden und zu organisieren bzw. die Zustände der im Prozess befindlichen Objekte zu speichern und darzustellen.\n\n\n\nBeispiel für einen im WMS eingerichteten Workflow\n\n\nAufgrund der hohen Spezialisierung ist die Auswahl an WMS für bibliothekarische Digitalisierungsprozesse überschaubar. Dazu gehören sowohl kommerzielle Produkte, wie z.B. Visual Library aber auch frei verfügbare Open Source Systeme wie Kitodo.\nDer wesentliche Unterschied zwischen den Tools – neben der Frage der Lizenz – ist deren Modularität. Bei einigen Lösungen sind z.B. die Erstellung, Anreicherung und Präsentation fest in einem System zusammengefasst, andere wählen z.B. den Ansatz, die Präsentation von der Erstellung zu trennen, sodass die Darstellung z.B. auch mit spezialisierten Präsentationswerkzeugen, wie Content-Management-Systemen erfolgen kann.\n\n\nMetadaten und Schnittstellen\nAn vielen Stellen des Digitalisierungsprozesses sind Schnittstellen (APIs) zu Drittsystemen notwendig. Das gilt sowohl für die Einbindung externer Metadaten (beispielsweise der Import aus Nachweissystemen via SRU aus Verbundkatalogen oder Kalliope) als auch die Weitergabe von Daten an externe Portale wie die Deutsche Digitale Bibliothek (DDB).\nAls Austauschformat verwenden diese Schnittstellen meist XML-Datenformate wie PICA/XML, METS/MODS, EAD und LIDO, die auf Struktur- oder Metadaten digitalisierter Objekte spezialisiert sind. Relevante Schnittstellen sind vor allem SRU, OAI-PMH und IIIF. Die Nutzung gemeinsamer Datenformate ermöglicht unter anderem die Auffindbarkeit, Verknüpfung, Präsentation und Verwendung von Objekten in unterschiedlichen Zusammenhängen.\nFür einzelne Ressourcentypen und Anwendungen existieren oft genauere Anwendungsprofile, die die Verwendung von Metadaten-Standards erweitern oder einschränken.\nBeispiel: Das METS/MODS-Anwendungsprofile der Deutschen Digitalen Bibliothek legt Anforderungen an Metadaten fest, wenn diese in das Portal eingebunden werden sollen. Zur Validierung der Daten stellt die DDB passende Werkzeuge zur Verfügung.\n\n\nBetriebsmodelle\nBei dem Einsatz der Werkzeuge im Digitalisierungsprozess stellt sich immer die Frage, ob sich die Anschaffung der Hardware oder die hausinterne Installation und Anpassung von Software lohnt oder aber Bestandteile ausgelagert werden können. Wie bei allen IT-Systemen sind grundsätzlich drei Betriebsmodelle denkbar:\n\nLokale Installation: Hard- und Software wird durch betriebseigenes Personal gehostet, betreut und weiterentwickelt. Dieses Modell ist vor allem dann empfehlenswert, wenn bereits Digitalisierungskompetenz in der Institution vorhanden ist und die Digitalisierung einen dauerhaften Schwerpunkt in den Alltagsprozessen einnimmt bzw. einnehmen soll.\nExternes (Cloud-)Hosting: Sofern nicht z.B. rechtliche Gründe dagegen sprechen, ist dieses Modell empfehlenswert, wenn nur punktuell oder in geringem Umfang digitalisiert werden soll und dabei keine eigene Expertise notwendig ist oder aufgebaut werden soll.\nHybrid-Lösungen: Es werden nur Teile des Prozesses ausgelagert, z.B. das Scannen der Objekte, während z. B. die Inhaltsanreicherung durch Metadaten im Haus verbleibt.\n\nDie Hybrid-Variante wird im Alltag am häufigsten umgesetzt. Die Herausforderung besteht darin, die Schnittstellen zwischen externen und internen Prozessen genau zu definieren und festzulegen. So muss zum Beispiel externen Scan-Dienstleistern Zugriff auf das intern betreute Workflowmanagementsystem gewährt werden, damit digitalisierte Objekte direkt im System abgelegt werden können.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html#digitalisierung-als-beitrag-zur-open-glam-bewegung",
    "href": "digitalisierung.html#digitalisierung-als-beitrag-zur-open-glam-bewegung",
    "title": "Digitalisierung",
    "section": "Digitalisierung als Beitrag zur Open GLAM-Bewegung",
    "text": "Digitalisierung als Beitrag zur Open GLAM-Bewegung\nMit Digitalisierung leisten Bibliotheken und andere GLAM-Einrichtungen wichtige Beiträge dazu, dass Kulturgüter weltweit zugänglich, gut auffindbar und nachnutzbar sind. Neben der einrichtungseigenen Präsentation und aggregierenden Portalen wie der „Deutschen Digitalen Bibliothek“ oder der „Europeana“ spielen auch offene Wissensplattformen im Web eine wichtige Rolle, um Daten zugänglich zu machen, zu teilen und Nutzende an der Erschließung zu beteiligen. Entsprechende Möglichkeiten sollen im Folgenden skizziert werden, da es sich um einen wichtigen Trend bei der Digitalisierung und Nutzung handelt.\nOffener Zugang und offene Lizenzen für freie Nutzungsszenarien, standardisierte Schnittstellen und offene Metadaten vereinfachen das Teilen von Daten bzw. Informationen aus digitalisierten Quellen – Texten, Illustrationen und Sammlungen – als freies Wissen.\n\nVeranstaltungsformate und Handlungsfelder\nHackathons und Editathons sind Veranstaltungsformate, um kurzfristig die Bekanntheit digitaler Sammlungen zu steigern, Datenexperimente zu ermöglichen und Datenkollaborationen in Pilotprojekten anzustoßen. Dabei können Impulse entstehen für weiterentwickelte Datenanwendungen. Lerneffekte entstehen dabei nicht nur bei den beteiligten Menschen, sondern auch institutionell bei den beteiligten Bibliotheken und anderen GLAM, die ggf. Bedürfnisse, Ideen und Defizite von Nutzenden kennenlernen. Ein Beispiel Hackathons ist die Initiative „Coding da Vinci“.\nCrowdsourcing und Citizen Science sind Handlungsfelder, die in Bibliotheken zunehmend Aufmerksamkeit wecken. Hierbei gewinnen Bibliotheken über externe engagierte Dritte zusätzliche Ressourcen für die Erschließung, Auswertung oder Korrektur von Daten.\n\n\n\n\n\n\nBeispiel\n\n\n\nIm Projekt „Die Datenlaube“ erfasst und korrigiert in der deutschsprachigen Wikisource eine Gemeinschaft Ehrenamtlicher seit 2008 die von verschiedenen Bibliotheken bereitgestellten Scans der Illustrierten „Die Gartenlaube“. Diese Wikisource-Volltexte und -Illustrationen werden mit offenen Metadaten in Wikidata strukturiert erschlossen.\n\n\n\nDie Datenlaube: Projektlogo\n\n\n\n\n\n\nOffene Infrastrukturen\nOffene digitale Infrastrukturen spielen für die Speicherung, Erschließung und Vernetzung von Digitalisaten eine wichtige Rolle, nicht zuletzt auch, um die beschriebene Beteiligung zu ermöglichen. Das prominenteste Beispiel dafür sind die Projekte der Wikimedia-Foundation, deren Grundprinzipien von Offenheit und Referenzierbarkeit die bibliothekarischen Erschließungs- und Präsentationskomponenten ergänzen:\n\nWikimedia Commons fungiert als zentraler Medienspeicher. Die Erschließung kann bis zur Details von Bildpositionen reichen. Die Illustrationen dieses Handbuchs sind in Wikimedia Commons in einer Medienkategorie gebündelt (siehe Abbildungsverzeichnis).\nMit der offenen Wissensdatenbank (Knowledge Graph) Wikidata können insbesondere Metadaten strukturiert und damit maschinenlesbar gespeichert, gemeinsam editiert, visualisiert, analysiert, geteilt und verknüpft werden.\nDie vielsprachigen Versionen der Wikipedia speichern Wissen in enzyklopädischen Artikeln und ermöglichen so die Zusammenführung von Digitalisaten mit Texten, Illustrationen aus den Wikimedia Commons, Referenzen, Links und damit Kontext.\nWikisource ist eine Quellensammlung gemeinfreier Werke und besteht sowohl aus transkribierten Volltexten und deren Illustrationen als auch thematischen Linksammlungen zu relevanten Digitalisaten in Digitalen Sammlungen öffentlicher und wissenschaftlicher Bibliotheken, Archive, Museen und Galerien (GLAM).\n\nHerausforderungen aus IT-bibliothekarischer Sicht bestehen darin, Wikimedia-Portale und bibliothekarische Werkzeuge durch Schnittstellen und Datentransformation (ETL-Tools) miteinander zu verknüpfen. GLAM-Labore (Open GLAM Labs) können dafür als Experimentier- und Ausbildungsort fungieren – in Bibliotheken und institutionenübergreifend z.B. im Rahmen von Landesdigitalisierungsprogrammen.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html#checkliste-für-digitalisierungsprojekte",
    "href": "digitalisierung.html#checkliste-für-digitalisierungsprojekte",
    "title": "Digitalisierung",
    "section": "Checkliste für Digitalisierungsprojekte",
    "text": "Checkliste für Digitalisierungsprojekte\nDiese Checkliste dient als Leitfaden für die Planung und Umsetzung von Digitalisierungsprojekten in Bibliotheken und hilft sicherzustellen, dass die Ziele erreicht und die digitalen Sammlungen effektiv genutzt werden können.\nVorbereitungsphase:\n\nFestlegung der Ziele: Klären Sie die Hauptziele und den Zweck der Digitalisierung (z.B. Zugänglichkeit, Erhaltung, Forschung).\nRessourcenplanung: Ermitteln Sie die erforderlichen Ressourcen wie Verfügbarkeit der Objekte, Personal, Technologie, Budget und Zeitrahmen.\nRechtliche Überlegungen: Klären Sie urheberrechtliche und rechtliche Fragen bezüglich der digitalisierten Materialien.\nAuswahl der Objekte: Wählen Sie die Objekte aus, die digitalisiert werden sollen, u. a. basierend auf ihrer Bedeutung und ihrem Zustand.\nMetadatenplanung: Entwickeln Sie einen Plan zur Erfassung und Verwaltung von Metadaten für die digitalisierten Materialien.\n\nTechnische Umsetzung:\n\nScantechnologie: Wählen Sie die geeignete Scantechnologie und -auflösung entsprechend der Art der Objekte und deren Verwendungszweck aus.\nQualitätskontrolle: Implementieren Sie Prozesse zur Sicherstellung der Qualität der Digitalisate.\nDateiformate: Entscheiden Sie über die geeigneten Dateiformate für die digitalisierten Materialien.\nSpeicher und Sicherung: Planen Sie die Speicherung und regelmäßige Sicherung der digitalen Sammlung.\n\nMetadaten und Beschreibung:\n\nMetadatenstandards: Verwenden Sie etablierte Metadatenstandards, um die Beschreibung der Objekte zu standardisieren.\nDokumentation: Dokumentieren Sie den Digitalisierungsprozess und die verwendeten Metadaten.\nZugänglichkeitsprüfung: Überprüfen Sie, ob die digitalen Sammlungen barrierefrei und benutzerfreundlich sind.\n\nLangzeitpflege:\n\nErhaltungsstrategien: Planen Sie langfristige Erhaltungsstrategien für digitale Materialien.\nAktualisierung und Migration: Berücksichtigen Sie die Notwendigkeit von Aktualisierungen und Migrationen von Dateiformaten und Systemen.\n\nZugang und Bereitstellung:\n\nPlattformwahl: Wählen Sie die geeignete Plattform oder das geeignete System zur Präsentation und Verwaltung der digitalen Sammlungen.\nNutzungsrechte: Klären Sie die Nutzungsrechte und -bedingungen für die digitalen Inhalte.\nNutzerfeedback: Implementieren Sie Mechanismen zur Erfassung von Nutzerfeedback zur Verbesserung der digitalen Sammlungen.\n\nVeröffentlichung und Dissemination:\n\nVerbreitung: Planen Sie die Verbreitung der digitalisierten Sammlungen über Online-Kanäle, soziale Medien und Partnerschaften.\nMarketing: Entwickeln Sie Marketingstrategien, um die digitalen Sammlungen einem breiten Publikum bekannt zu machen.\nEvaluation: Führen Sie regelmäßige Bewertungen durch, um den Erfolg Ihrer Digitalisierungsprojekte zu messen und zu verbessern.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "digitalisierung.html#zusammenfassung-und-ausblick",
    "href": "digitalisierung.html#zusammenfassung-und-ausblick",
    "title": "Digitalisierung",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nViele Bibliotheken mit historischen Beständen sind im Feld der Kulturgutdigitalisierung engagiert und kooperativ unterwegs. Werkzeuge, Standards und Dienstleister stehen zur Verfügung, um die unikalen Bestände ins Netz zu bringen. Um die Bandbreite der Objekte zu erweitern, finden gerade wichtige Entwicklungen statt, um auch AV-Medien und Objekte jenseits von 2D in die Routine aufzunehmen. Ebenso werden die Bibliotheken mit den Ergebnissen aus OCR-D immer mehr in der Lage sein, auch ohne Expertenwissen gute Volltexte zu erzeugen bzw. ihre gesamten Digitalisate inkl. handschriftlicher Dokumente mit Volltexten anzureichern. Hier sind ggf. auch kooperative Infrastrukturen aufzubauen, um die Fortschritte in den technischen Möglichkeiten schnell zur Verfügung zu stellen und eine regelmäßige Erneuerung der Volltexte zum Standard werden zu lassen. Daneben gilt es, gezielt Kompetenzaufbau unter den Mitarbeitenden zu betreiben, um den Aufbau und Betrieb solcher Infrastrukturen zu gewährleisten.\nErste Bibliotheken machen sich mit der schnell wachsenden Menge an Volltexten auf den Weg, mit Hilfe ausgewählter Tools aus dem Feld des Maschinenlernens, ihre Suche semantisch anzureichern auf Basis der vorgeschalteten automatisierten Analyseverfahren.\nMit dem großen Schatz der Volltexte wird es darüber hinaus wichtiger werden, diesen auch den Forscher:innen zusätzlich über eine API anzubieten, um individuelle Korpora für den Download in eigene Umgebungen zusammenstellen zu können. Hier landet man dann auch schnell bei der Suche nach weitergehenden nachhaltigen Lösungen für urheberrechtlich geschützte Materialien.\nAber auch die kollaborativen Aktivitäten an einzelnen Digitalisaten im Wikiversum oder in Citizen Science Projekten sind getragen von dem Leitgedanken, dass die digitalisierten Kulturgüter in neue Zusammenhänge gestellt werden und Interessierte zu einem Thema zusammenbringen.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Digitalisierung</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html",
    "href": "forschungsnahe-dienste.html",
    "title": "Forschungsnahe Dienste",
    "section": "",
    "text": "Einleitung\nUnter forschungsnahen Diensten werden verschiedene Bibliotheksservices zusammengefasst, die Wissenschaftler*innen im gesamten Forschungsprozess unterstützen und die überwiegend im Kontext von digitalem Wandel und Open Science angesiedelt sind (https://www.o-bib.de/bib/article/view/5718). Dazu zählen z. B. Services in den Bereichen Forschungsdatenmanagement, Bibliometrie sowie verschiedene Publikationsdienste.\nEinige dieser Dienste, z. B. Repositorien für Zeitschriftenartikel, die unter Open-Access-Bedingungen zweitveröffentlicht werden dürfen, gehören schon seit Jahrzehnten zum Dienstleistungsrepertoire wissenschaftlicher Bibliotheken. Inzwischen betreiben größere Einrichtungen zudem oft spezialisierte Repositorien für ein Spektrum verschiedener Objekttypen: Publikationen wie Zeitschriften, Monografien und Sammelbände, sowie Open Educational Resources (OER), Forschungsdaten, Forschungssoftware und mehr. Dazu gehören auch diverse Dienste, die übergreifend den Forschungsoutput bestimmter lokaler oder fachlicher Forschungscommunities besser auffindbar oder messbar machen sollen. Darunter fallen Forschungsinformationssysteme und Dienstleistungen im Bereich von Metriken oder zur Verwaltung von Artikelveröffentlichungsgebühren in Open-Access-Journals (APCs).\nForschungsförderer und Ministerien erwarten in Deutschland mittlerweile von den Forschungseinrichtungen eine Transformation des Publikationswesens hin zu Open Access und Open Data. Dazu gehören themenbezogene institutionelle Policies, Beratungs- und Schulungsangebote sowie technische Dienste. Auf der Ebene der europäischen Forschungsförderung wird u. a. versucht, durch Initiativen wie CoARA die Maßstäbe der Forschungsbewertung weiterzuentwickeln - weg von klassisch bibliometrischen Indikatoren wie h-Index und Journal-Impact-Factor hin zu einer Würdigung vielfältiger Forschungsergebnisse.\nIn Abgrenzung zur Dominanz von Großverlagen ist im Bereich der wissenschaftlichen Informationsinfrastrukturen darüber hinaus der Anspruch entstanden, Dienste – bis hin zur Ebene von Entwicklung und Betrieb zugehöriger IT-Infrastrukturen – durch die Communities der Forschenden selbst zu betreiben. In der englischsprachigen Fachdiskussion sind dazu Begriffe wie „scholar-led publishing“ oder „scholar-led conferences“ geprägt worden, vgl. exemplarisch hierzu das Scholar-Led.Network Manifesto. Bibliotheken sehen sich hier als vertrauenswürdige, öffentlich finanzierte Dienstleister für die Wissenschaftscommunities.\nDer Einsatz von und die aktive Mitarbeit an Open Source-Software ist bei den forschungsnahen Diensten stärker als in anderen Bereichen der Bibliotheks-IT eine Selbstverständlichkeit. Auch die Bedeutung der gemeinschaftlichen Pflege offener Standards, Datenmodelle und Daten-Gemeingüter ist hier besonders ausgeprägt.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#einleitung",
    "href": "forschungsnahe-dienste.html#einleitung",
    "title": "Forschungsnahe Dienste",
    "section": "",
    "text": "Definition\n\n\n\nOpen Science bezeichnet den Ansatz, wissenschaftliche Forschungsergebnisse, Daten und Methoden frei zugänglich und transparent zu teilen. Teilaspekte davon sind Open Access, Open Data, OER und Open Source. Ziel ist es, die Zusammenarbeit sowie die Reproduzierbarkeit von Forschung zu fördern, Innovationen zu beschleunigen und den gesellschaftlichen Nutzen wissenschaftlicher Erkenntnisse zu maximieren. Dieser offene Ansatz erleichtert es Forschenden weltweit, Informationen frei zu nutzen, zu teilen und weiterzuentwickeln.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#publikationsdienste",
    "href": "forschungsnahe-dienste.html#publikationsdienste",
    "title": "Forschungsnahe Dienste",
    "section": "Publikationsdienste",
    "text": "Publikationsdienste\nDie Veröffentlichung wissenschaftlicher Erkenntnisse ist ein zentraler Bestandteil des wissenschaftlichen Arbeitens. Solange sich dieser Prozess noch in den Geschäftsprozessen klassischer Subskriptionszeitschriften abbildete, befanden sich wissenschaftliche Bibliotheken eher auf der Seite der Medienbereitstellung. Mit zunehmender Stärkung des Open-Access-Gedankens und dem Aufkommen neuer Geschäftsmodelle rückten wissenschaftliche Bibliotheken stärker in die Rolle des*der Unterstützenden und Ermöglichenden. Die Digitalität der gesamten Prozesskette vom Schreiben bis zur Veröffentlichung der Artikel in teilweise von den Bibliotheken getragenen Infrastrukturen erfordert die Einbindung unterschiedlicher IT-Werkzeuge, die im Folgenden näher beschrieben werden.\n\nJournal Publishing-Dienste\nOpen Access bedeutet, dass wissenschaftliche Literatur kostenfrei und öffentlich im Internet zugänglich ist, sodass Interessierte die Volltexte lesen, herunterladen, kopieren, verteilen, drucken, in ihnen suchen, auf sie verweisen und sie auch sonst auf jede denkbare legale Weise benutzen können, ohne finanzielle, gesetzliche oder technische Barrieren jenseits von denen, die mit dem Internetzugang selbst verbunden sind. Zum ersten Mal wurde dieser Gedanke in der Grundsatzerklärung der Budapester Open-Access-Bewegung formuliert.\nDurch die Transformationsprozesse im wissenschaftlichen Publikationswesen weg von den traditionellen Abonnement-Modellen hin zu Open Access sehen sich Bibliotheken zunehmend auch in der Rolle eines*einer Publikationsdienstleistenden. Dies kann einerseits die Gründung eines Universitätsverlages bedeuten, andererseits aber auch die Bereitstellung der notwendigen Infrastruktur für wissenschaftliche Zeitschriften. Begünstigt durch Kostenverschiebung vom Lesen hin Publizieren wird der scholar-led-Ansatz immer gefragter und Bibliotheken müssen in diesem Bereich Expertise aufbauen.\nZur Schaffung einer technischen Infrastruktur lässt sich z. B. mit der Software Open Journal Systems (OJS) eine Plattform zur Verfügung stellen, welche die strukturierte Veröffentlichung von Zeitschriften(-artikeln) ermöglicht. Parallel dazu müssen auch die erforderlichen Abläufe und Organisationsstrukturen angepasst werden. Personelle Ressourcen müssen hier ebenso bedacht werden. In erster Linie gilt es, die Herausgeber*innen-Teams der Zeitschriften zu unterstützen. Gleichzeitig sollte die Bibliothek auch technischen Support für einreichende Autor*innen bieten. Der Funktionsumfang von OJS ermöglicht es auch, einen Workflow für den Peer-Review-Prozess abzubilden. Auch hier liegt Potenzial für die Unterstützung durch Bibliotheken. Wichtig ist somit ein Überblick über den Gesamtprozess des wissenschaftlichen Publizierens und nicht nur die Software-Aspekte.\nParallel dazu entwickeln sich derzeit alternative Publikations-Plattformen wie Preprint-Dienste, (Micro-)Blogs, Data Journals und ähnliche Dienste, die traditionelle Publikationswege wie peer-reviewte Journals ergänzen, siehe dazu auch die entsprechende Publikation der Initiative Knowledge Exchange. Hier können Bibliotheken ebenso in die Rolle des*der Dienstleistenden treten, Instanzen geeigneter Open-Source-Software hosten und diese Entwicklung unterstützen. Dies kann auch im Zusammenhang mit der externen Kommunikation von Bibliotheken (Link zu entsprechendem Kapitel) betrachtet werden. Ergänzend gibt es zahlreiche weitere Dienste, die das Open-Access-Publizieren vor allem administrativ unterstützen, von denen im Folgenden einige kurz erläutert werden.\n\n\nOpen-Access-Dienste\nNeben der Unterstützung bei Open-Access-Veröffentlichungen haben sich im Zusammenhang mit der Open-Access-Transformation weitere IT-Dienste in Bibliotheken entwickelt.\n\nVerlags-Software\nDa im Sinne der „scholar-led infrastructure“ zunehmend Universitätsverlage gegründet werden, existieren hier auch eigene Software-Lösungen für die Verwaltung der Titel, Lagerbestände, Kund*innen-, Adress- und Versanddaten. Häufig ist ein eigener Webshop mit entsprechenden Funktionalitäten integriert und es gibt Schnittstellen zur Buchhaltung und Auslieferung. Für kleinere Verlage reicht vermutlich ein einfacher Webshop aus; Spezialsoftware kann verlagsspezifische Anforderungen allerdings besser abbilden. Betrieb und Administration der Programme gehören mit in das zugehörige Dienstleistungsportfolio.\n\n\nPublikationsfonds-Verwaltung\nPublikationsfonds zur Finanzierung von Open-Access-Publikationen sind an vielen wissenschaftlichen Bibliotheken fest verankerte Hilfsmittel, um die Transformation des wissenschaftlichen Publikationswesens hin zu Open Access zu unterstützen. Durch sie werden anfallende Article Processing Charges (APC) entsprechend vorgegebener Kriterien der Einrichtung (mindestens anteilig) zentral gezahlt. Zum Monitoring der entstehenden Kosten ist häufig eine Tabellenkalkulationssoftware ausreichend. Für einen Vergleich verschiedener Einrichtungen und ein einheitliches Reporting existieren jedoch Services wie OpenAPC. Zur einheitlichen Darstellung aller Publikationskosten hat sich mit OpenCost ein xml-Metadatenschema etabliert.\n\n\nOpen Refine\nSollen Daten aus unterschiedlichen Quellen zusammengeführt werden oder ein Abgleich gegen externe Datenbanken stattfinden, ist OpenRefine ein geeignetes Werkzeug: Über offene Schnittstellen können Datensätze sowohl erweitert als auch beispielsweise in Wikidata exportiert werden. Hierbei ist keine zentrale Installation notwendig. Die Software wird lokal auf den Endgeräten ausgeführt. Lediglich eine korrekte Einstellung der Firewall, um die Kommunikation mit externen Datenbanken zu ermöglichen, gilt es zu beachten.\n\n\nJournal-Finder-Werkzeuge\nNeben den üblichen Fragen zu Lizenzen bei Open-Access-Publikationen spielt auch die Wahl eines geeigneten Journals eine zentrale Rolle. Derzeit verbreitete Journal-Finder-Werkzeuge wie B!SON oder der oa.finder greifen für die Journal-Daten zwar beide auf das Directory of Open Access Journals (DOAJ) zu, verfolgen jedoch unterschiedliche Ansätze. Der oa.finder zeigt eine Liste von Zeitschriften mit Filteroptionen und den jeweiligen Förderbedingungen an. Diese werden aus bestehenden Transformationsverträgen abgeleitet und enthalten keine spezifischen Förderbedingungen. B!SON bietet teilnehmenden Einrichtungen die Möglichkeit, eine eigene Liste mit Förderbedingungen zu hinterlegen. Treffer werden direkt mit den korrekten Förderbedingungen angezeigt. Zu diesem Zweck muss ein Web-Backend entsprechend der Vertragssituation gepflegt werden.\n\n\n\nRepositorien für Forschungsergebnisse\nZentral für die Veröffentlichung jedweder Art von wissenschaftlichem Output ist ein geeigneter Ort für deren Veröffentlichung, ganz besonders im Hinblick auf die zunehmende Datengetriebenheit der Wissenschaften.\nHeute haben sich Open-Access-Repositorien als verlässliche Speicherdienste für wissenschaftliche Ergebnisse etabliert, seien es Forschungsdaten, textuelle sichergestellt werden, dass publizierte Ergebnisse in der veröffentlichten Form erhalten werden, d. h. nicht verändert oder gelöscht werden. Da Internetadressen als flüchtig gelten, werden zur Identifikation Persistent Identifier-Systeme eingesetzt. Die Zitierfähigkeit und damit der langfristige, möglichst originalgetreue Erhalt der einmal eingestellten Informationen grenzen Repositorien gegenüber anderen Arbeitsplattformen wie Sync-and-Share-Plattformen (z. B. Nextcloud), Content-Management-Systemen (CMS) zur Erstellung von Blogs und Internetseiten sowie virtuellen Forschungsumgebungen mit integrierten Funktionen z. B. für die Datenanalyse ab.\nEs lassen sich grundlegend zwei Typen von Repositorien unterscheiden: Disziplinspezifische Repositorien sammeln die Inhalte einer bestimmten Forschungsdisziplin (Suche nach Disziplin möglich über re3data), während generische Repositorien Inhalte unterschiedlicher Disziplinen aufnehmen (z. B. Zenodo). Einen Sonderfall fachübergreifender Repositorien bilden institutionelle Repositorien, die speziell für die Mitglieder der jeweiligen Institution zur Verfügung stehen. Einige Repositorien widmen sich gezielt der Sammlung bestimmter Datentypen (z. B. Preprint- oder Publikationsserver, Forschungsdatenrepositorien, Bilddatenbanken etc.). Insbesondere im Bereich Forschungsdaten und OER gibt es seitens der Nutzenden häufig den Wunsch, dass in Repositorien eine interaktive Arbeit mit den Materialien möglich ist. Derzeit sind Repositorien in der Regel noch ausschließlich auf die sichere Speicherung und Bereitstellung der Inhalte ausgerichtet - eine Erweiterung um Funktionalitäten z. B. zur Visualisierung oder Analyse würden sie in die Nähe von virtuellen Forschungsumgebungen rücken. Häufig werden Repositorien und Repositoriensoftware zudem mit der digitalen Langzeitarchivierung als Kernfunktionalität in Verbindung gebracht - aus Sicht von Repositorien-Betreiber*innen und -Entwickler*innen handelt es sich bei der digitalen Langzeitarchivierung allerdings eher um eine Spezialfunktionalität, die nur für bestimmte Nutzungsszenarien relevant und daher auch nicht in allen Repositorien gegeben ist. Mit dem proprietären System Rosetta und der Open Source-Lösung Archivematica seien hier nur zwei Beispiele für Systeme genannt, die sich auf den Erhalt von Informationen im Sinne der Langzeitarchivierung spezialisieren.\n\n\n\n\n\n\nDefinition\n\n\n\nDer Begriff Repository bzw. Repositorium wird in diesem Kapitel vorwiegend für Plattformen verwendet, in denen Forschungsergebnisse dauerhaft archiviert, beschrieben, auffindbar und zugänglich gemacht werden. Darüber hinaus wird der Begriff Repository auch für Versionsverwaltungssysteme wie Git verwendet, die vorwiegend von Softwareprojekten genutzt werden, um es einer geschlossenen oder offenen Community zu ermöglichen, transparent und konfliktfrei zur Codebasis des jeweiligen Projekts beizutragen. U. a. Zenodo erlaubt über eine Schnittstelle zum Code-Verwaltungssystem GitHub die Archivierung von Softwareprojekten und ähnlichem entsprechend den Konventionen von Repositorien für Forschungsergebnisse, einschließlich der Vergabe von Digital Object Identifiers (DOIs).\n\n\nDas Open Directory of Open Access Repositories (OpenDOAR) stellt Statistiken über die Verbreitung von Softwarelösungen für Repositorien für Forschungsergebnisse bereit: Meistgenutztes System weltweit ist DSpace, das auch in Deutschland zunehmend Verbreitung findet. Während DSpace vor allem von Universitäten eingesetzt wird, ist bei Fachhochschulen und Hochschule für Angewandte Wissenschaften OPUS stark verbreitet, das meist durch den Kooperativen Bibliotheksverbund Berlin-Brandenburg (KOBV) oder das Bibliotheksservice-Zentrum Baden-Württemberg (BSZ) gehostet wird. Der Gemeinsame Bibliotheksverbund (GBV) bietet mit MyCoRe eine weitere Repositoriensoftwarelösung an. OPUS und MyCoRe finden bislang ausschließlich im deutschsprachigen Raum Anwendung, während DSpace von einer globalen Community getragen wird. Für Forschungsdatenrepositorien kommen DSpace und die auf Forschungsdaten spezialisierte Software Dataverse zum Einsatz.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#forschungsdatenmanagement",
    "href": "forschungsnahe-dienste.html#forschungsdatenmanagement",
    "title": "Forschungsnahe Dienste",
    "section": "Forschungsdatenmanagement",
    "text": "Forschungsdatenmanagement\nDie digitale Transformation hat Forschungsprozesse grundlegend verändert: In zahlreichen Fachdisziplinen entstehen an Forschungseinrichtungen täglich große Mengen digitaler Daten, die als Forschungsgegenstand dienen, angereichert, analysiert oder visualisiert werden. Dabei stehen Wissenschaftler*innen vor der Herausforderung, diese Daten nicht nur zu verwalten, sondern sie auch langfristig und nachvollziehbar vorzuhalten und möglichst offen zur Nachnutzung zur Verfügung zu stellen. Maßgeblich sind hierfür die sogenannten FAIR-Prinzipien, denen zufolge Forschungsdaten auffindbar (findable), zugänglich (accessible), interoperabel (interoperable) und nachnutzbar (reusable) sein sollen (vgl. https://doi.org/10.1038/sdata.2016.18).\n\n\n\n\n\n\nDefinition\n\n\n\nForschungsdaten sind alle digital vorliegenden Daten, die während des Forschungsprozesses entstehen (z. B. Messdaten, Laborwerte, Videoaufnahmen, Umfrageergebnisse). Klar davon abzugrenzen sind Forschungsinformationen. Der Lebenszyklus von Forschungsdaten beinhaltet die Erstellung, Speicherung, Archivierung bis hin zur Löschung aussortierter Daten.\n\n\n\n\n\n\n\n\nAbbildung 10.1: Darstellungen des Forschungsdaten-Lebenszyklus in unterschiedlichen Detailgraden helfen bei der Betrachtung der benötigten Werkzeuge für die forschungsnahen Dienste.\n\n\n\nDie Schnittmenge von Forschungsdaten und Forschungsinformationen liegt, wie in Abbildung 10.2 dargestellt, im Bereich Publikationen. Während sich allerdings Forschungsinformationen eher auf klassische, kontrolliert publizierte Dokumente beziehen, geht die Publikation von Forschungsdaten weit darüber hinaus und schließt alle Formen von Aufzeichnungen wie Notizen, Zwischenergebnisse und Forschungssoftware mit ein.\n\n\n\n\n\n\nAbbildung 10.2: Forschungsinformationen und ihre Sicht auf Forschungsdaten (CC-BY Franziska Mau)\n\n\n\nServices zum Forschungsdatenmanagement (FDM) sollen Wissenschaftler*innen beim Umgang mit ihren Forschungsdaten unterstützen, und zwar über den gesamten Forschungsdaten-Lebenszyklus hinweg, d. h. von der Datenplanung über die Datenerhebung und -analyse bis hin zur Datenarchivierung, -publikation und -nachnutzung. Bibliotheken nehmen beim Aufbau und Betrieb entsprechender Services eine zentrale Rolle ein - in aller Regel sind sie hierbei nicht die einzigen Akteur*innen, sondern das Serviceportfolio wird arbeitsteilig von Bibliotheken, Rechenzentren, Forschungsabteilungen und ggf. weiteren Akteur*innen angeboten. Angesichts der großen Heterogenität disziplinspezifischer Datentypen gelangen diese in aller Regel fachübergreifenden FDM-Dienste häufig an ihre Grenzen: Diese Erkenntnis ist konstitutiv für die seit 2020 im Aufbau befindliche Nationale Forschungsdateninfrastruktur (NFDI), in der fachspezifische und institutionsübergreifende Dienste entwickelt werden. In diesem Zusammenhang entwickeln sich derzeit neue Berufe wie Data Steward oder Data Librarian, die fachspezifische Unterstützung beim FDM leisten und entweder zentral an den FDM-Servicestellen oder dezentral in Projekten oder Fachbereichen angesiedelt sind.\nDie von Bibliotheken angebotenen Services zum FDM umfassen in der Regel sowohl nicht-technische Services (z. B. Schulungs- und Beratungsangebote) als auch verschiedene technische Dienste. Zu den wichtigsten technischen Diensten für das FDM, die von Bibliotheken (mit-)betrieben werden, gehören Repositorien für Forschungsergebnisse. Diese ermöglichen die Veröffentlichung von Forschungsdaten als eigene Informationsobjekte gemäß den FAIR-Prinzipien Daneben werden häufig weitere FDM-Tools angeboten, von denen einige im Folgenden vorgestellt werden.\n\nTools zur Erstellung von Datenmanagementplänen\nUm den Umgang mit Forschungsdaten über ein komplettes Projekt zu beschreiben, hat sich der Datenmanagementplan (DMP) als geeignetes Format erwiesen. Derartige Pläne werden zunehmend von Forschungsförderern bei der Antragstellung oder in der Frühphase des Projekts erwartet. Sie basieren häufig auf für die Förderlinie bzw. das Fachgebiet spezifischen Fragenkatalogen. Durch die einheitlichen Fragelisten und fest definierte Ausgabeformate lässt sich so ein menschen- und maschinenlesbares Dokument generieren. Um diese Fragenkataloge einheitlich zur Verfügung zu stellen und ggf. mit einrichtungs- oder programmspezifischen Daten anzureichern, wurden bereits einige Software-Werkzeuge entwickelt. In Deutschland verbreitet einem von der Deutschen Forschungsgemeinschaft (DFG) geförderten Projekt entstandene Research Data Management Organizer (RDMO), für den, unterstützt durch NFDI-Konsortien, ständig neue Fragenkataloge in einer Gemeinschaftsarbeit entwickelt werden. Obwohl einige öffentliche Instanzen der Software existieren, die z. B. einen Login über die ORCID ermöglichen, kann die Open-Source-Software auch selbst gehostet und inhaltlich sowie visuell auf die Bedarfe der jeweiligen Einrichtung zugeschnitten werden. DMP können innerhalb dieser Software kollaborativ erstellt werden, indem Personen als Mitarbeitende in das eigene Projekt eingeladen werden. Für EU-Projekte ist mit Stand 2023 die Software ARGOS verfügbar, die eine direkte Einbindung der DMP in die European Open Science Cloud (EOSC) ermöglicht. Für Software als Forschungsdatum beginnen sich Softwaremanagementpläne zu etablieren.\n\n\nElektronische Laborbücher\nDie Dokumentation der Forschungsergebnisse ist ein zentraler Punkt im Forschungsdaten-Lebenszyklus. Digital entstandene Daten sollten ohne Medienbruch mit ihrer Dokumentation verknüpft und zugehörige Metadaten erfasst werden. Hierzu gibt es eine Reihe dedizierter Software-Lösungen, die unter dem Begriff elektronische Laborbücher (abgekürzt häufig ELN von electronic laboratory notebook) zusammengefasst werden. Neben kommerziellen Programmen wie LabFolder, bei denen Bibliotheken eher in der Rolle des*der Vermittelnden sind, gewinnen Open-Source-Lösungen zunehmend an Bedeutung. Diese können Bibliotheken auf eigenen Servern hosten und selbst administrieren, sind jedoch bei der Entwicklung neuer Features auf eine Community bzw. den*die Entwickler*in oder eigene Fachkräfte angewiesen. Beispiele sind hier eLabFTW für generische ELN bzw. Chemotion für eine eher fachspezifische Lösung. Eine Handreichung zur Einführung eines ELN an der eigenen Einrichtung wurde 2023 von einer einrichtungsübergreifenden Autor*innengruppe erstellt. Open-Source-ELN erfahren häufig Unterstützung durch NFDI-Konsortien. Der daraus entstandene Wunsch eines einheitlichen Transferformats der Laborbucheinträge und gemeinsamer Spezifikationen wird im ELN Consortium adressiert. Hilfestellung bei der Auswahl eines passenden Produkts bietet z. B. der ELN-Finder.\n\n\nGit\nAls freie Software zur Versionsverwaltung ist Git ein Standardtool der Softwareentwicklung geworden. Durch einfache Befehle auf der Kommandozeile oder zusätzlich installierte Software mit grafischem Interface lassen sich textuelle Daten auf dem eigenen System mit einem externen (Code-)Repositorium abgleichen, das die notwendigen Protokolle versteht. Neben dem großen Anbieter GitHub gibt es die lokal zu installierende Software GitLab, um ein solches Repositorium in der eigenen IT-Infrastruktur bereitzustellen. Durch die im Protokoll integrierte Versionskontrolle der Daten lassen sich Änderungen im Code einfach nachvollziehen und ggf. zurückrollen. Kollaborative Arbeit in verteilten Teams wird z. B. über eine parallele Entwicklungsstruktur in „branches“ ermöglicht, die mit dem Hauptprojekt zu einem gewünschten Zeitraum zusammengeführt werden können. Bei diesem Funktionsumfang wird schnell klar, dass Git auch jenseits der forschungsnahen Dienste eine Vielzahl von Anwendungsmöglichkeiten hat. Hierfür sei auf das entsprechende Kapitel verwiesen. Eine Möglichkeit, die Funktion der Software spielerisch zu erkunden, bietet beispielsweise die vom Bundesministerium für Bildung und Forschung geförderte Webseite ohmygit.org.\nForschungssoftware lässt sich, nicht nur wegen der Verwaltung mit Git oder ähnlichen Programmen, nicht einfach komplett analog zu den Forschungsdaten behandeln, sondern bedarf eines genaueren Blicks.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#forschungssoftware",
    "href": "forschungsnahe-dienste.html#forschungssoftware",
    "title": "Forschungsnahe Dienste",
    "section": "Forschungssoftware",
    "text": "Forschungssoftware\nBei der Betrachtung von Forschungsprozessen setzt sich zunehmend die Erkenntnis durch, dass auch die dabei zum Einsatz kommende Software ein Teil der Forschungsdaten ist. Dies ist häufig kein kommerziell erhältliches Produkt, sondern ein speziell auf das Forschungsproblem zugeschnittener, selbst programmierter Code. Wird derartige Software nicht korrekt gesichert, versioniert und dokumentiert, leidet die Reproduzierbarkeit von Forschungsdaten. Komplexe externe Probleme wie prekäre Beschäftigungsverhältnisse können zusätzlich zum Verwaisen von Softwareprojekten führen, wenn diese nur lokal durch einzelne engagierte Personen vorangetrieben wurden. Zusätzlich führt eine Veröffentlichung der Forschungssoftware zu einer Auffindbarkeit und Zitierbarkeit, sodass diese zum wissenschaftlichen Output der Forschenden einen signifikanten Beitrag leisten kann. Aus der Wissenschaft getriebene Vereinigungen wie de-RSE e. V. treten als Vereinszweck für den Stellenwert von Forschungssoftware ein. Auch die FAIR-Prinzipien sollten für Forschungssoftware Anwendung finden (vgl. https://doi.org/10.1038/s41597-022-01710-x). Hier liegt es auch an den Bibliotheken, ein Bewusstsein dafür zu schaffen (z. B. durch dedizierte Policies) und die benötigte Infrastruktur bereitzustellen.\nZur Zitierbarkeit von Forschungssoftware/Code dient die Generierung von entsprechenden Metadaten, etwa über https://codemeta.github.io/ und CITATION.cff, um menschen- und maschinenlesbare Zitierinformationen für Software und Datensätze angeben zu können. Ein entsprechendes Beispiel ist die Datei CITATION.cff im Quelltext dieses Handbuchs.\nDie Bereitstellung eines Coderepositoriums, in der Regel über Git, sollte zum Standardangebot zählen. Für eine interaktive wissenschaftliche Datenauswertung bietet sich zusätzlich der Betrieb eines JupyterHub an. Dieser ermöglicht die Nutzung von Jupyter Notebooks auf einem zentralen Server der Einrichtung und ist so nicht abhängig von den jeweiligen Rechenleistungen der verfügbaren Endgeräte. Zusätzlich sollte geprüft werden, inwiefern eine Archivierung der Software bzw. eines gesamten Repositoriums in der eigenen Infrastruktur nötig und sinnvoll ist. Anbieter wie Software Heritage bieten eine Archivierung auf externen Servern an. Zumindest in diesem Fall ist es als UNESCO-Projekt als geeignete Alternative zu betrachten.\nDie Verknüpfung der im Gesamtprozess entstehenden Metadaten mit Systemen wie dem Forschungsinformationssystem (FIS) oder Forschungsdatenrepositorien hinsichtlich der Auffindbarkeit dieser wissenschaftlichen Ergebnisse ist ein Punkt, der eine Betrachtung der kompletten Toolchain notwendig macht.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#forschungsinformationssysteme",
    "href": "forschungsnahe-dienste.html#forschungsinformationssysteme",
    "title": "Forschungsnahe Dienste",
    "section": "Forschungsinformationssysteme",
    "text": "Forschungsinformationssysteme\n\n\n\n\n\n\nDefinition\n\n\n\nForschungsinformationen sind Angaben über Aktivitäten, Ergebnisse und Infrastrukturen von Forschungsprozessen wie z. B. Projekte, Publikationen und Forschungseinrichtungen. Davon zu unterscheiden sind Forschungsdaten.\n\n\nNeben Forschungsdaten gewinnt auch die strukturierte Erfassung von Forschungsinformationen an Bedeutung. Entsprechende Systeme werden Forschungsinformationssysteme (FIS) genannt. Dabei handelt es sich um Datenbanksysteme, die speziell für die Erfassung, Organisation, Speicherung und Verknüpfung von Forschungsinformationen konzipiert wurden. Sie können interne Anwendungen wie die leistungsorientierte Mittelvergabe unterstützen und für die Außendarstellung der Einrichtung genutzt werden. Eine Übersicht von Forschungsinformationen und ihre Sicht auf Forschungsdaten gibt Abbildung 10.2.\nFIS führen Informationen zusammen, die dezentral in verschiedenen hochschulinternen Systemen (z. B. Drittmittelverwaltung, Personalverwaltungssysteme, Repositorien) und externen Quellsystemen (z. B. Scopus, ORCID) vorgehalten werden, um einen strukturierten und aktuellen Überblick über die Forschungsleistungen beispielsweise einer Einrichtung, eines (Bundes-)Landes oder einer Fachdisziplin zu gewinnen.\nDie genauen Daten, die Nutzung der Daten und der Funktionsumfang eines FIS sind nicht festgelegt bzw. klar definiert. Verschiedene Softwarelösungen verfolgen unterschiedliche Ansätze. Bei einige steht die Auffindbarkeit von Forschungsergebnissen und deren Verknüpfung mit den Forschenden im Vordergrund, andere Systeme fokussieren eher auf dem Berichtswesen und Monitoring und ggf. darauf aufbauende Anreizsysteme. Wiederum andere Systeme legen den Schwerpunkt auf die Präsentation der Forschungsaktivitäten und deren öffentlichkeitswirksamen Bereitstellung . Die Systeme passen sich zunehmend aneinander an bzw. werden die Funktionen immer häufiger in einem System kombiniert.\nFIS sollten von Anfang an als Daueraufgabe einer Einrichtung betrachtet und entsprechende finanzielle und personelle Ressourcen eingeplant werden. Bei der Einführung eines FIS handelt es sich um ein langjähriges Organisationsentwicklungsprojekt, das eine Offenheit für Veränderungen in den Prozessen und Workflows der Einrichtung voraussetzt.\n\n\n\nHerausforderungen beim Aufbau eines Forschungsinformationssystem (CC-BY Franziska Mau)\n\n\nEine zentrale Herausforderung beim Aufbau eines FIS besteht darin, einen Überblick über die bestehenden Quellsysteme der Einrichtung zu gewinnen. In diesem Zusammenhang ist zu ermitteln, welche internen und externen Systeme relevant sind und wer die entsprechenden Ansprechpersonen an der Einrichtung sind. Dies betrifft u. a. die Bibliothek (z. B. Repositorien), die Personalverwaltung (Identitätsmanagement), die Drittmittelverwaltung (Datenbank für Projekte), die Doktorand*innenverwaltung oder die Patentverwaltung der Einrichtung.\nNeben der Identifikation der relevanten Datenquellen stellt die Integration der Daten in das FIS meist die größte Herausforderung dar. So muss zum einen für fehlende oder ungeeignete Schnittstellen eine Lösung gefunden werden. Zum anderen variieren Qualität und Konsistenz der vorhandenen Daten mitunter stark, was zusätzliche Zeit für die Datenbereinigung und -konvertierung erfordert. Gleichzeitig ist die Sicherstellung der Datenintegrität und -qualität von entscheidender Bedeutung, um zu gewährleisten, dass das FIS korrekte und aussagekräftige Informationen liefert.\nDer Markt für FIS-Software ist sehr dynamisch. Vor dem Hintergrund, dass sich gerade viele Forschungseinrichtungen in der Planungs- und Aufbauphase von FIS befinden, kommen in Deutschland immer neue Softwarelösungen zum Einsatz. Es zeigt sich ein vielgestaltiges Bild aus kommerziellen Produkten (z. B. PURE, Converis, HISinOne-RES), Open Source-Lösungen (z. B. DSpace-CRIS, VIVO) und Eigenentwicklungen. An deutschen Forschungseinrichtungen wird mittlerweile häufig HISinOne-RES genutzt - befördert u. a. durch Landesinitiativen wie CRIS.NRW, HeFIS oder FIS-Thüringen sowie den Umstand, dass es aktuell das einzige Produkt am Markt ist, dessen Datenmodell direkt am Kerndatensatz Forschung (KDSF) ausgerichtet ist. Obwohl sich ein Rückgang an Eigenentwicklungen andeutet, sind sie immer noch weit verbreitet. Des Weiteren gibt es die bereits lange etablierten kommerziellen Systeme Converis und PURE. Der Einsatz von Open Source-Lösungen wie DSpace-CRIS und VIVO nimmt erst in den letzten Jahren merklich zu – u. a. befördert durch das Verbundprojekt Hamburg Open Science.\nAn vielen Einrichtungen besteht das Bestreben, dass das FIS zusätzlich die Funktionalität eines Repositoriums übernehmen soll. Ein Vorteil eines solchen vereinigten Systems wird zum einen in den geringeren Systemkosten gesehen, zum anderen erscheint es weniger aufwendig, die bibliografischen Einträge in einem FIS schlicht mit den dazugehörigen Dateien anzureichern statt einen Workflow für das Zusammenspiel zwischen FIS und Repositorium zu entwickeln. Dem entgegen stehen die verschiedenen Zielsetzungen beider Systeme: Während es bei einem FIS vor allem darum geht, möglichst alle Forschungsaktivitäten z. B. einer Einrichtung in einem System zu erfassen, steht bei einem Repositorium die nachhaltige Bereitstellung der Ressourcen selbst im Vordergrund (z. B. textuelle Publikationen oder Forschungsdaten). Ein Problem bei Mischsystemen ergibt sich auch hinsichtlich Retrieval und Zugriff: So werden Forschende bei einer Suche in externen Suchmaschinen z. B. erst im FIS feststellen, dass nur bei einem Teil der Treffer tatsächlich Zugang zu den Ressourcen selbst besteht, sie in den meisten Fällen jedoch lediglich Nachweise der Ressourcen finden. In der Praxis sind FIS-Repositorien-Mischsysteme dennoch aufgrund von Ressourcenknappheit nicht wegzudenken.\nNichtsdestoweniger sind die Publikationsdaten ein wichtiger Bestandteil jedes FIS. Aus diesem Grund ist das FIS eine gute erste Anlaufstelle, um interne bibliometrische Recherchen über den Output der eigenen Forschenden durchzuführen. Darüber hinaus sind primär Anfragen in externen Datenbanken als ergänzende Arbeitsschritte notwendig, keine weiteren Tools, die unter dem Aspekt IT in Bibliotheken aufgeführt gehören. Aus diesem Grund wird hier auf weitere Details verzichtet.\nUm eine Interoperabilität der unterschiedlichen Systeme und eine gute Auffindbarkeit der enthaltenen Ressourcen zu ermöglichen, ist eine Standardisierung notwendig - z. B. über Zertifikate, Metadatenstandards und Schnittstellen. Die in diesem Zusammenhang wichtigen Grundlagen werden in den folgenden Abschnitten erläutert.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#gemeinsame-ressourcen",
    "href": "forschungsnahe-dienste.html#gemeinsame-ressourcen",
    "title": "Forschungsnahe Dienste",
    "section": "Gemeinsame Ressourcen",
    "text": "Gemeinsame Ressourcen\n\nZertifikate und Standards\nForschungsnahe Dienste bewegen sich an der Schnittstelle zwischen Wissenschaft und Infrastruktureinrichtungen, welche die Dienste betreiben. Zertifikate erfüllen in diesem Zusammenhang verschiedene Funktionen. Sie sind als vertrauensbildende Maßnahmen gedacht, die Qualitätsmerkmale der Dienstleistungen sein sollen. Das DINI-Zertifikat für Open-Access-Publikationsdienste versteht sich seit jeher auch als Ratgeber bei Einrichtung, Weiterentwicklung und Betrieb solcher Dienstleistungen, der „Maßstäbe, Richtlinien und Best Practices“ vermitteln will. Letztlich dienen Zertifikate auch dem Schaffen von Standards, welche die Interoperabilität der Dienste ermöglichen. Neben dem DINI-Zertifikat sind in Bezug auf forschungsnahe Dienste das Core Trust Seal sowie das Nestor-Siegel für vertrauenswürdige digitale Langzeitarchive zu nennen. Während es mit der DIN Norm 31644 (auch als ISO Norm 16363 verbreitet) „Information und Dokumentation - Kriterien für vertrauenswürdige Langzeitarchive“ eine offizielle Norm für die Bewertung der Vertrauenswürdigkeit von Langzeitarchiven gibt, werden die meisten Standards in diesem Bereich eher als Best Practices oder Konventionen, denn als offizielle Normen eingeführt. Unabhängig von der Frage, ob Zertifikate als vertrauensstiftend eingeschätzt werden, lohnt es sich, die Dokumentation der Zertifikate als Ratgeber oder Checkliste zu nutzen; sowohl beim Aufbau neuer Dienste als auch zur regelmäßigen Überprüfung des eigenen Dienstes mit Blick auf neue Entwicklungen und Optionen eigene Dienste weiterzuentwickeln.\nSchon im Bethesda Statement on Open Access Publishing taucht das Stichwort Interoperabilität in Zusammenhang mit Repositorien auf. Dazu gibt es verschiedene technische Ansätze (siehe u. a. unten „Schnittstellen“). Neben den technischen Voraussetzungen, um Inhalte zu teilen, braucht es jedoch auch eine Einigung über die inhaltliche Aufbereitung der Informationen. Repositorien nutzen dazu strukturierte Metadaten. Für die Bezeichnung von Dokumententypen haben die DINI AG Elektronisches Publizieren und die DINI AG Forschungsinformationssysteme das Gemeinsame Vokabular für Publikations- und Dokumenttypen herausgegeben. Im Sinne der Standardisierung enthält das DINI-Zertifikat weitere Vorgaben, wie z. B. die Klassifizierung nach zumindest den Dewey-Dezimalklassifikations-Sachgruppen (DDC) der Deutschen Nationalbibliografie und macht Vorgaben an die Ausgestaltung des Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH). Diese Standards ermöglichen es Diensten wie z. B. der Bielefeld Academic Search Engine und anderen Aggregatoren Inhalte aus verschiedenen Quellen einzubinden, Metadaten maschinenlesbar zu erhalten und nachzunutzen.\n\n\nMetadaten\nMetadaten sind Daten struktureller, technischer, administrativer, bibliografischer und deskriptiver Natur, die Daten beschreiben (siehe Kapitel Daten & Metadaten). Metadatenstandards und -schemas definieren, welche Inhalte in Metadaten erfasst werden, also welche Metadatenfelder existieren und mit Werten belegt werden können. Ein sehr einfaches Metadatenschema bilden die Dublin Core Element (kurz Dublin Core). Im Rahmen der DOI-Registrierung werden auch Metadaten erhoben. Das DataCite Schema hat sich dabei als ein wichtiges Metadatenschema etabliert, das auch unabhängig von DOIs zur Beschreibung von Forschungsdaten genutzt wird. Darüber hinaus entwickeln die verschiedenen Fachdisziplinen eigene domänenspezifische Standards. Eine Aufgabe wissenschaftlicher Bibliotheken besteht in der Beratung bei der Auswahl geeigneter Metadatenschemata und Standards. Übersichten gibt es unter Anderem bei FAIRsharing.org, forschungsdaten.info und format.gbv.de.\n\n\nPersistent Identifier\nMit dem Aufkommen elektronischer Archive kam die Frage nach der Zitierbarkeit auf. Auch wenn die technischen Protokolle, auf denen das Internet basiert, sowohl im Domain Name System (DNS) als auch http(s) Mechanismen enthalten, um URLs weiterzuleiten, bekamen URLs schnell den Ruf, flüchtig zu sein. Auch wenn URLs, die nicht mehr oder auf andere Inhalte auflösen, immer auf Managementprobleme zurückgehen, wurden Persistent Identifier-Systeme geschaffen, welche diese Probleme beim Zitieren elektronischer Quellen überwinden sollen. Dabei werden IDs geschaffen, die über einen sogenannten Resolver aufgelöst werden können. Der Resolver ist vergleichbar mit einem Melderegister: Man kann nach der aktuellen Adresse einer ID fragen und erhält die jeweils aktuelle URL zurück, unter der sich die Ressourcen befinden sollen. Die Antworten können also zu unterschiedlichen Zeitpunkten unterschiedlich ausfallen.\nWährend die Deutsche Nationalbibliothek (DNB) bis heute auf URN:NBN als Persistent Identifier (PID) setzt, haben sich im wissenschaftlichen Umfeld DOIs für die Identifikation von Artikeln, Daten und anderen Inhalten durchgesetzt. In Deutschland kommen dabei vor allem die DOI-Registrierungsagenturen DataCite und CrossRef zum Einsatz. Beide vergeben DOIs sowohl für textuelle Publikationen als auch für Datensätze und andere Inhalte. Von der technischen Einbindung her ist DataCite moderner aufgestellt und leichter zu integrieren. Mit ORCID und ROR gibt es inzwischen weitere PID-Systeme, die zunehmend Verbreitung finden und Personen bzw. Einrichtungen eindeutig identifizieren.\nDie Vergabe von PIDs für Publikationen (z. B. Texte, Forschungsdaten) auf Publikationsservern bzw. Datenrepositorien wird teilweise von den wissenschaftlichen Bibliotheken gewährleistet. Somit werden Forschungsdaten nachhaltig unter entsprechenden Lizenzen öffentlich verfügbar gemacht (Berg-Weiß et al. 2022). Mit dem Ziel, eine nationale Beratungs- und Austauschplattform zu PID aufzubauen, fördert die DFG seit 2023 das Projekt „PID Network Deutschland - Netzwerk für die Förderung von persistenten Identifikatoren in Wissenschaft und Kultur“.\n\n\nLangzeitarchivierung\nDie dauerhafte Aufbewahrung und Lesbarkeit von digitalen Objekten zu gewährleisten, stellt auch für Bibliotheken, die zunehmend für die Archivierung von Open-Access-Publikationen, Forschungsdaten und anderen elektronischen Ressourcen verantwortlich sind, eine große Herausforderung dar. Die sogenannte digitale Langzeitarchivierung (LZA) beinhaltet neben der Speicherung zusätzliche Maßnahmen wie die regelmäßige Überprüfung der Datenintegrität, die Migration der Daten auf neue Speichermedien und die Anpassung an sich verändernde Technologien. Digitale Informationen bleiben so langfristig erhalten und auch in der Zukunft zugänglich.\nBewahrt werden müssen der Bitstream der Datei sowie deren Eigenschaften und Semantik. Aktuell ist PREMIS in der LZA der wichtigste Metadatenstandard. Das Datenmodell beinhaltet alle Informationen, die man sowohl über die digitalen Objekte selbst (z. B. Name, Dateiformat, Größe) als auch über Akteur*innen, Rechte (z. B. AccessRights, Embargofristen) und Prozesse (z. B. Konvertierung, Migrationen, Reparatur, Formatvalidierung) wissen sollte. \nEs gibt auf LZA spezialisierte Software wie Rosetta (ExLibris) oder Libsafe (libnova). Diese Systeme basieren meist auf dem international anerkannten Referenzmodell für digitale Archivierung OAIS (Open Archival Information System, ISO 14721:2012) und bieten neben den Standardfunktionen eines Archivsystems (z. B. bitstream-preservation, regelmäßige Integritätstests, Reduplizierung) auch Funktionen wie eine Format-Validierung und implementierbare Workflows.\nAuf der Webpage COPTR - Community Owned digital Preservation Tool Registry - werden diese und zahlreiche weitere Tools und Workflows zur LZA vorgestellt. Als wichtige Anlaufstelle für Fragen rund um die digitale LZA dient außerdem das Kompetenznetzwerk nestor, dessen Geschäftsstelle an der DNB angesiedelt ist. Auch die NFDI behandelt „Long-term Archival (LTA)“ in der Sektion „Common Infrastructures“ als Querschnittsthema.\n\n\n\nDas als ISO 14721 verabschiedete Referenzmodell „Open Archival Information System” (OAIS), CC-BY Jørgen Stamp\n\n\n\n\nSchnittstellen\nIm Bereich von Repositorien hat sich das Open Archives Initative Protocol for Metadata Harvesting (OAI-PMH) für den Austausch von Metadaten durchgesetzt. Dieses Protokoll wird inzwischen auch im Zusammenspiel mit anderen forschungsnahen Diensten wie z. B. FIS genutzt. Das Protokoll tauscht Metadaten in XML aus. Es unterstützt mehrere Metadatenformate, wobei die Spezifikation von OAI-PMH nur Dublin Core vorgibt und das Protokoll vorsieht, dass man eine Liste mit weiteren unterstützten Formaten abrufen kann.\nFür das Einbringen von Daten in Repositorien hat sich das Protokoll Simple Webservice Offering Repository Deposit (SWORD) durchgesetzt, wobei auf die genaue Version dieses Standards geachtet werden muss. Einige Open-Access-Verlage bieten an, Dokumente über SWORD direkt in Repositorien zu übertragen. DeepGreen, ein Lieferdienst für Open-Access-Artikel, versorgt Repositorien über SWORD mit Verlagsinhalten.\nSpeziell für die Arbeit mit Bildern, Bildviewern und Bilddatenbanken wurde das International Image Interoperability Framework (IIIF) entwickelt. IIIF deckt umfangreich verschiedene Funktionen ab, wie die Ausgabe von Bildern in verschiedenen Formaten und Auflösungen oder Zoomstufen, die strukturelle Beschreibung von Bildern, Suchanfragen in einem Bildpool, Umgang mit Zugriffsbeschränkungen, Objektänderungen und so weiter.\nOAI-PMH und SWORD werden zum Teil zwar auch über Repositorien hinaus verwendet, z. B. bei FIS. Verbreitung und Einsatz beschränken sich jedoch weitgehend auf den Bereich der forschungsnahen Dienste. Als weiter verbreitete Prinzipien zur Bereitstellung von Informationen muss Linked Data gesehen werden. Die Bereitstellung der Repositorieninhalte als Linked Data wird z. B. von DSpace unterstützt, in der Praxis jedoch leider noch viel zu selten aktiviert und genutzt.\nAllgemein verbreiten sich derzeit REST-Schnittstellen, also Schnittstellen zur Einbindung von Diensten über das Internet in verschiedene Programme und Infrastrukturen. Auch forschungsnahe Dienste profitieren sehr von der Bereitstellung von REST-Schnittstellen, damit sie miteinander verschränkt und in andere Infrastrukturen eingebunden werden können.\nIm Bereich von Repositorien, der in Bezug auf Schnittstellen und Standards oft Auswirkungen auf andere forschungsnahe Dienstleistungen hat, wurden von der Coalition of Open Access Repositories (COAR) weitere neue Protokolle vorgeschlagen. Hierzu zählt Signposting, das typisierte Links einsetzt, um von einer Ressource auf andere in Verbindung stehende Ressourcen zu verlinken und so die Auffindbarkeit durch Crawler und Bots erleichtern soll. Derzeit ist offen, ob Signposting sich durchsetzt. Wenn möglich, sollte sollte es in Repositorien aktiviert werden.\nEine aktuelle Entwicklung ist das Notify Project, das ebenfalls von COAR betrieben wird. Notify soll es ermöglichen, dass verschiedene forschungsnahe Dienste sich Nachrichten über Aktivitäten senden und so auf Ressourcen aufmerksam machen. Als Beispiele gelten automatisiert ablaufende Aktivitäten zwischen einem textuellen Repositorium und einem Forschungsdatenrepositorium oder zwischen einem Repositorium und einem Service für die Organisation des Peer-Reviewing. Notify hat vor allem in den USA eine größere Förderung erhalten und wird derzeit in Softwarelösungen für verschiedene forschungsnahe Dienste integriert. Es ist zu erwarten, dass Notify sich in diesem Bereich verankern und helfen wird, Dienste dynamischer miteinander zu verknüpfen.\nVon diesen Entwicklungen wird auch die Veröffentlichung von Forschungsdaten profitieren. Bevor Daten allerdings in einem Zustand sind, dass sie veröffentlicht werden können, durchlaufen sie einen eigenen Lebenszyklus, bei dem unterstützende Dienste der Bibliotheken zunehmend gefragt sind.\n\n\nToolchains\n\n\n\n\n\n\nDefinition\n\n\n\nEine Toolchain („Werkzeugkette“) ist eine Reihe von miteinander verbundenen Anwendungen und Technologien, die gemeinsam eingesetzt werden, um spezifische Aufgaben oder Arbeitsabläufe zu optimieren und zu automatisieren. Die Einrichtung einer Toolchain hilft, den reibungslosen Informationsfluss zwischen Arbeitsschritten zu verbessern.\n\n\nForschungsnahe Dienste müssen immer im Kontext des Forschungsprozesses und im Hinblick auf den Nutzen für die Forschenden betrachtet werden. Dazu ist es wichtig, für jeden der Dienste, die an einer Einrichtung genutzt werden, ein klares Profil zu erstellen. Hierbei muss insbesondere geklärt werden, wie sich die Dienste voneinander abgrenzen, damit Nutzenden klar kommuniziert werden kann, welcher Dienst wofür verwendet wird.\nGleichzeitig muss auch das Zusammenspiel der einzelnen Dienste analysiert werden. Wie knüpfen die verschiedenen Dienste aneinander an? Wie kann der gebotene Mehrwert durch Verknüpfungen der Dienste gesteigert werden? Welche Dienste bauen wie aufeinander auf? Diese Fragen sind nur lokal und konkret zu vorhandenen oder in Planung befindlichen Diensten zu beantworten.\nBeispiel: An einer Bibliothek findet in Zusammenarbeit mit Fachwissenschaftler*innen ein großes Retro-Digitalisierungsprojekt statt. Für die Verwaltung der Digitalisierungsvorgänge wird Kitodo verwendet. Die Werke sind im BMS verzeichnet und die Metadaten werden über die Search/Retrieve via URL-Schnittstelle (SRU) nach Kitodo importiert. Weitere Strukturdaten werden in Kitodo direkt eingetragen. Nach dem Scannen werden die Dokumente von Kitodo über die REST-API oder SWORD in das Repositorium exportiert. Das Repositorium ruft weitere Metadaten über SRU aus dem Bibliothekskatalog ab und vergibt DOIs, die wieder in das BMS zurückgespeichert werden. Das FIS harvestet die Inhalte über OAI-PMH regelmäßig und weist die Digitalisate nach, die im Repositorium bereitgestellt werden.\nIn diesem ideellen Bild wird nicht betrachtet, wo die nötigen Systeme stehen. Dies muss nicht immer ein lokal betriebenes System auf eigener Hardware sein, sondern ist oftmals als externer Service verfügbar.\n\n\nZusammenarbeit mit Dienstleistern\nSelbstverständlich ist es nicht bei allen Problemstellungen möglich, IT-Services für forschungsnahe Dienste im eigenen Haus anzubieten. Bei bereits etablierten Anwendungen lohnt sich eine Kontaktaufnahme mit der jeweiligen Verbundzentrale. Häufig werden dort bereits Services angeboten, für die man kein zusätzliches Personal bzw. keine eigene Infrastruktur einplanen muss. Beispielsweise sind das der Repository-Service Reposis des GBV oder das Langzeitarchiv Ewig des KOBV.\nWird die Verwendung einer bestimmten Software gefordert oder eine bestimmte Art, die Software einzusetzen, die nicht im Dienstleistungsportfolio der Verbundzentralen liegt, bietet sich die Zusammenarbeit mit externen, kommerziellen Dienstleistern an. Wo immer möglich, sollte Software von Open-Source-Communities Vorrang vor proprietärer Software genießen, da sie vor Abhängigkeiten von einzelnen Anbietern schützt und Weiterentwicklungen, welche in die Community eingebracht werden, von anderen Einrichtungen wiederverwendet werden können. Vor allem im Bereich der forschungsnahen Dienste sind Open-Source-Lösungen oft vorherrschend. Ziel von Infrastruktureinrichtungen sollte es sein, das zu erhalten, anstatt den Aufbau proprietärer Software und neuer Oligopol- oder Monopolstellungen zuzulassen. Der Mehrwert, den IT-Dienstleister bieten, besteht ähnlich wie bei den Verbundzentralen darin, dass man selbst personelle und ggf. Infrastrukturressourcen sparen kann. In den meisten Fällen gibt es die Möglichkeit, entweder die Installation und Betreuung des Dienstes auf lokaler Infrastruktur oder auch ein „Rundum-Sorglos-Paket“ mit Hosting beim Dienstleister inklusive Betreuung einzukaufen.\nBei der Wahl des Anbieters gilt es, auf dessen Erfahrung im Umgang mit Open-Source-Projekten allgemein und mit der gewünschten Software im Speziellen zu achten. Genau wie nach Referenzen zu vergleichbaren Projekten zu erkundigen, lohnt es sich, nach konkreten bereits geleisteten Beiträgen zu der jeweiligen Open-Source-Lösung zu fragen. Wenn die Open-Source-Lizenzen der jeweiligen Software keine Auflage machen, dass Weiterentwicklungen unter derselben Lizenz verbreitet werden müssen, sollte die Frage, unter welcher Softwarelizenz Weiterentwicklungen stehen, zwingend im Vertrag geklärt werden.\nBesonders weit verbreitete Softwarelösungen haben große Entwicklergemeinschaften, die es einerseits zu unterstützen gilt, andererseits auch die Community-getriebene Entwicklungsrichtung der Software zu beachten ist. Ist der gewünschte Dienstleister noch neu, ist zumindest eine anderweitige Erfahrung in ähnlichen Projekten wünschenswert. Eine Recherche in öffentlichen Software-Repositorien der Projekte kann hier zielführend sein.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "forschungsnahe-dienste.html#zusammenfassung-und-ausblick",
    "href": "forschungsnahe-dienste.html#zusammenfassung-und-ausblick",
    "title": "Forschungsnahe Dienste",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nForschungsnahe Dienste können Wissenschaftler*innen grundsätzlich über den gesamten Forschungsprozess hinweg unterstützen – das Portfolio möglicher Services ist daher sehr groß. Bibliotheken setzen ihren Schwerpunkt hierbei insbesondere auf Services zur Unterstützung des Publikationsprozesses sowie des FDM.\nWie das vorliegende Kapitel gezeigt hat, umfassen diese Services auch eine Vielzahl an IT-Diensten, so z. B. Journal-Publishing-Systeme, Repositorien und FIS. Der stabile und nachhaltige Betrieb solcher Dienste umfasst technische, organisatorische und inhaltliche Aspekte. Der Aufbau und Betrieb forschungsnaher Dienste an Bibliotheken bindet daher umfangreiche Ressourcen und erfordert ggf. auch eine Verlagerung von Ressourcen aus anderen Bereichen. Die Ausweitung des bibliothekarischen Serviceportfolios um forschungsnahe Dienste ist daher auch eine Frage der Organisations- und Personalentwicklung.\nZur vertieften Beschäftigung mit forschungsnahen Diensten an Bibliotheken wird folgende Literatur empfohlen:\n\nKonrad u. a. (2020)\nStille u. a. (2021)\nBerg-Weiß u. a. (2022)\nAzeroual (2021)\nSchirrwagen (2022)\nForschungsinformationssysteme (2022)\nDruskat u. a. (2022)\nGrossmann und Franke (2023)\nCyra, Politze, und Timm (2022)\nBarker u. a. (2022)\nPutnings, Neuroth, und Neumann (2021)\n\n\n\n\n\nAzeroual, Otmane. 2021. „Untersuchungen zur Datenqualität und Nutzerakzeptanz von Forschungsinformationssystemen“. https://doi.org/10.25673/45118.\n\n\nBarker, Michelle, Neil P. Chue Hong, Daniel S. Katz, Anna-Lena Lamprecht, Carlos Martinez-Ortiz, Fotis Psomopoulos, Jennifer Harrow, u. a. 2022. „Introducing the FAIR Principles for Research Software“. Scientific Data 9 (1): 622. https://doi.org/10.1038/s41597-022-01710-x.\n\n\nBerg-Weiß, Alexander, Sibylle Hermann, Miriam Kötter, Caroline Leiß, Christoph Müller, und Annette Strauch-Davey. 2022. „Openness in Bibliotheken : Positionspapier der Kommission für forschungsnahe Dienste des VDB“. o-bib. Das offene Bibliotheksjournal / Herausgeber VDB 9 (2): 1–4. https://doi.org/10.5282/o-bib/5826.\n\n\nCyra, Magdalene Alice, Marius Politze, und Henning Timm. 2022. „A push for better RDM: Erfahrungsbericht aus dem Einsatz von git für Forschungsdaten“. Bausteine Forschungsdatenmanagement, Nr. 2 (August). https://doi.org/10.17192/bfdm.2022.2.8435.\n\n\nDruskat, Stephan, Oliver Bertuch, Guido Juckeland, Oliver Knodel, und Tobias Schlauch. 2022. „Software publications with rich metadata: state of the art, automated workflows and HERMES concept“. arXiv. https://doi.org/10.48550/arXiv.2201.09015.\n\n\nForschungsinformationssysteme, Dini Ag. 2022. Management von Forschungsinformationen in Hochschulen und Forschungseinrichtungen. Humboldt-Universität zu Berlin. https://edoc.hu-berlin.de/handle/18452/26130.\n\n\nGrossmann, Yves Vincent, und Michael Franke. 2023. „Software ist kein Beiprodukt!“\n\n\nKonrad, Uwe, Konrad Förstner, Johannes Reetz, Klaus Wannemacher, Jürgen Kett, und Florian Mannseicher. 2020. „Digitale Dienste für die Wissenschaft - wohin geht die Reise?“, Dezember. https://doi.org/10.5281/zenodo.4301924.\n\n\nPutnings, Markus, Heike Neuroth, und Janna Neumann, Hrsg. 2021. Praxishandbuch Forschungsdatenmanagement. De Gruyter Saur. https://doi.org/10.1515/9783110657807.\n\n\nSchirrwagen, Jochen. 2022. „Repositorien und Forschungsinformationssysteme bilden keine Dichotomie“. Bibliothek Forschung und Praxis 46 (2): 284–88. https://doi.org/10.1515/bfp-2022-0013.\n\n\nStille, Wolfgang, Stefan Farrenkopf, Sibylle Hermann, Gerald Jagusch, Caroline Leiß, und Annette Strauch-Davey. 2021. „Forschungsunterstützung an Bibliotheken: Positionspapier der Kommission für forschungsnahe Dienste des VDB“. o-bib. Das offene Bibliotheksjournal / Herausgeber VDB 8 (2): 1–19. https://doi.org/10.5282/o-bib/5718.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Forschungsnahe Dienste</span>"
    ]
  },
  {
    "objectID": "kommunikation.html",
    "href": "kommunikation.html",
    "title": "Kommunikation",
    "section": "",
    "text": "Einleitung\nIm Verhältnis zwischen Intranet und Öffentlichkeitsarbeit bewegt sich die moderne Bibliothekslandschaft zunehmend weg von einem dualen Kommunikationsverständnis hin zu einem integrierten Ansatz. Dabei überlappen Kommunikationsräume oft. Werkzeuge dienen sowohl der internen Kommunikation als auch dem Dialog mit der Öffentlichkeit.\nIndem Bibliotheksmitarbeitende aktiv mit Intranet-Tools und -Plattformen arbeiten, bauen sie technische Fähigkeiten und Selbstvertrauen im Umgang mit ähnlichen Kommunikationswerkzeugen für die externe Kommunikation auf. Dadurch wird die Einführung von Intranetwerkzeugen zu einer Schlüsselstrategie der Organisationsentwicklung. Mitarbeitende verwandeln sich so in authentische Botschafter der digitalen Transformation, interagieren kompetent und auf Augenhöhe mit den Nutzenden und verkörpern eine innovative, nutzerzentrierte Kommunikationsweise:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#einleitung",
    "href": "kommunikation.html#einleitung",
    "title": "Kommunikation",
    "section": "",
    "text": "Stufenmodell von Öffentlichkeit (Stockmann, 2019)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#ziele",
    "href": "kommunikation.html#ziele",
    "title": "Kommunikation",
    "section": "Ziele",
    "text": "Ziele\nDas Stufenmodell macht deutlich, dass der Übergang von einem internen zu einem externen Publikum ein Kontinuum darstellt. Dennoch lassen sich die Ziele interner und externer Kommunikation klar voneinander abgrenzen.\n\nZiele interner Kommunikation\nInterne Kommunikation bezieht sich auf den gezielten Austausch von Informationen, Meinungen und Vorstellungen innerhalb einer Organisation. Sie umfasst sämtliche Kommunikationsprozesse und -instrumente, die darauf abzielen, die Mitarbeiter:innen eines Unternehmens oder einer Einrichtung miteinander und mit der Organisation insgesamt zu vernetzen. Hauptziele sind oft die Sicherstellung eines konsistenten Informationsflusses, die Unterstützung der organisatorischen Ziele und die Förderung einer positiven Unternehmenskultur. Im besten Falle vermeidet gelungene interne Kommunikation Wissensinseln und zu starke Wissensbindung an einzelne Personen.\nWissensmanagement beschreibt hingegen den Prozess des Erfassens, Organisierens, Bewahrens, Anwendens und Weitergebens von Wissen innerhalb einer Organisation. Dieses Wissen kann sowohl explizit (in Dokumenten, Datenbanken etc.) als auch implizit (in den Köpfen der Mitarbeitenden) vorhanden sein. Das Wissensmanagement zielt darauf ab, das im Unternehmen vorhandene Wissen effektiv zu nutzen, um Arbeitsprozesse zu verbessern, Innovationen zu fördern und die Wettbewerbsfähigkeit zu sichern.\nBezogen auf die interne Kommunikation bedeutet dies:\n\nDie interne Kommunikation ist ein zentrales Instrument des Wissensmanagements. Durch gezielte Kommunikationsmaßnahmen wird Wissen im Unternehmen verteilt, Mitarbeitende werden über Neuerungen informiert und der Austausch zwischen den Abteilungen wird gefördert.\nDas Wissensmanagement wiederum stellt sicher, dass die in der internen Kommunikation übermittelten Informationen von Relevanz und Qualität sind und zur richtigen Zeit am richtigen Ort ankommen.\n\nIn der Praxis sind interne Kommunikation und Wissensmanagement oft eng miteinander verknüpft, da eine effektive Kommunikation innerhalb einer Organisation es erleichtert, Wissen zu identifizieren, zu teilen und anzuwenden.\n\n\nZiele externer Kommunikation\nBibliotheken bedienen eine Vielzahl verschiedener Zielgruppen. In den Wissenschaftlichen Bibliotheken sind es Studierende, Lehrende und Wissenschaftler*innen. Öffentliche Bibliotheken bedienen sehr unterschiedliche Bedürfnisse und Gruppen, für die sie jeweils spezielle Angebote bereit halten. Dies gilt es bei der externen Kommunikation stets im Auge zu behalten.\nExterne Kommunikation bezeichnet den systematischen und zielgerichteten Austausch von Informationen, Meinungen und Vorstellungen zwischen einer Organisation und ihren externen Stakeholdern. Dies sind in erster Linie die Nutzenden der Bibliothek, kann aber darüber hinaus auch andere Bibliotheken, Medien oder auch die breitere Öffentlichkeit bzw. die Politik umfassen. Externe Kommunikation kann sowohl zur Imagepflege als auch zur Informationsweitergabe über Produkte, Dienstleistungen oder andere relevante Entwicklungen in der Bibliothek dienen. Je nach Anspruch der Bibliothek kann auch mehrsprachige Kommunikation sinnvoll sein.\nÖffentlichkeitsarbeit (PR) ist ein spezifischer Bereich der externen Kommunikation, der sich darauf konzentriert, das Verständnis und das Image einer Organisation in der Öffentlichkeit zu fördern und zu pflegen. Die Ziele der Öffentlichkeitsarbeit können beinhalten:\n\nAufbau und Pflege eines positiven Images und eines guten Rufs der Organisation in der Öffentlichkeit.\nManagement von Krisensituationen und negativen Entwicklungen, die das Image der Organisation schädigen könnten.\nErhöhung der Sichtbarkeit und Anerkennung der Organisation, ihrer Produkte oder Dienstleistungen.\nEinflussnahme auf die öffentliche Meinung oder politische Entscheidungsfindung.\n\nÖffentlichkeitsarbeit verwendet eine Vielzahl von Kommunikationstools und -strategien, darunter Pressemitteilungen, Veranstaltungen, Social Media, Sponsoring und vieles mehr.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#rahmenbedingungen",
    "href": "kommunikation.html#rahmenbedingungen",
    "title": "Kommunikation",
    "section": "Rahmenbedingungen",
    "text": "Rahmenbedingungen\nDie Einführung digitaler Werkzeuge und Verfahren ist eingebettet in verschiedene rechtliche und organisatorische Rahmenbedingungen:\n\nBeteiligungsverfahren: Etablierung klarer Dienstvereinbarungen und intensiver Dialog mit dem Personalrat und dem Kollegium, um eine transparente und mitarbeiter:innenorientierte Implementierung von digitalen Werkzeugen sicherzustellen.\nDSGVO-Konformität: Alle eingesetzten digitalen Lösungen und Technologien erfüllen die Anforderungen der Datenschutz-Grundverordnung (DSGVO), um den Schutz personenbezogener Daten sicherzustellen. Dies beinhaltet die Prüfung und Klarstellung, ob bei der Nutzung bestimmter Dienste ein Vertrag zur Auftragsverarbeitung (AV-Vertrag) notwendig, ist sowie Aufnahme der Dienste in das Verfahrensverzeichnis.\nBetriebsmodell: Für jedes Werkzeug ist die Entscheidung zu treffen, ob es a) im eigenen Rechenzentrum – so vorhanden – betrieben wird, b) bei einem externen Hosting-Dienstleister aufgesetzt wird oder c) als Software-as-a-Service (SaaS) von Dienstleistern eingekauft wird. Die Entscheidung darüber hängt von der vorhandenen IT-Infrastruktur ab.\nTechnische und organisatorische Maßnahmen (TOMs): Etablierung von TOMs, um sowohl den sicheren Betrieb der technischen Infrastruktur zu garantieren als auch bei möglichen Sicherheitsvorfällen vorbereitet und reaktionsschnell zu sein.\nWeiterbildung und Kompetenzaufbau: Sicherstellung, dass das Bibliothekspersonal durch kontinuierliche Weiterbildungen, Schulungen und Dokumentation in den kompetenten und souveränen Umgang mit den neuen digitalen Werkzeugen eingeführt wird.\nBarrierefreiheit nach BITV 2: Alle digitalen Angebote und Dienste sollen den Anforderungen der Barrierefreie Informationstechnik-Verordnung (BITV 2.0) entsprechen, um einen inklusiven und für alle zugänglichen digitalen Raum zu schaffen, der keine Nutzungsgruppe ausschließt und die Diversität der Gemeinschaft respektiert.\nInstitutioneller Rahmen: Tools in Bibliotheken sind oft abhängig von der Institution bzw. dem jeweiligen Träger. Software, die beispielsweise innerhalb einer Universität angewendet wird, nutzt die Bibliothek dann ebenfalls zur Kommunikation. Beispiele hierfür sind der Rocket-Chat oder Big Blue Button als Videokonferenztool.\n\nEin ausgeprägtes Maß an digitaler Souveränität kann die Auseinandersetzung mit den aufgeführten Rahmenbedingungen erheblich vereinfachen.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#prozesse",
    "href": "kommunikation.html#prozesse",
    "title": "Kommunikation",
    "section": "Prozesse",
    "text": "Prozesse\nFür welche Bibliotheksprozesse interne und externe Kommunikation notwendig ist, wird in den folgenden Abschnitten beschrieben.\n\nMarkenbildung\nAuch in Bibliotheken spielt das Corporate Design eine wichtige Rolle in der Öffentlichkeitsarbeit. Ein einheitliches Corporate Design stellt sicher, dass die visuelle Identität, einschließlich Logos, Farben und Schriftarten, in allen gedruckten und digitalen Materialien konsistent ist. Die Einhaltung des Corporate Designs über alle Kommunikationskanäle hinweg sorgt für ein kohärentes Auftreten der Bibliothek und vermittelt den Eindruck von Stabilität und Seriosität. So stärkt ein gutes Corporate Design unmittelbar das Vertrauen der Nutzenden bzw. der Öffentlichkeit in die Bibliothek.\nGepflegt werden kann ein Corporate Design anhand eines Styleguides. Dieser legt fest, wie visuelle Elemente in verschiedenen Kommunikationskanälen verwendet werden sollen, sei es in gedruckten Broschüren, bei Aushängen oder dem Leitsystem im Bibliotheksgebäude, auf der Website und in sozialen Medien. Ggf. können auch sprachliche Vorgaben gemacht werden. Dies erleichtert die Erstellung von Marketingmaterialien und eine effektive Kommunikation der Bibliotheksangebote. Durch die Verwendung von vordefinierten Vorlagen und Design-Richtlinien können Bibliotheken nicht zuletzt auch die Kosten für die Gestaltung von Materialien senken und Ressourcen effizienter nutzen; das Rad muss (und sollte) nicht immer wieder neu erfunden werden.\nWenn möglich, sollte man Pressematerial wie Logos, standardisierte Infotexte zur Bibliothek für Journalist:innen zum Download auf der Website anbieten, sodass diese schnell auf grundlegende Infos zugreifen können.\nAuch die interne Kommunikation sollte sich entlang der Vorgaben orientieren. Dies spart zum einen Zeit, da Vorlagen genutzt werden können, und übt zum anderen den Gebrauch auch für externe Kommunikation.\n\n\nRedaktion\nFür die Erstellung von internen wie externen Inhalten benötigt man Zeit und Ideen. Um nicht unter „Zugzwang“ zu geraten, hilft es, sich eine Übersicht über die Inhalte zu machen, die man nach außen kommunizieren möchte und zu diesen entsprechende Formate zu entwickeln oder festzulegen. Denn: Meist mangelt es nicht an Themen, vielmehr hat man i.d.R. bereits sehr viele Themen, die nur noch „verpackt“ werden müssen.\nDabei helfen Fragen zum Beispiel nach dem Umfang und der Tiefe des Inhaltes. Nicht jedes Thema eignet sich dafür, ein ausführliches Video zu drehen. Andererseits kann es sein, dass ein Thema nicht in Form eines einzelnen Social-Media-Posts verständlich aufbereitet werden kann. Es geht also darum, sich über die unterschiedlichen medialen Darstellungsformen - Video, Text, Text und Bild, Grafik, etc. - klar zu werden und Themen zuzuordnen.\nAuch innerhalb einer Darstellungsform gibt es unterschiedliche Formate: informative Social Media Posts, ausführliche Blogbeiträge oder verschiedene Videoformate. Gleichzeitig sollten die Voraussetzungen, Möglichkeiten und Konventionen der Plattformen berücksichtigt werden.\nAuch empfiehlt es sich, (grafische) Templates zu nutzen. Beispielsweise können bei *Canva Templates für Infoposts, Veranstaltungshinweise, Personenvorstellung etc. angelegt werden. So müssen die vorgefertigten Templates nur noch mit Inhalten gefüllt werden. Auch bei der Produktion von Videos ist die Arbeit mit Formaten (Interviews, Erklärvideos, Tutorials), nach deren Muster die Videos erstellt werden, von Vorteil.\nEs ist hilfreich bei der Erstellung eines konkreten Inhalts zu überlegen, welche weiteren Formate sich anbieten. So braucht man die inhaltliche Recherche/Arbeit nur einmal machen, kann diese aber in mehrfacher Form und an verschiedenen Stellen wieder aufbereiten und nachnutzen. Wird zum Beispiel ein ausführlicher Blogbeitrag zu einem Thema verfasst, so lassen sich daraus häufig mehrere Social-Media-Posts erstellen. Besonders bei der Produktion eines Videos bietet es sich an, „Hinter den Kulissen“ Content zu produzieren, z. B. indem man den Entstehungsprozess dokumentiert durch kurze Videos und Fotos oder zu einem ausführlichen Video eine Kurzversion dreht - wenn die Technik schon mal steht.\n\nVorproduzieren und Redaktionsplan nutzen:\nNeben tagesaktuellem Content, wie zum Beispiel Hinweisen auf Veranstaltungen, gibt es auch den sogenannten Evergreen Content, zeitlose Themen mit hoher Relevanz für die Zielgruppe. Bei WBs können das z. B. Inhalte zum wissenschaftlichen Publizieren, Forschungsdatenmanagement o. ä. sein, die sich auf verschiedenen Unterseiten des Webauftritts verbergen. Diese lassen sich für andere Plattformen aufbereiten, man kann sie vorproduzieren, beliebig erweitern und mit Hilfe eines Redaktionsplans ausspielen.\nIn einem Redaktionsplan wird festgehalten, welche Inhalte wann (und von wem) publiziert werden. So behält man den Überblick und kann über einen längeren Zeitraum im Voraus planen, vor allem Evergreen Content lässt sich so gut verteilen. Es bietet sich an, eine Regelmäßigkeit für solchen Content festzulegen, wie etwa an einem bestimmten Tag in der Woche o. ä. In ÖBs eignen sich hierfür beispielsweise Buchempfehlungen oder regelmäßig wiederkehrende Veranstaltungen.\nContent anteasern/Social Storytelling:\nVor allem „größerer“ Content, wie zum Beispiel aufwändige Videoproduktionen sollte vorab auf den Social Media Plattformen durch kurze Begleitbeiträge angeteasert werden. Das können etwa Behind-the-scenes-Berichte oder allgemeine Infos zum Thema sein. So kann die Neugier der Nutzenden geweckt werden.\nProzesse optimieren:\nHäufig wird in Bibliotheken die externe Kommunikation von mehr als einer Person übernommen. Zur Strukturierung und zur Absprache innerhalb eines Teams eignen sich verschiedene Kommunikationstools, vom 1:1 Gespräch über eine Chatgruppe bis hin zur Nutzung eines Redaktionsplanes, um gemeinsam Inhalte zu planen. Wichtig ist hierbei Konsistenz und die Nutzung der für den vorliegenden Zweck effektivsten Tools. Dabei sollten nicht zu viele unterschiedliche Kanäle genutzt werden, um nicht den Überblick zu verlieren. Es sollte vorab klar sein, welche Voraussetzungen erfüllt sein müssen, um bestimmte Formate zu kommunizieren. Dazu kann es hilfreich sein, einen Zeitplan sowie eine Checkliste zu erstellen, um sicherzugehen, dass alle relevanten Punkte abgedeckt sind (Wo wird Was Wie kommuniziert).\nSocial Media Planung:\nBetreibt eine Bibliothek mehrere Social Media Kanäle, kann ein einziges Tool zur Planung und Veröffentlichung von Beiträgen sinnvoll sein. Damit kann der Content zeitlich und inhaltlich vorgeplant werden. Integrierte Analysewerkzeuge helfen zudem bei der Zeitplanung, indem die vergangenen Veröffentlichungen ausgewertet werden und der beste Zeitpunkt für die Zielgruppe ermittelt wird. Beispiele für Tools sind:\n\nHootsuite, (kommerziell, in der Basisversion frei nutzbar)\nBuffer, (kommerziell, in der Basisversion frei nutzbar)\nTrello (kommerziell, in der Basisversion frei verfügbar)\nFedica (kommerziell, in der Basisversion frei verfügbar)\n\n\n\n\nWissensmanagement\nExplizites Wissen ist gesichert in Texten, Datenbanken und anderen Dokumenten – doch wie sichern wir das implizite Wissen, das nur in den Köpfen von Mitarbeitenden vorhanden ist? Für diesen Teil des Wissensmanagements gibt es verschiedene mündliche und schriftliche Verfahren. Dabei helfen unter anderem strukturierte Interviews, Erzählungen (Storytelling), Visualisierungen, Verschriftlichung mündlicher Anleitungen, Videobotschaften und nicht zuletzt das Vormachen von Tätigkeiten. Zur kontrollierten Umsetzung des Wissenstransfers gibt es mehrere Werkzeuge (Mittelmann, 2011):\n\nDie Jobmap ist ein Instrument, das dazu dient, das Wissen ausscheidender Mitarbeiter*innen systematisch festzuhalten und zu dokumentieren. Sie kann händisch oder mit Mindmapping-Werkzeugen erstellt werden. Sie kann als Protokoll eines moderierten Gesprächs zwischen Wissensgeber*in (Ausscheidende*r) und Wissensnehmer*in (Nachfolger*in) befüllt oder von Mitarbeiter*innen selbst erstellt werden.\nWissenslandkarten (Knowledge Maps) sind grafische Darstellungen von Wissen in Organisationen. Sie dienen vor allem der Identifikation von Wissen in Unternehmen, verankern graphisch das Wissen von Experten und Teams, Wissensentwicklungsstationen sowie organisationale Fähigkeiten und Abläufe. Es werden nur Verweise, aber nicht das Wissen selbst abgebildet.\nWissensträgerkarten sind Wissenslandkarten für einzelne Personen und veranschaulichen, bei welchen Wissensträger*innen welche Kompetenzen in welchen Wissensgebieten vorhanden sind. Sie vermitteln keine Wissensinhalte, sondern zeigen, welche Kompetenzen ein bestimmter Mitarbeitender hat und wer Wissensträger*in für ein bestimmtes Gebiet ist.\nPersönlicher Wissenstransfer kann in einem Gespräch stattfinden.\nKodifizierter Wissenstransfer wird mittels Informations- und Kommunikationssystemen wie z. B. Datenbanken, Laufwerk, CMS, Wiki erreicht.\nDie Infokarte ist eine Themenkarte oder ein Mikroartikel zur Beschreibung einer Aufgabe.\nEin Projektplan fasst den vollständigen Ablauf des Wissenstransfers zusammen. Er enthält Informationen über die Zeitplanung, den Umfang, die Fälligkeitsdaten und die Ergebnisse.\n\n\n\n\nJobmap nach Mittelmann (2011)\n\n\n\n\nEvaluation\nSind Tools oder Kanäle einmal eingeführt, wird leider oft nicht weiter verfolgt, wie sie angenommen und genutzt werden. Regelmäßige Evaluation der Nutzung und auch Fehlerauswertung helfen dabei, Schwachstellen zu erkennen, Verbesserungen einzubinden und Tools oder Kanäle ggf. auch wieder abzuschaffen bzw. durch andere abzulösen.\nViele Tools und Werkzeuge bringen ihre eigenen Statistiken und Auswertungen mit. Aber auch eine einfache, regelmäßige Abfrage der Nutzungsgewohnheiten der Mitarbeitenden hilft bereits, die Praxistauglichkeit einzuschätzen. Daher ist es sinnvoll, die Evaluation bereits bei der Planung, spätestens aber beim Start mit einzuplanen (siehe Kapitel Anforderungsanalyse.\nNeben herkömmlichen Nutzungsstatistiken (z. B. mittels des Open Source Tools Matomo für Webseiten oder mit integrierten Analysewerkzeugen von Social Media Plattformen und CMS) können einheitliche Fragebögen bei der Einschätzung helfen, ob Kanäle oder Tools geeignet sind, die gesetzten Ziele zu erreichen.\nPlattformen und Kanäle sollten regelmäßig evaluiert werden, um festzustellen, ob die Zielgruppe über diesen Weg tatsächlich erreicht wird.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#werkzeuge",
    "href": "kommunikation.html#werkzeuge",
    "title": "Kommunikation",
    "section": "Werkzeuge",
    "text": "Werkzeuge\nDer bekannte Sinnspruch „Wer als Werkzeug nur einen Hammer hat, sieht in jedem Problem einen Nagel“ gilt auch im digitalen Raum. Die Positionierung zwischen den Extremen „wir bilden alles über das Dateisystem ab“ (ich habe nur einen Hammer) und „wir haben für jede Anforderung ein spezialisiertes Werkzeug“ (man schleppt einen großen Werkzeugkasten durch die Gegend) ist alles andere als trivial.\nEinen ersten Überblick bietet dieses Schaubild, was die populären Werkzeuge entlang der Achsen Intern/Extern sowie Solitär/Kollaborativ positioniert:\n\n\n\nQuadrantenmodell von Kommunikations-Werkzeugen\n\n\nIm Folgenden werden die grundlegenden Funktionen und Werkzeuge kurz beschrieben:\n\nKommunikation\nSynchrone und asynchrone Kommunikation sind zwei verschiedene Ansätze für den Informationsaustausch, die jeweils ihre eigenen Vorteile und Anwendungsgebiete haben. Bei synchroner Kommunikation tauschen die Kommunikationspartner*innen Informationen in Echtzeit aus. Sie erlaubt sofortiges Feedback, d.h. Fragen können direkt gestellt und beantwortet werden. Es müssen allerdings beide Parteien gleichzeitig verfügbar sein. Das kann zu Ablenkungen bzw. Druck führen, da eine sofortige Reaktion erwartet wird. Bei asynchroner Kommunikation findet der Informationsaustausch zeitlich versetzt statt. Eine Person sendet eine Nachricht und die andere antwortet, wenn es passt. Die Kommunikationspartner*innen sind also nicht an eine gemeinsame Zeit gebunden. Das ermöglicht eine größere Flexibilität in der Zeitplanung und eröffnet die Möglichkeit zur gründlichen Reflexion vor einer Antwort. Es ist außerdem oft weniger störend für den Arbeitsfluss. Allerdings kann asynchrone Kommunikation zu Verzögerungen und fehlendes sofortiges Feedback zu Missverständnissen führen.\n\nTelefon ist das klassische Medium mit synchroner 1-zu-1 Kommunikation für schnelle Absprachen; mittlerweile durch die Verbreitung von Videokonferenzen, Chat etc. in der internen Kommunikation eher in den Hintergrund geraten. Mit Voice-over-IP und Weiterleitungen können Telefonnummern auch standortunabhängig genutzt werden.\nE-Mail ist ein Kommunikationsmedium, welches in den meisten Fällen vom Anbieter der IT-Infrastruktur angeboten wird. Ideal ist der Zugang per IMAP/SMTP und nicht nur über proprietäre Protokolle, sodass beliebige Clients genutzt werden können.\nUrsprünglich als elektronische Variante des Briefes für die asynchrone 1-zu-1 Kommunikation entwickelt, wird E-Mail auch häufig für die Gruppenkommunikation zweckentfremdet und teilweise synchron verstanden wenn eine sofortige Antwort erwartet wird. Für beide Einsatzszenarien gibt es bessere Alternativen.\nVideokonferenzsysteme gibt es als eigenständige Tools oder die Videofunktion ist in andere Werkzeuge integriert. Kommerzielle Beispiele sind Microsoft Teams, Webex und Zoom. Daneben stehen freie Alternativen wie jitsi und DFNconf zur Verfügung.\nUm Besprechungsräume so auszustatten, dass Menschen auch per Videokonferenz teilnehmen können, ist technische Infrastruktur erforderlich. Die Anwesenden müssen für die Anwesenden sichtbar und hörbar sein, ohne dass die Anwesenden Headsets tragen müssen. Dies erreicht man mit einer Art überdimensionaler Webcam, die automatisch auf die sprechende Person fokussieren und im Idealfall auch Störgeräusche (Echos, Rauschen, Rascheln) ausblenden. Die gängigen Systeme im knapp vierstelligen Eurobereich genügen für Konferenzen mit bis zu sechs anwesenden Personen um einen Tisch herum. Sind mehr Personen anwesend, steigt der technische Aufwand stark an, wenn man häufig hybrid arbeiten möchte und die anwesenden Personen nicht benachteiligen möchte.\nChat kann als synchrone und asynchrone Kommunikation 1-zu-1 und in Teams sowohl in der externen als auch in der internen Kommunikation verwendet werden:\nChats können in Bibliothekswebsiten (Homepage, OPAC) ohne großen Aufwand integriert werden. Die Nutzung ist anonym oder unter Angabe von personenbezogenen Daten (z. B. E-Mail-Adresse) möglich. Chat-Anfragen werden zentral bearbeitet und können bei Bedarf weitergegeben werden. Ein Pool von Mitarbeitenden kann sich anmelden, die Anfragen werden vom System auf die Mitarbeitenden verteilt. Steht kein Mitarbeitender zur Verfügung, leitet die Software z. B. auf ein Mailformular um. Je nach Verwendung ist der Abschluss eines Vertrages zur Auftragsverarbeitung mit dem Anbieter notwendig. Chat-Systeme werden i.d.R. als SaaS-Lösung angeboten\nChats zwischen Mitarbeiter*innen dienen der niedrigschwelligen Kommunikation, oft als Alternative zu Telefon und E-Mail. Gruppenchats in Abteilungen können für einfache Fragen und Absprachen genutzt werden und Menschen, die mobil arbeiten, können mit einem Chatsystem besser integriert werden. Die Nutzung von kommerziellen Systemen, die noch dazu außerhalb von Europa gehostet werden, ist aus Datenschutzgründen nur in Ausnahmefällen zulässig (WhatsApp, Slack …). Eine Alternative für Bibliotheken mit IT-Abteilung oder Rechenzentrum-Unterstützung ist die Nutzung von Open-Source-Software wie mibew.org oder rocket.chat die selbst betrieben werden können.\nChat ist häufig Bestandteil von integrierten Webkonferenz- und Kommunikationsplattformen. Auch ein Forensystem wie Discourse bietet eine Chat-Komponente, sodass ernsthaft abgewogen werden sollte, ob es wirklich eine dezidierte Chat-Lösung braucht.\nVideokonferenzsysteme gibt es als eigenständige Tools, ein kommerzielles Beispiel ist Zoom. Daneben stehen freie Alternativen wie jitsi und DFN conf zur Verfügung. Eine Chatfunktion während der laufenden Konferenz ist dabei standardmäßig integriert, häufig können auch eine Umfragen benutzt werden.\nIntegrierte Webkonferenz- und Kommunikationsplattformen kombinieren Videokonferenzen mit verschiedenen anderen Kommunikationslösungen. Die kommerziellen Tools Microsoft Teams und Cisco Webex integrieren Chatlösungen, auch außerhalb von Videokonferenzen, für Gruppen und Einzelpersonen. Vor allem im Bildungsbereich wird die Open Source-Plattform BigBlueButton verwendet, die Präsentationen sowie Lern- und Inhaltsverwaltungssysteme. Mattermost ist ein weiteres Open Source-Beispiel, das viele verschiedenen Kommunikationswege vereint und auch Plugins für viele andere Werkzeuge bereitstellt, um eine All-In-One Lösung für Kollaborationen zu sein.\n\nJe technisch anspruchsvoller die Kommunikationswerkzeuge werden, um so mehr muss darauf geachtet werden, die gleichberechtige Teilnahme aller Kommunikationspartner*innen zu ermöglichen (siehe Kapitel Infrastruktur).\n\n\nContenterstellung\nUnter Contenterstellung versteht man das Produzieren von Inhalten (Informationen, Werbung, …) in unterschiedlichen Medienarten und für unterschiedliche Kanäle. Mit Content wird immer eine bestimmte Zielgruppe angesprochen. Inhalte sind zu planen, zu erstellen und zu veröffentlichen.\n\nBild-/Grafikbearbeitung: dient der Nachbearbeitung von Bilddateien (Schärfen, Farben, Dateigröße, Belichtung, Kontraste)\n\nBildbearbeitungs-Software z.B. Affinity Photo, Adobe Photoshop, Canva\nfreie Bilddatenbanken z.B. Pixabay, CC-Search\nGrafikdesign z.B. Canva\n\nAudiobearbeitung: hilft beim Schnitt von Audiodateien und dient der Klangoptimierung mit Effekten wie Equalizer, Kompressor oder Hall\n\nAudiobearbeitungs-Software z.B. Audacity\nProfessionelle Podcast-Produktion z.B. Ultraschall\n\nMit einer Aufnahmesoftware wird Videomaterial über Kameras aufgenommen oder zu Bildschirmaufzeichnungen (Screencasts) gemacht. Je nach Aufnahmegerät wird der Ton gleichzeitig aufgenommen oder muss separat eingespielt werden. Beispiele für Screencast-Software sind Camtasia, Screenflow und OBS. OBS kann zudem verwendet werden, um Inhalte auf Videoplattformen zu streamen.\nZur Nachbearbeitung von Roh-Videomaterial gibt es spezielle Videobearbeitungssoftware, die Video- und Tonspuren zusammenführen können. Beim Videoschnitt können verschiedene Sequenzen aus Quellmaterialien aneinandergereiht werden. Ebenso sind Veränderungen durch Filter oder Zusammenführen von verschiedenen Bild- und Videoaufnahmen möglich. In der Regel bieten die Tools diverse Exportmöglichkeiten in verschiedenen Qualitäten und Formaten. Beispiele für Bearbeitungssoftware sind DaVinci Resolve, Camtasia und SimpleShow. Letzteres dient insbesondere zur Erstellung von Lernvideos mit einzelnen Folien, auf denen ein Skript anhand von Schlagwörtern von der Software mit einfachen Grafiken illustriert wird.\nHardware: Bei der Anschaffung von Hardware für Video- oder Podcastsaufnahmen sowie die Erstellung von Video-Tutorials in Bibliotheken ist es wichtig, Qualität und Funktionalität im Auge zu behalten. Mit der richtigen Ausrüstung können Bibliotheken professionelle Ergebnisse erzielen. Die Anschaffung muss dabei nicht zwangsläufig teuer sein. Es lohnt sich immer, in eine hochauflösende externe Kamera zu investieren. Auch ein qualitativ hochwertiges Mikrofon bzw. Headset ist unerlässlich für einen überzeugenden Klang. Die in Standard-Laptops integrierten Kameras und Mikrofone genügen üblicherweise nicht, um eine halbwegs professionelle Ton- und Bildqualität zu erzeugen. Bei geringem Budget ist die Hardware in einem guten Smartphone besser geeignet. Hilfestellung - für nahezu jeden Budgetrahmen - erhält man etwa im Podcast-Forum Sendegate oder im Wiki des Netzwerkes „Tutorials in Bibliotheken“\nUmfragen/Abstimmungen: können mittels verschiedener Tools erstellt und direkt online veröffentlicht werden (z.B. mit SurveyMonkey, Limesurvey, Mentimeter, Lamapoll).\nE-Learning-Software: dient der Erstellung von Lernformen, die durch elektronische, technische oder digitale Medien unterstützt werden (z.B. Capterra und Moodle). Die Inhalte können auch als eigenständige Open Education Ressources (OER) publiziert und nachgenutzt werden.\nGamification: ist die Anwendung spielerischer Elemente im spielfremden Kontext. So können beispielsweise mit Actionbounds Multimedia-Guide für interaktive Stadtralleys und Handy-Schnitzeljagden auch für den Bibliothekskontext erstellt werden. Eine spielbasierte Lernplattform für Quizze ist Kahoot.\n\n\n\nEchtzeit-Kollaboration\nKollaborative Online-Tools ermöglichen es Teams, in Echtzeit an Dokumenten, Tabellen und Präsentationen zu arbeiten und diese gemeinsam zu nutzen. Sie können die Zusammenarbeit innerhalb einer Bibliothek (und darüber hinaus) deutlich beflügeln und effizienter gestalten. Dies gilt insbesondere, wenn man es mit dem oftmals mühsamen Zusammenführen von Dokumentversionen, die in klassischen Office-Lösungen in Einzelarbeit entstanden sind, vergleicht. Das Spektrum reicht dabei vom digitalen Notizzettel bis zum gemeinsam erarbeiteten Förderantrag, vom Brainstorming bis zur fertig ausgearbeiteten Präsentation.\n\nFür den ad hoc Einsatz bietet sich HedgeDoc als Open-Source-Tool für kollaboratives Schreiben in Echtzeit mit Markdown-Unterstützung an.\nFür die gemeinsame Bearbeitung von klassischen Office-Formaten sind OnlyOffice und Collabora veritable Alternativen zu Google Docs, insbesondere unter Gesichtspunkten der digitalen Souveränität.\nCloudspeicher sind die zeitgemäße Variante von Netzwerklaufwerken z. B. Nextcloud bzw. ownCloud die gut mit OnlyOffice und Collabora zusammenspielen.\nOnline-Whiteboards sind digitale Tools, die ein traditionelles physisches Whiteboard simulieren. Sie ermöglichen es, in Echtzeit gemeinsam zu brainstormen, indem etwa Post-Its in den virtuellen Raum geklebt werden, aber auch um zu zeichnen und multimediale Inhalte zu integrieren. Damit eignen sie sich gut sowohl für die kreative Zusammenarbeit innerhalb der Bibliothek als auch für die Durchführung von Workshops und anderen Formaten mit Nutzenden. Verbreitet sind Conceptboard und miro, ernstzunehmende offene Alternativen gibt es leider noch nicht.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#kanäle",
    "href": "kommunikation.html#kanäle",
    "title": "Kommunikation",
    "section": "Kanäle",
    "text": "Kanäle\nEs gibt verschiedene Online-Kanäle, mit Hilfe derer man die Nutzenden informieren kann. Diese werden in den folgenden Abschnitten beschrieben.\n\nWebsite\nDie Website wird im allgemeinen Sprachgebrauch häufig auch als Homepage bezeichnet, im eigentlichen Sinne ist damit aber nur die Startseite eines Internetauftritts gemeint. Die Website dient der Vorstellung der Bibliothek, ihrer verschiedenen Angebote und Kontaktmöglichkeiten. Hier werden u.a. Neuigkeiten über Veranstaltungen, digitale Angebote oder Kundenservices (z. B. Anmeldemodalitäten) präsentiert.\nWebsites werden meistens auf Basis eines Content Management Systems (CMS) erstellt. Mit Hilfe von Content Management Systemen können redaktionelle Inhalte dargestellt, organisiert und bearbeitet werden, ohne dass sich die Websiteredakteur*innen mit dem Aufbau und dem Design der Website beschäftigen müssen.\nÖffentliche Bibliotheken sind oft Teil der kommunalen Infrastruktur, Hochschulbibliotheken Teil der Universitätsinfrastruktur. In diesen Fällen ist die Bibliotheksseite häufig als Unterseite angelegt. Hier ist das CMS vorgegeben und der Gestaltungsspielraum begrenzt; es sollte darauf geachtet werden, dass die Grundfragen und -dienste mit einer solch vorgefertigten Variante umgesetzt werden können. Alternativ kann es auch sein, dass allein ein Webserver als technische Basis bereitgestellt wird und sich die Bibliothek selbst um die Umsetzung einer Webseite kümmern muss. Im besten Fall hat die Bibliothek eine eigene Domain, kann ein eigenes CMS auswählen und hat damit auch die Gestaltungsfreiheit, beliebige Unterseiten anzulegen und eigene Inhalte zu beschreiben.\nIn beiden Fällen - ob eigene Domain oder eingegliedert in die übergeordnete Einrichtung - gibt es in der Regel Designvorgaben wie eine Corporate Identity. Dennoch sollten insbesondere bei der Planung und späteren Konzeption gängige Usability-Gesichtspunkte berücksichtigt werden.\nFolgende Basisinformationen für Bibliotheksbesucher*innen und Interessierte sollten zu finden sein\n\nÖffnungszeiten, Anmeldemodalitäten, Gebühreninformationen\nVeranstaltungshinweise\nDigitale Angebote\nFachspezifische Angebote wie medienpädagogische Inhalte\nVerlinkung zu Bibliotheks-OPAC oder anderen Online-Katalogen\n\nSchließlich muss darauf geachtet werden, dass nicht nur die eigentliche Webseite der Einrichtung die Basisinformationen enthält, sondern auch, dass diese in einschlägigen Suchmaschinen indexiert sind, etwa die Adresse und Öffnungszeiten in einer Google Suche und auf Google Maps. Hierzu müssen entsprechende SEO (Search Engine Optimization) Parameter eingestellt bzw. an die jeweiligen Plattformen übermittelt werden.\nGängige CMS sind Typo3, Wordpress oder Drupal. Ob eine CMS vorgegeben ist oder selbst festgelegt werden kann, ist von Institution zu Institution unterschiedlich und von den Rahmenbedingungen abhängig. Das gleiche gilt für das Hosting und den technischen Support der Website. Dies kann entweder hausintern passieren, über einen Verbundpartner oder bei einem kommerziellen Drittanbieter.\nIst eine Bibliothek in der komfortablen Lage, ein eigenes CMS auszuwählen, sollten auch immer die anzubindenden Portale wie Discovery-Systeme oder Repositorys berücksichtigt werden.\n\n\nBlog\nEin Blog ist eine spezielle Form einer Website und dient in erster Linie als Plattform für die Veröffentlichung redaktioneller (Text-)Beiträge. Blogbeiträge eignen sich vor allem für die Darstellung von Inhalten, die ausführlicher und tiefer in eine Thematik einsteigen und über den rein informativen Charakter hinausgehen. Ein Blog kann einerseits als ein Bereich innerhalb eines Webauftritts angesiedelt werden oder als eigene Website existieren. Für das Aufsetzen eines eigenständigen Blogs eignet sich vor allem das CMS Wordpress.\n\n\nSocial Media\nÜber Social-Media-Kanäle kann die Zielgruppe schnell erreicht werden, es lassen sich eigene Inhalte mit Nutzenden teilen und kann mit ihnen in Kontakt getreten werden. Auch dienen Social-Media-Kanäle der Markenbildung. Jede Social-Media-Plattform hat eigene Konventionen und Bedingungen, sowohl was die Gestaltung der Beiträge angeht als auch die zugrunde liegenden Algorithmen, mit denen man sich vertraut machen sollte. Ziel sollte es sein, möglichst viele Nutzende zu erreichen und neue Nutzende dazu zu gewinnen. Um dieses Ziel zu erreichen, ist es wichtig, nicht nur die eigenen Kanäle zu bespielen, sondern auch mit anderen Kanälen zu interagieren.\nJe nach Zielgruppe sind einzelne Social-Media-Kanäle mehr oder weniger geeignet. Ist eine Zielgruppe auf einem Netzwerk nicht aktiv, so lohnt es sich nicht, dieses Netzwerk zu bespielen und man sollte sich eher auf die Plattformen fokussieren, in denen die eigene(n) Zielgruppe(n) anzutreffen sind.\n\nMastodon, Bluesky, Threads…: Diese Plattformen für Kurznachrichten mit vor allem textbasierten Inhalten werden auch gerne genutzt, um auf Inhalte auf anderen Plattformen (Fachartikel, Blogbeiträge, Veranstaltungen..) zu verweisen. Für das deutschsprachige Bibliothekswesen ist insbesondere die Mastdon-Instanz openbiblio.social etabliert.\nFacebook, Instagram: Inhalte in Form von Bildern/Grafiken, Texten, (Kurz-)Videos\nTikTok: Videoportal für kurze Videos im Hochformat\nYouTube, Vimeo: Plattformen für die Veröffentlichung von Videos\n*Fediverse: Netzwerk unterschiedlicher dezentraler Dienste, die mit dem gleichen Account genutzt werden können (siehe FediDB für eine statistische Übersicht; Mastodon ist sicherlich der bekannteste davon, aber auch das CMS WordPress gehört dazu, so dass sich WordPress-Beiträge in den unterschiedlichen Fediverse-Kanälen teilen lassen.\nLinkedIn: Das ursprüngliche Berufsnetzwerk hat sich inzwischen zu einer allgemeinen Plattform entwickelt, auf der Inhalte geteilt und diskutiert werden. Vor allem für Themen aus Wissenschaft und Forschung eignet sich LinkedIn als Plattform. Allerdings ist hier zu beachten, dass persönliche Accounts vom Algorithmus bevorzugt behandelt werden im Gegensatz zu Unternehmensseiten, sodass über persönliche Accounts geteilte Inhalte eine deutlich höhere Reichweite erzielen.\n\n\n\nNewsletter\nNewsletter sind redaktionell aufbereitete Texte und Grafiken, die an einen bestimmten Verteilerkreis (Studierende, Kund*innen) in einem regelmäßigen Rhythmus versendet werden. Ein Beispiel sind Veranstaltungsnewsletter der ÖBs. Dabei ist insbesondere der Datenschutz zu beachten. Vor dem Versand ist eine Einwilligung jedes Empfängers einzuholen, der durch Double-Opt-In verifiziert wurde. Es ist ratsam, für den Versand einen Newsletter-Dienst zu nutzen, um technische Fallstricke zu umgehen. Diese sind u.a. auf große Versandmengen, Umgang mit zurückkommenden Mails oder Einwilligungsverwaltung spezialisiert.\n\n\nMailinglisten und Foren\nMailinglisten dienen primär der Verteilung von Inhalten an Gruppen von Personen, sie werden aber auch zu allgemeinen Diskussionen genutzt. Beispiele für Mailinglisten aus dem Bibliotheksbereich sind bibnez und Forum ÖB. Ein fachliches Forum für Austausch zu Metadaten und andere bibliothekarische IT-Themen ist metadaten.community.\n\n\nOffene Schnittstellen und Daten\nOffene Daten verändern die Wissens- und Wissenschaftskommunikation auch in Bibliotheken. Frei lizensierte und ggf. über Schnittstellen wie RSS und ActivityPub (Fediverse) bereitgestellte Daten können verbreitet und in neuen digitalen Publikationen und Social Media verknüpft werden. Offene Zitationsdaten ermöglichen bibliografische Analysen und Datenvisualisierungen, die selbst als Gegenstand in der internen und externen Wissenschaftskommunkation genutzt werden können. Dabei ermöglichen offene Werkzeuge wie Wikidata gewissermaßen eine „Demokratisierung“ der Metadatenproduktion und -pflege unabhängig bzw. in Ergänzung zu traditionellen Bibliothekssystemen. Wikidata fungiert dabei zudem als „Linked-Open-Data“-Knoten (Datenhub) für Identifikatoren verschiedener Datenquellen, die jeweils identische Entitäten beschreiben.\n\n\nDigitale Anzeigen/Poster\nUnter digitalen Anzeigen werden alle Arten von elektronischen Displays verstanden, die Informationen zu Institutionen oder Angeboten und Veranstaltungen anzeigen. Beispiele sind Infotafeln und Wegweiser.\n\n\nWiki\nEin Wiki ist eine webbasierte Sammlung von Informationen, Artikeln oder Beiträgen zu bestimmten Themen. Diese können von Nutzenden selbst bearbeitet werden. Es kann frei im Internet oder nur für einen festgelegten Nutzerkreis (z.B. Mitarbeitende) verfügbar sein. Verbreitete Beispiele für Wiki-Software sind MediaWiki, PmWiki, Xwiki und Confluence.\n\n\nSocial Intranet\nBei der Vielzahl vorhandener Werkzeuge und Methoden besteht die Gefahr, nach und nach eine immer weiter fragmentierte Softwarelandschaft zu erzeugen mit entsprechend hohen Anforderungen an Betrieb, Dokumentation und Schulungen. Ein sinnvoller Ansatz ist es deshalb, zunächst das Verhältnis der verschiedenen Lösungen auf konzeptioneller Ebene klar festzulegen, um Redundanzen zu vermeiden. Dieses Konzept kann sich, soweit möglich, aber auch in einer technisch integrierten Lösung für möglichst viele der genannten Aspekte widerspiegeln: einem so genannten Social Intranet einem typischen internen Infrastruktur-Angebot.\n\n\n\n\n\n\nInfo\n\n\n\nEin Social Intranet ist ein internes, web-basiertes Netzwerk, das speziell darauf ausgelegt ist, die Kommunikation, Zusammenarbeit und Informationsverbreitung innerhalb einer Organisation zu fördern. Es kombiniert die klassische Funktion eines Intranets, die Informationserstellung und -bereitstellung, mit eher sozialen, kommunikativen Funktionen und ermöglicht den Mitarbeitenden so den einfachen Austausch von Informationen und Ideen.\n\n\nEin gut geplantes und eingerichtetes Social Intranet ist in der Lage, die meisten der oben genannten Aspekte und Ideen abzudecken. Nicht immer in der vollen Funktionstiefe, dafür aber in einer einheitlichen, gut nutz- und administrierbaren Oberfläche. Auch sonst nur aufwändig implementierbare Features wie eine Volltextsuche über alle Materialien wird problemlos möglich.\nUm im Bild vom Anfang des Kapitels zu bleiben: ein Social Intranet ist weder ein Hammer noch ein schwerer Werkzeugkasten, sondern am ehesten ein Schweizer Taschenmesser.\nKern der Idee eines Social Intranets ist, dass neben Dokumenten die Menschen und deren Austausch untereinander im Fokus stehen. Daher kann jede*r Mitarbeiter*in ein Profil pflegen, um seine*ihre Rollen, Fachgebiete und Kontaktinformationen zu teilen. Je nachdem, ob eher die sozialen Features, die themenspezifischen Diskussionsräume oder die Informationsbereitstellung im Vordergrund stehen, bieten sich zur Implementierung Wiki-Software (etwa: Confluence, kommerziell) oder Foren-Software (etwa: Discourse, Open Source) an.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "kommunikation.html#zusammenfassung-und-ausblick",
    "href": "kommunikation.html#zusammenfassung-und-ausblick",
    "title": "Kommunikation",
    "section": "Zusammenfassung und Ausblick",
    "text": "Zusammenfassung und Ausblick\nDie externe Kommunikation dient dazu, Beziehungen zu externen Stakeholdern zu pflegen. Dafür werden Webseiten, E-Mail-Marketing, Social Media, Medienproduktionstools sowie weitere Kanäle und Werkzeuge genutzt. Die interne Kommunikation konzentriert sich auf den Informationsaustausch innerhalb einer Organisation. Hier kommen Tools wie Social Intranets, Kollaborationstools und Videokonferenzsysteme zum Einsatz. Die aktuellen Entwicklungen im Bereich der Künstlichen Intelligenz mit Large Language Models (LLM) haben das Potenzial, die Art und Weise, wie Organisationen sowohl intern als auch extern kommunizieren, tiefgreifend zu verändern. Sie können bspw. in Chatbots und andere Support-Systeme integriert werden, um Anfragen von Nutzenden - zumindest initial - in Echtzeit zu beantworten und so Mitarbeitende zu entlasten. Auch bei der Erstellung von Content können sie helfen, diesen schneller und kohärenter zu gestalten. Im Bereich des Wissensmanagements könnten Mitarbeiter*innen LLMs nutzen, um spezifische, auch komplexe Fragen zu beantworten oder Hintergrundinformationen zu bestimmten Themen zu erhalten, ohne dabei ständig auf menschliche Experten zurückgreifen zu müssen. Es ist jedoch wichtig, dabei ethische Überlegungen und Fragen zur Datenvertraulichkeit zu berücksichtigen, insbesondere wenn LLMs in sensiblen Kommunikationsbereichen eingesetzt werden.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kommunikation</span>"
    ]
  },
  {
    "objectID": "literatur.html",
    "href": "literatur.html",
    "title": "Literaturverzeichnis",
    "section": "",
    "text": "Hier nur die explizit im Handbuch referenzierten Publikationen. Die vollständige Bibliografie wird in einer gemeinsamen Zotero-Gruppe verwaltet.\n\n\nAssfalg, Rolf. 2023. “Metadaten.” In Grundlagen Der\nInformationswissenschaft, edited by Rainer Kuhlen, Dirk\nLewandowski, Wolfgang Semar, and Christa Womser-Hacker, 7. Ausgabe,\n245–56. Berlin, Boston: De Gruyter Saur. https://doi.org/10.1515/9783110769043-021.\n\n\nAzeroual, Otmane. 2021. “Untersuchungen Zur Datenqualität Und\nNutzerakzeptanz von Forschungsinformationssystemen.” https://doi.org/10.25673/45118.\n\n\nBach, Nicolas. 2022. “Das Handbuch IT in\nBibliotheken: Einblicke in den ersten bibliothekarischen Book Sprint\nDeutschlands.” Informationspraxis 8 (1). https://doi.org/10.11588/ip.2022.1.94475.\n\n\nBarker, Michelle, Neil P. Chue Hong, Daniel S. Katz, Anna-Lena\nLamprecht, Carlos Martinez-Ortiz, Fotis Psomopoulos, Jennifer Harrow, et\nal. 2022. “Introducing the FAIR Principles for\nResearch Software.” Scientific Data 9 (1): 622. https://doi.org/10.1038/s41597-022-01710-x.\n\n\nBerg-Weiß, Alexander, Sibylle Hermann, Miriam Kötter, Caroline Leiß,\nChristoph Müller, and Annette Strauch-Davey. 2022. “Openness in\nBibliotheken : Positionspapier der Kommission für forschungsnahe Dienste\ndes VDB.” o-bib. Das offene Bibliotheksjournal /\nHerausgeber VDB 9 (2): 1–4. https://doi.org/10.5282/o-bib/5826.\n\n\nBorgman, Christine L. 1997. “From Acting Locally to Thinking\nGlobally: A Brief History of Library Automation.” The Library\nQuarterly: Information, Community, Policy 67 (3): 215–49. https://www.jstor.org/stable/40039721.\n\n\nBreeding, Marshall. 2022. “How to Secure Library Systems from\nMalware, Ransomware, and Other Cyberthreats.” 2022. https://www.infotoday.com/cilmag/jan22/Breeding--How-to-Secure-Library-Systems-From-Malware-Ransomware-and-Other-Cyberthreats.shtml.\n\n\nBreeding, Marshall. n.d. “Library Technology Industry Mergers and\nAcquisitions. Library Technology Guides.” Accessed April 28,\n2022. http://librarytechnology.org/mergers/.\n\n\nChristensen, Anne. 2022. “Wissenschaftliche Literatur entdecken:\nWas bibliothekarische Discovery-Systeme von der Konkurrenz lernen und\nwas sie ihr zeigen können.” LIBREAS, no.\n41. https://doi.org/10.18452/24798.\n\n\nChristensen, Anne, and Frank Seeliger. 2022. “„Wie Schreiben Wir\nGemeinsam Ein Nützliches Buch?”.” B.i.t.online 25 (6):\n509–10. https://www.b-i-t-online.de/heft/2022-06-nachrichtenbeitrag-christensen.pdf.\n\n\nCyra, Magdalene Alice, Marius Politze, and Henning Timm. 2022. “A\npush for better RDM: Erfahrungsbericht aus dem Einsatz von\ngit für Forschungsdaten.” Bausteine\nForschungsdatenmanagement, no. 2 (August). https://doi.org/10.17192/bfdm.2022.2.8435.\n\n\nDFG. 2021. “Datentracking in Der Wissenschaft: Aggregation Und\nVerwendung Bzw. Verkauf von Nutzungsdaten Durch Wissenschaftsverlage.\nEin Informationspapier Des Ausschusses Für Wissenschaftliche\nBibliotheken Und Informationssysteme Der Deutschen\nForschungsgemeinschaft.” Deutsche Forschungsgemeinschaft. https://www.dfg.de/download/pdf/foerderung/programme/lis/datentracking_papier_de.pdf.\n\n\nDIN. 2020. “DIN EN ISO\n9241-110 Ergonomie Der Mensch-System-Interaktion - Teil 110:\nInteraktionsprinzipien (ISO 9241-110:2020).” https://www.din.de/de/mitwirken/normenausschuesse/naerg/veroeffentlichungen/wdc-beuth:din21:320862700.\n\n\nDruskat, Stephan, Oliver Bertuch, Guido Juckeland, Oliver Knodel, and\nTobias Schlauch. 2022. “Software Publications with Rich Metadata:\nState of the Art, Automated Workflows and HERMES\nConcept.” arXiv. https://doi.org/10.48550/arXiv.2201.09015.\n\n\nForschungsinformationssysteme, Dini Ag. 2022. Management von\nForschungsinformationen in Hochschulen Und Forschungseinrichtungen.\nHumboldt-Universität zu Berlin. https://edoc.hu-berlin.de/handle/18452/26130.\n\n\nFreyberg, Linda, and Sabine Wolf, eds. 2019. Smart Libraries:\nKonzepte, Methoden Und Strategien. B.i.t.online Innovativ 76.\nWiesbaden: b.i.t. Verlag.\n\n\nGesetzliche, Deutsche, and Unfallversicherung e.V. (DGUV). 2019.\n“Bildschirm- Und Büroarbeitsplätze: Leitfaden Für Die\nGestaltung.” https://publikationen.dguv.de/widgets/pdf/download/article/409.\n\n\nGould, J. D., and C. Lewis. 1987. “Designing for Usability: Key\nPrinciples and What Designers Think.” In Human-Computer\nInteraction: A Multidisciplinary Approach, 528–39. San Francisco,\nCA, USA: Morgan Kaufmann Publishers Inc.\n\n\n“Government Design Principles.\nGOV.UK.” 2012. 2012. https://www.gov.uk/guidance/government-design-principles.\n\n\nGrossmann, Yves Vincent, and Michael Franke. 2023. “Software ist\nkein Beiprodukt!”\n\n\nHanson, Cody. 2015. “Opinion: Libraries Are Software.”\n2015. https://www.codyh.com/writing/software.html.\n\n\nHolländer, Stephan. 2023. “Cyberangriffe Auf Universitäten,\nFachhochschulen Und Deren Bibliotheken – Ein Unterschätztes\nProblem?” B.I.T. Online 26 (3). https://www.b-i-t-online.de/heft/2023-03-nachrichtenbeitrag-hollaender.pdf.\n\n\nJakob Nielsen. 2000. “Why You Only Need to Test with 5 Users.\nNielsen Norman Group.” 2000. https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/.\n\n\nJetter, Hans-Christian. 2023. “Informationsvisualisierung Und\nVisual Analytics.” In Grundlagen Der\nInformationswissenschaft, edited by Rainer Kuhlen, Dirk\nLewandowski, Wolfgang Semar, and Christa Womser-Hacker, 7. Ausgabe,\n295–306. Berlin, Boston: De Gruyter Saur. https://www.degruyter.com/document/doi/10.1515/9783110769043-025/pdf.\n\n\nKern, Christian. 2011. RFID Für Bibliotheken.\nSpringer.\n\n\n———. 2014. “Die Datenmodellstandardisierung Und Ihre Auswirkungen\nAuf RFID-Bibliotheken.” In RFID Für\nBibliothekare: Ein Vademecum, edited by Frank Seeliger, 3. Auflage.\nVerlag News & Media. https://core.ac.uk/download/pdf/33985396.pdf.\n\n\nKling, Rob, and Susan Leigh Star. 1998. “Human Centered Systems in\nthe Perspective of Organizational and Social Informatics.”\nACM SIGCAS Computers and Society 28\n(1): 22–29. https://doi.org/10.1145/277351.277356.\n\n\nKluge, Matthias. 2022. Anbieter von Bibliothekssoftware.\nLandesfachstelle für das öffentliche Bibliothekswesen. https://www.oebib.de/bau-einrichtung-it/it-und-internet/bibliothekssoftware.\n\n\nKonrad, Uwe, Konrad Förstner, Johannes Reetz, Klaus Wannemacher, Jürgen\nKett, and Florian Mannseicher. 2020. “Digitale Dienste Für Die\nWissenschaft - Wohin Geht Die Reise?” December. https://doi.org/10.5281/zenodo.4301924.\n\n\nKost, Michael, Bastian Loibl, Peter Reuter, and Matthias Stenke. 2022.\n“#JLUoffline. Der Cyber-Angriff Auf Die\nJustus-Liebig-Universität Gießen Im Dezember 2019.”\nABI Technik 42 (1). https://doi.org/https://doi.org/10.1515/abitech-2022-0005.\n\n\nKuhlen, Rainer, Dirk Lewandowski, Wolfgang Semar, and Christa\nWomser-Hacker, eds. 2023. Grundlagen Der\nInformationswissenschaft. 7. Ausgabe. Berlin, Boston: De Gruyter\nSaur. https://doi.org/doi:10.1515/9783110769043.\n\n\nMatthews, Joseph R., and Carson Block. 2020. Library Information\nSystems. Second edition. Library and Information Science Text\nSeries. Santa Barbara, California: Libraries Unlimited.\n\n\nMDR. 2023. “Vermehrt Hackerangriffe Auf Hochschulen Und\nUniversitäten.” January 25, 2023. https://www.mdr.de/wissen/vermehrt-hackerangriffe-auf-hochschulen-und-universitaeten-100.html.\n\n\nMichaelis, Barbara. 2014. In RFID Für Bibliothekare:\nEin Vademecum, edited by Frank Seeliger, 3. Auflage, 145–50. Verlag\nNews & Media. https://doi.org/10.15771/RFID_2014_13.\n\n\nMüller, Heiko. 2023. “Vorsorge Für Den Angriffsfall: Der Weg in\nDie Cloud.” iX Magazin Für Professionelle\nIT 11.\n\n\nPutnings, Markus, Heike Neuroth, and Janna Neumann, eds. 2021.\nPraxishandbuch Forschungsdatenmanagement. De Gruyter Saur. https://doi.org/10.1515/9783110657807.\n\n\nRehm, Stefan-Marc. 2023. “IT-Sicherheitsbeauftragten\nBestellen. Www.haufe.de.” August 23, 2023. https://www.haufe.de/compliance/management-praxis/cybersicherheit-it-sicherheitsbeauftragter_230130_447256.html.\n\n\nRölke, Heiko, and Albert Weichselbraun. 2023. “Ontologien Und\nLinked Open Data.” In Grundlagen Der\nInformationswissenschaft, edited by Rainer Kuhlen, Dirk\nLewandowski, Wolfgang Semar, and Christa Womser-Hacker, 7. Ausgabe,\n257–70. Berlin, Boston: De Gruyter Saur. https://www.degruyter.com/document/doi/10.1515/9783110769043-022/pdf.\n\n\nSchirrwagen, Jochen. 2022. “Repositorien und\nForschungsinformationssysteme bilden keine Dichotomie.”\nBibliothek Forschung und Praxis 46 (2): 284–88. https://doi.org/10.1515/bfp-2022-0013.\n\n\nSchmidt, Caroline. 2023. “Wann Die Bestellung Eines\nDatenschutzbeauftragten Für Ihr Unternehmen Unabdingbar Ist.\nWww.e-Recht24.de.” June 12, 2023. https://www.e-recht24.de/datenschutz/10744-datenschutzbeauftragter-dsgvo.html#.\n\n\nSchweitzer, Roswitha. 2016. “Anforderungen an Ein\nBibliothekssystem Der Neuen Generation - Der Kriterienkatalog von Hbz\nUnd VZG.” Köln. https://docplayer.org/61296444-Anforderungen-an-ein-bibliothekssystem-der-neuen-generation.html.\n\n\nSeeliger, Frank, ed. 2014. RFID Für Bibliothekare: Ein\nVademecum. 3. Auflage. Verlag News & Media. https://doi.org/10.15771/978-3-936527-32-2.\n\n\n———. 2019. “Smart Services Als Marketinginstrument.” In\nPraxishandbuch Informationsmarketing: Konvergente Strategien,\nMethoden Und Konzepte, edited by Frauke Schade and Ursula Georgy,\n343–57. De Gruyter Saur. https://doi.org/10.1515/9783110539011-023.\n\n\nShneiderman, Ben, and Catherine Plaisant. 2005. Designing the User\nInterface. Strategies for Effective Human-Computer Interaction. 4th\ned. Pearson.\n\n\nSiems, Renke. 2022. “Das Lesen der Anderen: Die Auswirkungen von\nUser Tracking auf Bibliotheken.” o-bib. Das offene\nBibliotheksjournal / Herausgeber VDB 9 (1): 1–25. https://doi.org/10.5282/o-bib/5797.\n\n\nSteilen, Gerald. 2012. “Discovery-Systeme - Die OPACs\nDer Zukunft?” Hamburg. https://www.slideshare.net/steilen/discoverysysteme-die-opacs-der-zukunft.\n\n\nStille, Wolfgang, Stefan Farrenkopf, Sibylle Hermann, Gerald Jagusch,\nCaroline Leiß, and Annette Strauch-Davey. 2021.\n“Forschungsunterstützung an Bibliotheken: Positionspapier der\nKommission für forschungsnahe Dienste des VDB.”\no-bib. Das offene Bibliotheksjournal / Herausgeber\nVDB 8 (2): 1–19. https://doi.org/10.5282/o-bib/5718.\n\n\n“Tracking in Der Wissenschaft: So Können Bibliotheken Daten Und\nWissenschaftsfreiheit Schützen.” 2022. https://www.zbw-mediatalk.eu/de/2022/01/tracking-in-der-wissenschaft-so-koennen-bibliotheken-daten-und-wissenschaftsfreiheit-schuetzen.\n\n\n“UHF RFID Almanach 2022.” 2022.\nEECC. https://eecc.info/rfidalmanach.html.\n\n\nVoß, Jakob. 2021. “Datenqualität als Grundlage qualitativer\nInhaltserschließung.” In Qualität in der\nInhaltserschließung, 167–76. De Gruyter Saur. https://doi.org/10.1515/9783110691597-010.\n\n\n———. 2022. “Einführung in Die Verarbeitung von\nPICA-Daten.” 2022. https://pro4bib.github.io/pica/.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literaturverzeichnis</span>"
    ]
  },
  {
    "objectID": "glossar.html",
    "href": "glossar.html",
    "title": "Anhang A — Abkürzungsverzeichnis",
    "section": "",
    "text": "APC\n\nArticle processing charge, gebräuchlich auch als publication fee\n\nAPI\n\nApplication Programming Interface, Programmierschnittstelle\n\nBMS/LMS\n\nBibliotheksmanagementsystem, gebräuchlich auch als Library Management System\n\nBSZ\n\nBibliotheksservice-Zentrum Baden-Württemberg, zentrale Einrichtung des Südwestdeutscher Bibliotheksverbund (SWB)\n\nCBS\n\nKatalogdatenbank von OCLC, wird insbesondere für den Verbundkatalog K10plus verwendet\n\nCMS\n\nContent-Management-System\n\nCOUNTER\n\nCounting Online Usage of NeTworked Electronic Resources\n\nDDC\n\nDewey-Dezimalklassifikation, Klassifikation für die Sacherschließung von Bibliotheksbeständen\n\nDMP\n\nDatenmanagementplan\n\nDNS\n\nDomain Name System. Auflösung von Server-Namen zu IP-Adressen\n\nDOI\n\nDigital Object Identifier, eindeutiger und dauerhafter digitaler Identifikator für Objekte\n\nEDIFACT\n\nElectronic Data Interchange for Administration, Commerce and Transport\n\nELN\n\nelektronisches Laborbuch\n\nERM\n\nElectronic Resource Management, Verwaltung von Lizenzinformationen zu E-Ressourcen\n\nERP\n\nEnterprise-Ressource-Planning, Ressourcen wie z.B. Finanzen unternehmerisch überwachen und planen\n\nFDM\n\nForschungsdatenmanagement\n\nFID\n\nFachinformationsdienst\n\nFIS\n\nForschungsinformationssystem\n\nGOKb\n\nGlobal Open Knowledgebase, Freie Datenbank zu E-Ressourcen, insbesondere für ERM\n\nID\n\nIdentifikator, eindeutige Referenz auf einen Datensatz\n\nIDM\n\nIdentity Management, das Speichern von Metadaten zu Personen\n\nIIIF\n\nInternational Image Interoperability Framework, Standard für digitale Bilder\n\nKBART\n\nKnowledgebases and related tools, Datenformat zum Transfer von Metadaten\n\nKDSF\n\nKerndatensatz Forschung, ein Standard für Forschungsinformationen für das deutsche Wissenschaftssystem\n\nLLM\n\nLarge Language Model, eine statistisches Sprachmodell zur Erzeugung von Text\n\nLZA\n\nLangzeitarchivierung\n\nMARC\n\nMAchine-Readable Cataloging, ältestes und noch immer wichtigstes bibliothekarisches Austauschformat\n\nNFDI\n\nNationale Forschungsdateninfrastruktur\n\nOAI-PMH\n\nOpen Archives Initiative Protocol for Metadata Harvesting\n\nOCR\n\nAutomatische Erkennung von Texten in Bildern (Optical Character Recognition)\n\nOER\n\nOpen Educational Resources, freie Bildungsmaterlien wie Kurse, Lernvideos und Bilder\n\nOPAC\n\nOnline Public Access Catalogue, Katalog einer Bibliothek\n\nPDA\n\nPatron-Driven-Acquisition\n\nPID\n\nPersistent Identifier, eine eindeutige und dauherhafte Benennung einer digitalen Ressource\n\nSIP2\n\nProtokoll zur Anbindung von Selbstbedienungsautomaten an Bibliothekssysteme\n\nSRU\n\nSearch/Retrieve via URL, ein technischer Standard für Suchanfragen in Bibliothekskatalogen\n\nSWORD\n\nSimple Webservice Offering Repository Deposit\n\nSaaS\n\nSoftware as a Service, Software und Hardware bei externem Dienstleister\n\nWCAG\n\nWeb Content Accessibilty Guidelines",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Abkürzungsverzeichnis</span>"
    ]
  },
  {
    "objectID": "abbildungen.html",
    "href": "abbildungen.html",
    "title": "Anhang B — Abbildungsverzeichnis",
    "section": "",
    "text": "Hier fehlt noch ein vollständiges Abbildungsverzeichnis mit Quellenangaben\n\nmedia/Sketch_FIS_FDM_CC_BY_Mau.png Sketchnote: Forschungsinformationen und ihre Sicht auf Forschungsdaten. https://doi.org/10.5281/zenodo.4899000 CC-BY 4.0 Franziska Mau\nSketchnote: Auf der Suche nach dem heiligen Gral - Forschungsinformationssysteme. https://doi.org/10.5281/zenodo.4388855 CC-BY 4.0 Franziska Mau\nmedia/etl-prozess.svg\nhttps://commons.wikimedia.org/wiki/File:Etl-prozess.svg Public Domain von Jakob Voß und Clemens Kynast\nmedia/sso-approaches.svg Three aproaches to Single Sign On. https://commons.wikimedia.org/wiki/File:Single_sign_on_aproaches.svg Public Domain\nmedia/die-datenlaube.jpg Die Datenlaube: Projektlogo https://commons.wikimedia.org/wiki/File:Die_Datenlaube.jpg Public Domain\nmedia/persona.png https://nbn-resolving.org/urn:nbn:de:0290-opus4-35267 CC-BY Kerstin Wendt, Matthias Finck\nmedia/wireframe.png https://www.kitodo.org/fileadmin/groups/kitodo/Dokumente/Kitodo.Production_Abschlussbericht_DFG-Projekt.pdf CC-BY Matthias Finck und Kerstin Wendt\nmedia/tools.svg CC-BY Ralf Stockmann. Quelle bearbeitbar unter https://next.wolkenbar.de/s/4AQ24X6jjiwcKDd",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Abbildungsverzeichnis</span>"
    ]
  },
  {
    "objectID": "mitarbeit.html",
    "href": "mitarbeit.html",
    "title": "Anhang C — Hinweise zur Mitarbeit",
    "section": "",
    "text": "Arbeitsablauf\nAlle Kapitel wurden in Booksprints vor Ort geplant und in Form von Google Docs Dokumenten geschrieben. Diese wurden anschließend einmalig ins Markdown-Format übertragen und auf it-in-bibliotheken.de veröffentlicht (siehe Technik).\nWeitere Änderungen und Beiträge zum Handbuch sind auf folgenden Wegen möglich:\nDie Bearbeitungsvorschläge und Kommentare aus Google Docs werden in unregelmäßigen Abständen von der Redaktion in die Markdown-Quellen übernommen und die Google Docs Dokumente anschließend aktualisiert. Für Bilder, Literaturverzeichnis und Glossar gibt es eigene Abläufe. Der aktuelle Workflow nach Ende der Booksprints beinhaltet folgende Schritte:",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hinweise zur Mitarbeit</span>"
    ]
  },
  {
    "objectID": "mitarbeit.html#arbeitsablauf",
    "href": "mitarbeit.html#arbeitsablauf",
    "title": "Anhang C — Hinweise zur Mitarbeit",
    "section": "",
    "text": "Bearbeitungsvorschläge und Kommentare in Google Docs („Kapitel Kommentieren“): im Zweifelsfall die einfachste Methode\nDirekte Bearbeitung der Markdown-Quellen durch einen Pull-Request („Seite editieren“): erfordert einen GitHub-Account und rudimentäre Markdown-Kenntnisse\nÖffentlicher Kommentar als GitHub-Issue („Problem melden“): erfordert einen GitHub-Account\nPersönlicher Hinweis an die Autor*innen\n\n\n\n\nFeedback-Links unter dem Inhaltsverzeichnis rechts\n\n\n\n\nLektorat\n\nFertige Kapitel stehen auf der Webseite und als Google Docs Dokument zum Korrekturlesen und Kommentieren bereit. Je Kapitel sind zwei externe (bisher nicht am Schreiben beteiligte) Lektor*innen eingeteilt, um die Einhaltung des Styleguide zu überprüfen und Änderungen vorzuschlagen.\n\nRedaktion\n\nHinweise und Änderungsvorschläge werden von den Autor*innen diskutiert und entschieden. Falls die daraus resultierenden Änderungsvorschläge nicht als Pull-Request vorliegen, müssen sie in Google-Docs bestätigt und parallel in die Markdown-Quellen eingepflegt werden.\n\nPublikation\n\nBestätigte Änderungen an der Markdown-Dateien im git-Repository führen direkt dazu, dass das Buch automatisch aktualisiert wird (siehe Technik, bisher nur HTML-Version, Druckversion ist geplant).",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hinweise zur Mitarbeit</span>"
    ]
  },
  {
    "objectID": "mitarbeit.html#styleguide",
    "href": "mitarbeit.html#styleguide",
    "title": "Anhang C — Hinweise zur Mitarbeit",
    "section": "Styleguide",
    "text": "Styleguide\nDieser Styleguide soll eine einheitliche Form trotz unterschiedlicher Autor*innen gewährleisten. Dazu gibt es Hinweise zu Zielgruppe, Stil und Aktualität, Schreibweise und Struktur sowie Vorgaben zu besonderen Inhalten wie Bildern und anderen Medien, Literaturverzeichnis und Abkürzungsverzeichnis.\nBei der Bearbeitung sollten folgende Grundsätze beachtet werden:\n\nWir verzichten auf individuelle Autor*innenschaft an einzelnen Textteilen. Alle können an allen Teilen mitarbeiten.\nDas Handbuch ist keine wissenschaftliche Arbeit, sondern soll einen Überblick geben. Für Details kann auf weiterführende Quellen verwiesen werden.\n\n\nZielgruppe\nZur Klärung der Zielgruppe dieses Handbuchs wurden einige sogenannte Personas definiert:\n\nJanine Buchinger: Janine leitet die Stadtbibliothek in einer Stadt mit 250.000 Einwohnern. Die Bibliothek besteht aus einer Zentrale und zwei Zweigstellen. Mit den Schulbüchereien besteht eine Kooperation für fachliche Beratung und gemeinsame Aktivitäten bei der Informationskompetenz-Vermittlung.\nDr. Tillmann Schuppe: Tillmann ist Leiter einer Fachhochschulbibliothek mit 500.000 Medieneinheiten. Die Bibliothek gehört einem Bibliotheksverbund an. Die Bibliothek plant einen Neubau, der gemeinsam mit dem Rechen- und Medienzentrum bezogen werden soll.\nMagda Olsowski: Magda ist studierte Informatikerin und leitet die Gruppe Forschungsdatenmanagement an einer großen Universitätsbibliothek. Sie hat keine bibliothekarische Vorbildung.\nAlicia Meyer: Alicia studiert Bibliotheksmanagement und plant eine Masterarbeit, in der sie die Implementierungsprozesse von Software analysieren möchte.\nRobert Pohlmann: Robert leitet die IT-Abteilung einer mittelgroßen Universitätsbibliothek und ist nebenberuflich Lehrbeauftragter für einen bibliothekarischen Studiengang.\n\n\n\nStil und Aktualität\n\nJournalistische oder enzyklopädische Neutralität sind nicht oberstes Prinzip dieses Buches. Es soll vielmehr fundiert und praxisorientiert informieren und beraten und darf dabei auch parteiisch sein.\nDieses Buch ist keine wissenschaftliche Forschungsveröffentlichung. Nicht jede Aussage muss mit einer Quelle belegt werden. Für die Anwendung in der Hochschullehre reicht es, wenn das Buch zentrale Aussagen belegt bzw. auf die wichtigsten aktuellen Studien verweist und somit auch Tipps zur weiterführenden Lektüre bietet.\nDas Buch sollte in 2-5 Jahren noch aktuell und verständlich sein, aber nicht mehr unbedingt in 10 Jahren.\nDas Buch soll als Nachschlagewerk dienen, das nicht vollständig durchgelesen werden muss. Dabei helfen Redundanz und Querverweise (siehe Hinweise zur Struktur).\nZumindest die einzelnen Kapitel sollen sich gut durchlesen lassen. Insbesondere Brüche in Stil und Aussage, Widersprüche oder unnötige Redundanzen sind innerhalb der Kapitel unbedingt zu vermeiden. Unter Berücksichtung des Nachschlagewerk-Charakters streben wir auch einen guten Lesefluss innerhalb der Kapitel an.\n\n\n\nStruktur\n\nStruktur des Texts\n\nWir verwenden kurze, unverschachtelte Sätze.\nWir erzeugen Sinnabschnitte, die möglichst für sich stehend verständlich sind.\nWir schreiben stark strukturiert, also\n\nmit vielen Zwischenüberschriften, bis maximal zur vierten Gliederungsebene,\nwo es inhaltlich passt, in stichpunktartigen Listen und\nmit Hervorhebung wichtiger Begriff durch Fettdruck als Gliederungshilfe.\n\nWir verwenden Infoboxen, die auch unabhängig vom übrigen Text lesbar sind.\nWir liefern wichtige Informationen zusätzlich zum Text in Form von Bildern, Tabellen, Listen, Infoboxen und/oder Zusammenfassungen.\n\n\n\nStruktur der Hauptkapitel\nDas Handbuch behandelt aufgeteilt in Hauptkapitel die wesentlichen Themen rund um IT in Bibliotheken.\n\nJedes Kapitel beginnt mit einer Kurzfassung als Infobox gefolgt von einer Einleitung und endet mit einem Abschnitt Zusammenfassung und Ausblick.\nKapitel haben normalerweise einen Umfang von rund 4.000 Wörtern. Deutlich längere Kapitel sind darauf zu prüfen, ob sie sich in mehrere Kapitel trennen lassen, und wenn das nicht möglich ist, müssen sie sorgfältig in Unterkapitel aufgeteilt werden.\nDie Strukturierung in Unterkapitel sollte stimmig und ausgewogen sein, insbesondere mit Blick auf die Ziele des Buches.\nDie Übergänge zwischen einzelnen Unterabschnitten sollten stimmig sein.\nJedes Kapitel beinhaltet ein aussagekräftiges Metadatenfeld description für Suchmaschinen (maximal 158 Zeichen)\n\n\n\n\nSchreibweise, Fachbegriffe und Verweise\n\nWir verwenden im gesamten Buch gendergerechte Schreibweise mit Sternchen (*). In Markdown ist es sicherer dem Sternchen einen Backslash voranzustellen, z.B. Autor\\*innen.\nEine Schreibweise für häufig verwendete Fachbegriffe sollte quer durch das Buch eingehalten werden, so z.B. BMS für Bibliotheksmanagementsysteme\nFachbegriffe (z.B. Bibliotheksverbund) werden dort verwendet, wo sie wiederholt relevant sind, und werden bei ihrer ersten Erwähnung definiert. Abkürzungen werden zusätzlich im Abkürzungsverzeichnis erfasst.\nWir vermeiden IT-Jargon.\nWir vermeiden Substantivierung („Digitalisierungswürdigkeit“) und unnötige Passiv-Formen.\nQuellen sollten nur dann genannt werden wenn in der jeweiligen Textpassage auch wirklich paraphrasiert oder wörtlich zitiert wird.\n\n\nTypografie\n\nAbkürzungen werden durch geschützte Leerzeichen getrennt (z. B.)\nGerade Anführungszeichen \"...\" werden automatisch durch die Deutschland und Österreich übliche Anführungszeichen („…“) ersetzt.\n\nExterne Links, die nur auf Anbieter oder andere Websites verweisen, werden inline verlinkt.\n\nLinks, die auf später entstehende Kapitel verweisen, werden durch eckige Klammern kenntlich gemacht.\nKursive Hervorhebung sollte nur für Namen und Glossareinträge verwendet werden.\nWir verzichten auf Fußnoten.\n\n\n\n\nBilder und andere Medien\n\nBilder und andere Mediendateien kommen in das Verzeichnis media im git-Repository. Alternativ können sie von externen Quellen per URL eingebunden werden wenn die Quelle voraussichtlich dauerhaft verfügbar ist.\nBilder sollten möglichst als Vektorgrafik (SVG) bereitgestellt werden.\nStandardschriftart ist Source Sans und Standard-Farbe für Hervorhebungen ist Blau mit dem Farbcode #2780e3 und darauf aufbauende Hellblau-Töne.\nBitte nutzt sprechende Dateinamen!\n\n\n\nLiteraturverzeichnis\nDie zitierte und weiterführende Literatur wird in einer Zotero-Gruppe unter https://www.zotero.org/groups/4673379/it_in_bibliotheken verwaltet. Der BibLaTex-Export dieser Bibliographie wird mit Aufruf von make refs von dort heruntergeladen und unter references.bib gespeichert. Diese Datei sollte also nicht direkt bearbeitet werden! Innerhalb des Markdown-Quelltext kann mittels Pandoc-Citation Syntax und dem jeweiligen Citekey aus references.bib auf Literatur verwiesen werden.\n\n\nGlossar\nDas Abkürzungsverzeichnis in der Datei glossar.yml enthält erklärungswürdige Begriffe mit Kurzbeschreibung und optionalem Link auf eine weiterführende Quelle (meist Wikipedia). Es werden _keine_ Firmennamen aufgenommen, auch wenn sie Akronyme sind. Die Glossarbegriffe werden in den Textdateien zur Hervorhebung kursiv gesetzt (in Markdown so ein *Fachbegriff*). Bei Erzeugung der HTML-Version des Handbuchs wird die Hervorhebung in einen Tooltip umgewandelt.\n\n\nAutor*innen-Verzeichnis\nWenn Du etwas beigetragen hast und möchtest, dass Du im Verzeichnis der Autor*innen auftauchst, trage Dich in der YAML-Datei contributors.yml ein. Die Einträge sollten nach Nachname sortiert werden. Die Felder email, position und orcid sind optional.",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hinweise zur Mitarbeit</span>"
    ]
  },
  {
    "objectID": "mitarbeit.html#technik",
    "href": "mitarbeit.html#technik",
    "title": "Anhang C — Hinweise zur Mitarbeit",
    "section": "Technik",
    "text": "Technik\nDie Master-Version des Handbuch liegt in einem git-Repository unter https://github.com/pro4bib/handbuch-it-in-bibliotheken. Die Ergebnisdateien werden automatisch via GitHub und einen Server der VZG mit der Software quarto aktualisiert, sodass unter https://it-in-bibliotheken.de/ immer der aktuellste Stand einsehbar sein sollte.\n\nVerzeichnisstruktur\nDie Markdown-Dateien im Wurzelverzeichnis (*.md) sind die Masterdateien.\n\nabout.yml bibliographische Metadaten (Titel, Abstract…)\n_quarto.yml zentrale Konfigurationsdatei zur Anpassung der Konvertierung mit Quarto\ncontributors.yml Autor*innen-Verzeichnis\nreferences.bib Literaturverzeichnis (bitte nicht direkt bearbeiten!)\n\nWeitere Unterverzeichnisse:\n\nmedia/ Bilder und andere Medien\n\nDie Dateien in folgenden Verzeichnissen sollen nicht per Hand geändert werden:\n\n_gdrive/ von bzw. nach Google-Drive importierte bzw. exportierte Kapitel (siehe README.md)\n\n\n\nKonvertierung\nZur Anpassung der Konvertierung des Handbuchs mit quarto muss das Repository lokale geklont und Quarto installiert werden. Für die DOCX-Ausgabe muss außerdem rsvg-convert installiert werden (Paket librsvg2-bin bzw. libsrvg).\nDie Aufrufe sind zur Vereinfachung in Makefile zusammengefasst:\n\nmake preview konvertiert das Handbuch nach HTML und ermöglicht eine Vorschau unter http://localhost:15745/ (der Port ist die Postleitzahl von Wildau). Die HTML-Ansicht wird automatisch aktualisiert wenn die Quelldateien lokale geändert werden.\nmake build konvertiert das Handbuch in alle konfigurierten Formate und legt die Ergebnisse im Verzeichnis _book ab. Dieser Schritt wird auch automatisch nach jedem Push auf GitHub ausgeführt.\nmake all ruft make build und docx auf und kopiert die DOCX-Dateien ins Publikationsverzeichnis _book.\nmake html erzeugt nur HTML-Dateien in _book.\nmake docx erzeugt nur das Gesamt-DOCX in _book.\nmake refs aktualisiert das Literaturverzeichnis in references.bib von Zotero.",
    "crumbs": [
      "Anhang",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Hinweise zur Mitarbeit</span>"
    ]
  }
]